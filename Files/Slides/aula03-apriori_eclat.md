# Slide: aula03-apriori_eclat (Aula 3, 4) - Minera√ß√£o de itens frequentes: Apriori e Eclat

## Aula 03 | 25/03/2025 | Minera√ß√£o de conjuntos de itens - Faltei - Minera√ß√£o de itens frequentes: Apriori e Eclat

### Introdu√ß√£o (Aula 03)

- Como vimos na aula anterior, o principal problema do algoritmo ing√™nuo para minera√ß√£o de conjuntos de itens frequentes era a replica√ß√£o de esfor√ßos para avaliar o suporte dos candidatos
- As m√∫ltiplas passadas no conjunto de dados (armazenado em mem√≥ria secund√°ria) torna o algoritmo impratic√°vel at√© mesmo para pequenos volumes
- Os algoritmos que veremos hoje exploram propriedades do problema para amortizar o custo da computa√ß√£o de suporte, e evitar retrabalho na avalia√ß√£o dos candidatos

- [JV]
  - O que √© mesmo o suporte? ü§î
  - Recapitulando da aula anterior, o Suporte √© a quantidade de vezes que determinado item aparece no conjunto de dados; j√° o "Suporte M√≠nimo" ($minsup$) √© o limiar que define se determinado item √© frequente o bastante ou n√£o.
  - Esse valor √© dado pela seguinte f√≥rmula:
    - $sup(X) = |c(X)|$, onde $c(X)$ √© a cobertura do itemset $X$.
  - Mas o que √© mesmo a cobertura?
    - A cobertura √© o conjunto de transa√ß√µes que cont√©m um itemset $X$. Ou seja, √© o conjunto de transa√ß√µes que cont√©m todos os itens do itemset $X$.

### Apriori

- O Apriori foi proposto por Rakesh Agrawal e Ramakrishnan Srikant em 1994
  - O artigo possui mais de 30K cita√ß√µes
- Os autores √† √©poca trabalhavam no projeto da IBM para o Wal-Mart
- A ideia central √© evitar computa√ß√µes desnecess√°rias para candidatos infrequentes
- Isso √© viabilizado pela propriedade de **anti-monotonicidade** da fun√ß√£o suporte
- Essa √© uma das propriedades mais importantes para a √°rea

- Autores: Rakesh Agrawal (Data Insights Labs) e Ramakrishnan Srikant (Google Fellow) (1994)

#### Anti-monotonicidade do suporte

- Considere dois itemsets $A$ e $B$ quaisquer. Se $A \subseteq B$, ent√£o $sup(A) \geq sup(B)$.
- Essa observa√ß√£o nos diz que a cobertura de conjuntos de itens √©, no m√°ximo, t√£o grande quanto a de seus subconjuntos
  - No caso mais simples, um conjunto de dois itens n√£o pode ocorrer em mais transa√ß√µes que cada um dos itens individualmente
- Consequentemente, se o itemset A √© infrequente, B tamb√©m ser√°.
- Isso define a propriedade de anti-monotonicidade da fun√ß√£o suporte, tamb√©m conhecida como a propriedade do Apriori

  - **Todo superconjunto de um conjunto infrequente √© infrequente**
  - **Todo subconjunto de um conjunto frequente √© frequente**

- [JV]
  - Muito interessante isso da√≠ de cima.
  - Basicamente entendemos que $sup(X=\lbrace A, B \rbrace) \geq sup(Y=\lbrace A, B, C \rbrace)$ com isso, se X √© frequente, nada garante que Y tamb√©m seja. Por√©m, se Y √© frequente, isso garante que todos os poss√≠veis subconjuntos de Y tamb√©m ser√£o frequentes.

### Apriori [2]

- O Apriori utiliza uma busca em largura no espa√ßo de busca para minerar os padr√µes
  - Frequentemente, o termo usado na literatura √© abordagem por n√≠veis (level-wise approach)
- A busca inicia com a identifica√ß√£o dos itens frequentes
- Depois, os conjuntos de tamanho $k$ s√£o explorados antes dos de tamanho $k+1$
- Assim como o algoritmo ing√™nuo, ele tamb√©m opera em duas etapas:
  - Gera√ß√£o de candidatos
  - C√¥mputo do suporte e elimina√ß√£o dos infrequentes

---

- A gera√ß√£o dos candidatos √© feita a partir dos conjuntos frequentes encontrados na fase anterior
- Conjuntos compartilhando um prefixo de $k-1$ itens s√£o combinados para gerar candidatos de tamanho $k+1$
  - Novamente, assume-se que eles s√£o ordenados pela ordem lexicogr√°fica
- Candidatos que possuam algum subconjunto infrequente s√£o descartados imediatamente
  - A propriedade do Apriori √© empregada
- Os suportes dos candidatos s√£o atualizados com uma √∫nica passada no conjunto de dados
  - Subconjuntos de tamanho $k$ de cada transa√ß√£o s√£o usados para atualizar o suporte dos candidatos

---

- **APRIORI** $(D, \mathcal{I}, minsup)$:
  - $\mathcal{F} \gets \emptyset$
  - $\mathcal{C}^{(1)} \gets \lbrace \emptyset \rbrace$ `// Initial prefix tree with single items`
  - **foreach** $i \in \mathcal{I}$ **do** Add $i$ as child of $\emptyset$ in $\mathcal{C}^{(1)}$ with $sup(i) \gets 0$
  - $k \gets 1$ `// k denotes the level`
  - **while** $\mathcal{C}^{(k)} \neq \emptyset$ **do**
    - **ComputeSupport** $(\mathcal{C}^{(k)}, D)$
    - **foreach** _leaf_ $X \in \mathcal{C}^{(k)}$ **do**
      - **if** $sup(X) \geq minsup$ **then** $\mathcal{F} \gets \mathcal{F} \cup \lbrace (X, sup(X)) \rbrace$
      - **else** remove $X$ from $\mathcal{C}^{(k)}$
    - $\mathcal{C}^{(k+1)} \gets$ ExtendPrefixTree($\mathcal{C}^{(k)}$)
    - $k \gets k+1$
  - **return** $\mathcal{F}^{(k)}$

---

- **ComputeSupport** $(\mathcal{C}^{(k)}, D)$:

  - **foreach** $\langle t, i(t) \rangle \in D$ **do**
    - **foreach** k-subset $X \subseteq i(t)$ **do**
      - **if** $X \in \mathcal{C}^{(k)}$ **then** $sup(X) \gets sup(X) + 1$

- **ExtendPrefixTree** $(\mathcal{C}^{(k)})$:
  - **foreach** leaf $X_a \in \mathcal{C}^{(k)}$ **do**
    - **foreach** leaf $X_b \in SIBLING(X_a)$, such that $b > a$ **do**
      - $X_{ab} \gets X_a \cup X_b$ `// prune candidate if there are any infrequent subsets`
      - **if** $X_j \in \mathcal{C}^{(k)}$, **for all** $X_j \subset X_{ab}$, such that $|X_j| = |X_{ab}|-1$ **then**
        - Add $X_{ab}$ as child of $X_a$ with $sup(X_{ab}) \gets 0$
    - **if** _no extensions from_ $X_a$ **then**
      - Remove $X_a$, and all ancestors of $X_a with no extensions, from $\mathcal{C}^{(k)}$
  - **return** $\mathcal{C}^{(k)}$

---

- Exemplo $(minsup=3)$:

$$
\begin{bmatrix}
  TID & Muesli & Oats & Milk & Yoghurt & Biscuits & Tea \\
  1   &      1 &    0 &    1 &       1 &        0 &   1 \\
  2   &      0 &    1 &    1 &       0 &        0 &   0 \\
  3   &      0 &    0 &    1 &       0 &        1 &   1 \\
  4   &      1 &    0 &    0 &       1 &        0 &   0 \\
  5   &      0 &    1 &    1 &       0 &        0 &   1 \\
  6   &      1 &    0 &    1 &       0 &        0 &   1 \\
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
  TID & Muesli & Milk & Tea \\
    1 &      1 &    1 &   1 \\
    2 &      0 &    1 &   0 \\
    3 &      0 &    1 &   1 \\
    4 &      1 &    0 &   0 \\
    5 &      0 &    1 &   1 \\
    6 &      1 &    1 &   1 \\
\end{bmatrix}
$$

---

- O n√∫mero de passadas √© drasticamente reduzido em rela√ß√£o ao algoritmo ing√™nuo
  - $O(2^I) \rightarrow O(I)$
- As podas baseadas na anti-monotonicidade do suporte tamb√©m s√£o bastante efetivas na pr√°tica
- O algoritmo tamb√©m apresenta alguns problemas:
  - Busca em largura requer que todos os candidatos de um n√≠vel sejam mantidos em mem√≥ria. Esse custo √© proibitivo em alguns (muitos) casos
  - Tanto a contagem do suporte quanto a poda do Apriori podem ser consideravelmente caras, dependendo da implementa√ß√£o

---

- O custo de mem√≥ria √© inerente √† abordagem, e n√£o podemos fazer muita coisa para melhor√°-lo
- O custo da contagem e verifica√ß√£o pode ser atenuado, usando estruturas de dados mais 'sofisticadas'
- Existem duas abordagens mais comuns:
  - Usar uma √°rvore hash
  - Usar uma √°rvore de prefixos (Trie)
- Na primeira abordagem, cada n√≥ folha armazena um conjunto de candidatos/conjuntos frequentes
  - O n√∫mero de compara√ß√µes √© reduzido
- Na segunda abordagem, os n√≥s da Trie armazenam os candidatos/conjuntos frequentes. Os subconjuntos das transa√ß√µes s√£o usadas para index√°-la e atualizar o suporte

---

- A redu√ß√£o do suporte m√≠nimo tem um impacto muito grande no custo computacional do algoritmo
  - O tamanho dos candidatos aumenta $\to$ Mais candidatos s√£o avaliados em cada n√≠vel $\to$ o tamanho dos conjuntos frequentes aumenta $\to$ mais n√≠veis s√£o explorados

[Imagem(a): Number of candidate itemsets]

[Imagem(b): Number of frequent itemsets]

---

- A densidade da base de dados tamb√©m tem muito impacto no custo
  - Transa√ß√µes passam a ter mais itens
  - Isso tem duas implica√ß√µes: tamanho m√©dio dos itemsets aumentam; mais subconjuntos s√£o gerados durante a contagem do suporte $\binom{|t|}{k}$

[Imagem(a): Number of candidate itemsets]

[Imagem(b): Number of frequent itemsets]

### Eclat - Pr√≥xima aula

## Aula 04 | 27/03/2025 | Minera√ß√£o de conjuntos de itens

### Aula passada

- Algoritmo Apriori
- Frequente, infrequente.
- Como calcula o suporte

  - A partir dos itens frequentes: tabelas de 0 e 1.
  - Pra isso usava √°rvore de prefixos
  - Para cada transa√ß√£o gerava os itemsets de tamanho k, ia na √°rvore de prefixos
  - E incrementava o suporte daquela chave

- [JV: escrevi o que ele t√° falando, mas n√£o t√¥ entendendo]
  - Duas coisas influenciam o desempenho do algoritmo
    1. Ele falou algo
    2. Se o BD √© denso, as transa√ß√µes s√£o mais largas.
       - $\binom{|t|}{k}$
       - Quando √© esparso, funciona bem. Quando √© denso que come√ßa a dar problema.
  - Eu t√¥ achando que se eu compro $J = \lbrace A, B, C \rbrace$, Ent√£o o conjunto pot√™ncia dele √© $P(J) = \lbrace \emptyset, A, B, C, AB, AC, BC, ABC \rbrace$, e ent√£o, incrementaria 1 para um desses grupos
  - C√°lculo de suporte:
    - Para cada um dos itemsets tem que verificar se ele t√° na √°rvore K(?)
  - Se os itemsets est√£o em mem√≥ria...
  - Se quero gerar o itemset $XY$ partindo de $X \cup Y$, posso dizer que o suporte ser√° $|c(X) \cap c(Y)|$

### Eclat (Equivalence Class Transformation)

- Dadas as defici√™ncias do Apriori, M. Zaki prop√¥s, em 2000, o algoritmo Equivalence Class Transformation (Eclat)
- A proposta do algoritmo √© 'eliminar' a necessidade de passadas no conjunto de dados para computar o suporte
- Para isso, ele parte de uma representa√ß√£o vertical dos dados, e se baseia no fato de que a cobertura da uni√£o de dois itemsets √© a interse√ß√£o de suas coberturas

- [JV]
  - Dada a representa√ß√£o vertical dos dados, consigo calcular o suporte por essa intercess√£o.
  - Problema: como mantenho todos os itemsets gerados em mem√≥ria?

---

- Ou seja, a ideia central do algoritmo tentar manter os tidsets em mem√≥ria principal para computar o suporte dos itemsets atrav√©s de interse√ß√µes desses conjuntos
- Contudo, todos os tidsets podem n√£o caber na mem√≥ria principal. Assim, √© necess√°rio algum mecanismo que possibilite a divis√£o do espa√ßo de busca em subproblemas independentes que caibam na mem√≥ria
- A divis√£o √© feita conforme uma rela√ß√£o de equival√™ncia estabelecida sobre os candidatos

- [JV]
  - Tenta manter tudo na mem√≥ria principal
  - A ideia √© partir o problema em subproblemas e trazer esses subproblemas pra mem√≥ria.
  - Surgiu atrav√©s da cria√ß√£o de uma rela√ß√£o de equival√™ncia entre os itemsets

---

- Seja $p: P(I) \times \mathbb{N} \rightarrow P(I)$ uma fun√ß√£o prefixo. $p(X, k) = X[1:k]$.
- A rela√ß√£o $\theta_k \subseteq P(I) \times P(I), A \theta_k B \equiv p(A, k) = p(B, k)$, √© uma rela√ß√£o de equival√™ncia
- Dessa forma, ela induz uma parti√ß√£o dos conjuntos de itens em classes de equival√™ncia, onde todos os elementos compartilham um certo prefixo
- Por exemplo, todos os conjuntos que cont√™m o item Muesli pertencem √† classe de equival√™ncia $[Muesli]_{\theta_1}$
- Intuitivamente, essas classes servem como proje√ß√µes do conjunto de dados, em que somente as transa√ß√µes contendo aquele prefixo s√£o consideradas

- [JV]
  - Cria-se uma rela√ß√£o de equival√™ncia pelos prefixos.
  - Diz-se que dois itemsets s√£o equivalentes se o prefixos dos dois s√£o iguais.
  - Consideremos que temos o seguinte conjunto pot√™ncia: $\mathcal{P}(I) = \lbrace \emptyset, A, B, C, AB, AC, BC, ABC \rbrace$. Na forma de representa√ß√£o, seria como se agrup√°ssemos os dados em grupos de prefixos:
    - $\emptyset: \lbrace \emptyset, A, B, C, AB, AC, BC, ABC \rbrace$
    - $A: \lbrace A, AB, AC, ABC \rbrace$
    - $B: \lbrace B, BC \rbrace$
    - $C: \lbrace C \rbrace$
  - E ent√£o seriam varridos de C para A (ou o $\emptyset$).
  - Poderia-se tamb√©m fazer subgrupos de subgrupos, dependendo do tamanho do conjunto de prefixos.

---

- Durante a busca em profundidade, o algoritmo particiona os conjuntos de itens conforme a rela√ß√£o de equival√™ncia e o n√≠vel da √°rvore
- O particionamento pode ser encerrado t√£o logo os tidsets caibam na mem√≥ria e as interse√ß√µes possam ser computadas facilmente
- Contudo, a estrat√©gia pode ser usada durante toda a execu√ß√£o do algoritmo
- O c√°lculo do suporte no algoritmo se restringe a calcular o tamanho do tidset

- [JV]
  - Ele faz uma busca em profundidade (DFS)
  - Ele faz subparti√ß√µes at√© que o n√∫mero de transa√ß√µes seja pequeno o suficiente para caber na mem√≥ria.

---

- **ALGORITHM 8.3. Algorithm ECLAT**
- // Initial Call: $\mathcal{F} \gets \emptyset, P \gets \lbrace  \langle i, t(i) \rangle | i \in \mathcal{I}, |t(i)| \geq minsup  \rbrace$
- **ECLAT** $(P, minsup, \mathcal{F})$:
  - **foreach** $\langle X_a, t(X_a) \rangle \in P$ **do**
    - $\mathcal{F} \gets \mathcal{F} \cup \lbrace (X_a, sup(X_a)) \rbrace$
    - $P_a \gets \emptyset$
    - **foreach** $\langle X_b, t(X_b) \rangle \in P$, with $X_b > X_a$ **do**
      - $X_{ab} = X_a \cup X_b$
      - $t(X_{ab}) = t(X_a) \cap t(X_b)$
      - **if** $sup(X_{ab}) \geq minsup$ **then**
        - $P_a \gets P_a \cup \lbrace  \langle X_{ab}, t(X_{ab}) \rangle  \rbrace$
    - **if** $P_a \neq \emptyset$ **then** ECLAT $(P_a, minsup, \mathcal{F})$

[JV: Droga, foquei em transcrever brevemente e esqueci de prestar aten√ß√£o na explica√ß√£o do professor]

- [JV]
  - P guarda todos os frequentes da chamada anterior. porque ele filtrou todos que s√£o infrequentes pelo minsup
  - para cada um dos frequentes dos candidatos, armazena no conjunto de itens frequentes globais
  - e a partir dele gera em profundidade a combina√ß√£o dele com todos os outros que v√™m pra frente.
  - Evitam redund√¢ncia: 1. parti√ß√µes; 2. Ordem sistem√°tica de combina√ß√£o dos itens.
    1. A B C
    2. A com B e C: AB AC
    3. AB com AC: ABC
    4. B com C: BC

#### Representa√ß√µes de conjuntos de dados (Aula 4)

$$
\begin{bmatrix}
  TID & Muesli & Oats & Milk & Yoghurt & Biscuits & Tea \\
  1 & 1 & 0 & 1 & 1 & 0 & 1\\
  2 & 0 & 1 & 1 & 0 & 0 & 0\\
  3 & 0 & 0 & 1 & 0 & 1 & 1\\
  4 & 1 & 0 & 0 & 1 & 0 & 0\\
  5 & 0 & 1 & 1 & 0 & 0 & 1\\
  6 & 1 & 0 & 1 & 0 & 0 & 1\\
\end{bmatrix}
\Rightarrow
[\emptyset]_{\theta_0}
\begin{bmatrix}
  X      & c(x)  \\
  Muesli & 146   \\
  Milk   & 12356 \\
  Tea    & 1356  \\
\end{bmatrix}
$$

- [JV]
  - Pelo que eu t√¥ entendendo:
    1. Come√ßa pegando todos os itens que tenham uma quantidade de transa√ß√µes maior que o minsup (o valor m√≠nimo aceit√°vel para que consideremos relevante)
    2. Depois disso, come√ßamos fazendo a intercess√£o das transa√ß√µes entre o primeiro conjunto de itens que passou pela compara√ß√£o com o segundo conjunto.
    3. Depois disso, v√™ se o resultado dessas intercess√µes √© grande o bastante.
  - Se $A \subseteq B$, ent√£o $c(B) \subseteq c(A)$ (Cobertura)

### Eclat (Equivalence Class Transformation) [2]

- O custo computacional do algoritmo est√° diretamente relacionado ao tamanho dos tidsets
  - O tempo de execu√ß√£o depende do c√°lculo da interse√ß√£o dos tidsets
- O custo de espa√ßo tamb√©m √© dependente do tamanho. Quanto mais denso o conjunto de dados, mais largos ser√£o os tidsets
- H√° duas formas de se implementar o algoritmo:
  - Usando vetores de bits
  - Usando vetores de Ids
- Os vetores de bits s√£o interessantes para o c√°lculo do suporte
  - Eles facilitam a computa√ß√£o da interse√ß√£o e o c√°lculo do tamanho do tidset pode ser feito usando uma tabela auxiliar (palavras de 16bits mapeadas para valores)
- Contudo, se o conjunto for esparso, isso representar√° um desperd√≠cio muito grande de espa√ßo. Ent√£o vetores de Ids se tornam mais interessantes.
  - As computa√ß√µes de interse√ß√£o s√£o feitas como na fun√ß√£o merge do mergesort

#### Diffsets e dEclat [Pr√≥xima aula]

- Percebendo o problema de se manter os tidsets em mem√≥ria, Zaki e Gouda propuseram em 2001 (o artigo s√≥ foi publicado em 2003) uma solu√ß√£o alternativa
- Eles propuseram armazenar as diferen√ßas entre os tidsets dos membros de uma classe e dos prefixos que a definem
- Eles chamaram esse conjunto de **diffset**
- Formalmente, para um prefixo P e um itemset PX, o diffset de X, $d(PX) = c(P) - c(PX)$
- Seriam armazenados, portanto, o suporte do itemset e seu diffset

- [JV]
  - Em bases de dados densos, varia bem pouco o suporte entre os itens. Ent√£o, faria mais sentido guardar s√≥ a diferen√ßa ao inv√©s de guardar o todo.
  - Ao inv√©s de chamar de tidset, passaram a chamar de diffset.
  - ...
  - Pode-se armazenar em vetores de bits ao inv√©s de vetores de inteiros.
  - Usando o vetor de bits, √© como se fosse:
    - $A = [00110, 01001, 01100, 00011]$
  - E para calcular o suporte (?) cobertura(?)
  - basta fazer um c√°lculo r√°pido de 0 a 255 para dizer quantos bits est√£o ativos, e ent√£o fazer a contagem de bits ativos somando esses valores.
    - $0 \to 1$
    - $1 \to 1$
    - $2 \to 1$
    - ...
    - $255 \to ...$
  - Outra forma de condensar √©: Se sei que um determinado conjunto √© grande o bastante, posso inferir que todos os que s√£o menores que eles tamb√©m s√£o grandes o bastante.
  - Se s√≥ √© guardado o valor das diferen√ßas, acaba sendo um problema fazer as intercess√µes.

---

- [JV]

  - Por defini√ß√£o, $d(PXY) = c(PX) - c(PXY) = c(PX) - c(PY)$
  - Podemos adicionar, ao conjunto acima, o conjunto vazio $(c(P) - c(P))$ sem alter√°-lo
  - Logo, $d(PXY) = c(PX) - c(PY) + c(P) - c(P) - c(P) = (c(P)-c(PY)) - (c(P) - c(PX)) = d(PY) - d(PX)$
  - Em outras palavras, podemos usar os diffsets dos conjuntos base para calcular o diffset do novo candidato
  - A variante do Eclat que usa diffsets ficou conhecida como dEclat

  - $C(PX) - C(PY)$
  - $C(PX) - C(PY) \cup C(P) - C(P)$
  - $C(PX) \cap \overline{C(PY)} \cup C(P) \cap \overline{C(P)}$
  - $C(PX) \cup \overline{C(P)} \cap C(P) \cup \overline{C(P)}$

## Aula 05 | 01/04/2025 | Minera√ß√£o de conjuntos de itens

### Diffsets e dEclat [Aula 05]

- Percebendo o problema de se manter os tidsets em mem√≥ria, Zaki e Gouda propuseram em 2001 (o artigo s√≥ foi publicado em 2003) uma solu√ß√£o alternativa
- Eles propuseram armazenar as diferen√ßas entre os tidsets dos membros de uma classe e dos prefixos que a definem
- Eles chamaram esse conjunto de **diffset**
- Formalmente, para um prefixo $P$ e um itemset $PX$, o diffset de $X$, $d(PX) = c(P) - c(PX)$
- Seriam armazenados, portanto, o suporte do itemset e seu diffset

---

- O fato √© que, se somente os diffsets s√£o armazenados, o suporte n√£o √© mais obtido como a cardinalidade desse conjunto
- Dessa forma, como calcular o suporte de um itemset $PXY$ obtido a partir de outros dois $PX$ e $PY$, usando somente seus diffsets?
- O suporte de $PX$ √© calculado pela diferen√ßa entre o suporte de $P$ e o diffset de $PX$, $sup(PX) = sup(PX) - |d(PXY)|$
  - Portanto, $sup(PXY) = sup(PX) - |d(PXY)|$
- A solu√ß√£o passa por computar o diffset de $PXY$. Por√©m, temos somente os diffsets de $PX$ e $PY$

---

- Por defini√ß√£o, $d(PXY) = c(PX) - c(PXY) = c(PX) - c(PY)$
- Podemos adicionar, ao conjunto acima, o conjunto vazio $(c(P) - c(P))$ sem alter√°-lo
- Logo, $d(PXY) = c(PX) - c(PY) + c(P) - c(P) - c(P) = (c(P)-c(PY)) - (c(P) - c(PX)) = d(PY) - d(PX)$
- Em outras palavras, podemos usar os diffsets dos conjuntos base para calcular o diffset do novo candidato
- A variante do Eclat que usa diffsets ficou conhecida como dEclat

- [JV]
  - $d(PXY) = c(PX) - c(PY) = c(PX) - c(PXY)$
  - $c(PXY) = c(PX) \cap c(PY)$
    - "Diferen√ßa √© a mesma coisa que interse√ß√£o com complemento"
    - - $d(PXY) = ...$
  - Ele fez um monte de igualdades com opera√ß√µes de conjuntos.
    - $d(PXY) =$
    - $c(PX) - c(PY) =$
    - $c(PX) - c(PXY) =$
    - $[c(PX) - c(PXY)] \cup [c(P) - c(P)] =$
    - $[c(PX) \cap \overline{c(PXY)}] \cup [c(P) \cap \overline{c(P)}]$
    - $[c(PX) \cup c(P)] \cap [\overline{c(PY)} \cup \overline{c(P)}] \cap \overline{d(PX)}$
    - $(c(PX) \cap \overline{c(PY)}) \cup (c(P) \cap \overline{c(P)}) \cup (c(P) \cap \overline{c(PY)}) \cup (c(P) \cap \overline{c(P)} \cap \overline{d(PX)})$
    - $(Q \subseteq d(PY)) \cup (\emptyset) \cup (d(PY)) \cup (\emptyset) \cap (\overline{d(PX)})$
    - $d(PY) \cap \overline{d(PX)} =$
    - $d(PY) - d(PX)$
  - $d(PX) = ...$
  - Outro exemplo:
    - $P = \lbrace 1, 2, 3, 4, 5 \rbrace$
    - $X = \lbrace 1, 3, 5 \rbrace$
    - $Y = \lbrace 2, 3, 4 \rbrace$
    - $PX = \lbrace 1, 3, 5 \rbrace$
    - $PY = \lbrace 2, 3, 4 \rbrace$
    - $\overline{PY} = \lbrace 1, 5 \rbrace$
    - $PX \cap \overline{PY} = \lbrace 1, 5 \rbrace$

---

- **ALGORITHM 8.4. Algorithm dEclat**
  - //`Initial Call:` $\mathcal{F} \gets \emptyset, P \gets \lbrace  \langle i, d(i), sup(i) \rangle | i \in \mathcal{I}, d(i) = \mathcal{T}\\t(i), sup(i) \geq minsup  \rbrace$
  - **dEclat** $(P, minsup, \mathcal{F})$:
    - **foreach** $\langle X_a, d(X_a), sup(X_a) \rangle \in P$ **do**
      - $\mathcal{F} \gets \mathcal{F} \cup \lbrace (X_a, sup(X_a)) \rbrace$
      - $P_a \gets \emptyset$
      - **foreach** $\langle X_b, d(X_b), sup(X_b) \rangle \in P$, with $X_b > X_a$ **do**
        - $X_{ab} = X_a \cup X_b$
        - $d(X_{ab}) = d(X_b) \setminus d(X_a)$
        - $sup(X_{ab}) = sup(X_a) - |d(X_{ab})|$
        - **if** $sup(X_{ab}) \geq minsup$ **then**
          - $P_a \gets P_a \cup \lbrace  \langle X_{ab}, d(X_{ab}), sup(X_{ab}) \rangle  \rbrace$
      - **if** $P_a \neq \emptyset$ **then** dEclat $(P_a, minsup, \mathcal{F})$

---

- Essa abordagem se mostrou muito eficiente para conjuntos densos
- Por√©m, em conjuntos esparsos, o algoritmo original √© a melhor op√ß√£o

- [JV]
  - Para bases esparsas: Eclat; para bases densas: dEclat

[Imagem (a): Minimun Support (%) - Connect]

[Imagem (b): Minimun Support (%) - pumsb*]

[Imagem (c): Minimun Support (%) - T40I10D100K]

### Leitura (Aula 05)

- Se√ß√µes 8.1, 8.2 (Zaki e Meira)
- Se√ß√µes 6.1, 6.2 (Introduction to Data Mining)
- Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3): 372-390 (2000)
- Zaki, M.J., Gouda, K.: Fast vertical mining using diffsets. Technical Report 01-1, Computer Science Dept., Rensselaer Polytechnic Institute (March 2001) 10
- Christian Borgelt. Efficient Implementations of Apriori and Eclat. Workshop of Frequent Item Set Mining Implementations (FIMI 2003, Melbourne, FL, USA).

- [JV]
  - Boa parte da explica√ß√£o est√£o nos artigos. A implementa√ß√£o t√° no Borgelt.
