{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwbAfcEpcVym"
   },
   "source": [
    "# Descrição do projeto prático da disciplina Aprendizado Descritivo 2025/1\n",
    "\n",
    "- Renato Vimeiro\n",
    "- 2025-06-02\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "O principal objetivo do projeto é explorar de forma prática os conceitos vistos na disciplina. Os alunos se reunirão em grupos e simularão o processo de análise de dados através de aprendizado descritivo. Para isso, deverão obter os dados, limpá-los/pré-processá-los, e aplicar alguma(s) técnica(s) de aprendizado descritivo vista no curso. Os grupos também deverão analisar os resultados e comunicá-los de forma escrita e oral. Dessa forma, o aluno irá experienciar os desafios relacionados a análise de dados em ambientes mais reais. A apresentação escrita e oral dos resultados servirá também como experiência para o grupo tanto do ponto de vista acadêmico, na apresentação de resultados em forma de artigo e palestras públicas, quanto do ponto de vista de formulação de relatórios que possam ser absorvidos por pessoas não diretamente relacionadas ao problema investigado.\n",
    "\n",
    "## Tarefas\n",
    "\n",
    "São tarefas do projeto:\n",
    "\n",
    "1. Obter dados para executar a análise. Os dados têm de ser representativos de problemas reais. Ou seja, dados obtidos de repositórios de benchmarks para Aprendizado de Máquina, tais como UCI ML Repository, não constituem dados viáveis para o projeto. Outras fontes como o Kaggle podem ser consultadas, porém, deve-se ter cuidado com o conjunto de dados escolhido. Conjuntos muito simples podem não permitir a aplicação das técnicas.\n",
    "2. Limpeza dos dados. Tendo obtido os dados, os alunos deverão aplicar as transformações necessárias para tratar os dados e adequá-los à análise que será aplicada. Entre as possíveis tarefas podem estar eliminação/preenchimento de dados ausentes, mudança de formato dos dados, codificação...\n",
    "3. Aplicação de aprendizado descritivo. Nessa etapa, os alunos deverão aplicar alguma(s) técnica(s) vista(s) no curso para analisar os dados e extrair padrões interessantes dos mesmos. Podem ser aplicadas tanto técnicas supervisionadas quanto não-supervisionadas. Recomenda-se que os alunos não implementem as técnicas, e tentem obter os algoritmos em algum repositório ou pacote. Existem diversos pacotes para diversos ambientes com praticamente todos os algoritmos vistos nas aulas.\n",
    "4. Análise dos resultados. Após a aplicação dos métodos, os alunos devem explorar os resultados, interpretando-os em busca de padrões interessantes. Deve-se levar em consideração aqui as métricas de qualidade dos padrões.\n",
    "5. Documentação/apresentação dos resultados. Os alunos devem narrar/documentar o processo que executaram durante a análise, bem como os resultados obtidos. Essa documentação se dará de forma escrita através de artigo científico, e de forma oral através de apresentação em sala de aula virtual. O artigo terá no máximo 12 páginas em formato da SBC. A apresentação terá no máximo 15min, e mais 5min para discussão com a turma.\n",
    "\n",
    "## O que entregar\n",
    "\n",
    "Os alunos devem criar um repositório público GitHub contendo todo o material usado nas análises (pacotes de software não devem ser incluídos no projeto, e sim referenciados para permitir a reprodução). Os dados também não precisam ser compartilhados. No entanto, caso sejam abertos, colocar o link para o local onde possam ser obtidos.\n",
    "\n",
    "No Moodle, devem ser entregues: o artigo em pdf, os slides, e o link para o repositório com o conteúdo das análises.\n",
    "\n",
    "## Organização\n",
    "\n",
    "O projeto pode ser executado em grupo de até 5 integrantes. Para diminuir a carga de trabalho de todos os envolvidos, é recomendável que o projeto seja desenvolvido em grupo. Além disso, cada grupo tem que designar atividades a todos os seus membros. Devem ser evitadas divisões de tarefas interdependentes. O ideal é que todos estejam trabalhando ao mesmo tempo.\n",
    "\n",
    "Os grupos devem eleger um representante para enviar mensagem ao professor indicando o nome completo e matrícula de todos os membros. Essa tarefa é importante para que o professor possa dimensionar o número as aulas necessárias para apresentação dos projetos. Ela deve ser realizada o mais breve possível, até o dia 15/06.\n",
    "\n",
    "## Datas\n",
    "\n",
    "A entrega do projeto será no dia 03/07. As apresentações serão nos dias 03, 08 e 10/07.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0AbN9E_yxHM"
   },
   "source": [
    "# Etapa 1: Instalar e importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5195,
     "status": "ok",
     "timestamp": 1751236705797,
     "user": {
      "displayName": "João Vitor Fernandes Dias",
      "userId": "17773602428702936204"
     },
     "user_tz": 180
    },
    "id": "QEWDjN3WaX3r",
    "outputId": "d9c046b5-8209-4168-a16b-392ccd6d95bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Instalação e importação de bibliotecas '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Instalação e importação de bibliotecas '''\n",
    "\n",
    "! pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H33J_e9Ky4ZP"
   },
   "source": [
    "# Etapa 2: Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78TCUzB9rpjp"
   },
   "outputs": [],
   "source": [
    "''' Carregar os dados '''\n",
    "\n",
    "# dataset_url = 'https://www.kaggle.com/datasets/arjunbhasin2013/ccdata'\n",
    "path = 'Files/Projeto/CC GENERAL.csv'\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "error",
     "timestamp": 1751234313892,
     "user": {
      "displayName": "João Vitor Fernandes Dias",
      "userId": "17773602428702936204"
     },
     "user_tz": 180
    },
    "id": "kLi--QNKsoC2",
    "outputId": "d5dbe759-e8f8-4ae3-a597-53b2d1847fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: fg: no job control\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3-2310048475.py:12: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3-2310048475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load the latest version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m df = kagglehub.load_dataset(\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mKaggleDatasetAdapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPANDAS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;34m\"arjunbhasin2013/ccdata\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;34m\"load_dataset is deprecated and will be removed in a future version.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     )\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mdataset_load\u001b[0;34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs, polars_frame_type, polars_kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             return kagglehub.pandas_datasets.load_pandas_dataset(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/pandas_datasets.py\u001b[0m in \u001b[0;36mload_pandas_dataset\u001b[0;34m(handle, path, pandas_kwargs, sql_query)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mpandas_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpandas_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpandas_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mfile_extension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mread_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_read_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_extension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Now that everything has been validated, we can start downloading and processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglehub/pandas_datasets.py\u001b[0m in \u001b[0;36m_validate_read_function\u001b[0;34m(file_extension, sql_query)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;34mf\"Supported file extensions are: {', '.join(SUPPORTED_READ_FUNCTIONS_BY_EXTENSION.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         )\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextension_error_message\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mread_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSUPPORTED_READ_FUNCTIONS_BY_EXTENSION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_extension\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt"
     ]
    }
   ],
   "source": [
    "''' Testing '''\n",
    "\n",
    "# Install dependencies as needed:\n",
    "\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"CC GENERAL.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"arjunbhasin2013/ccdata\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like\n",
    "  # sql_query or pandas_kwargs. See the\n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlnzR8QWy8_m"
   },
   "source": [
    "## Etapa 2.1: pre-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFVk2pk2zDUo"
   },
   "source": [
    "### Etapa 2.1.1: Remover valores ausentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsRhtLE0zOTx"
   },
   "source": [
    "### Etapa 2.1.2: Remover colunas que não serão usadas diretamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA_LkO7zzU9H"
   },
   "source": [
    "### Etapa 2.1.3: Codificar variáveis categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnr4pjxtzbqr"
   },
   "source": [
    "### Executar"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPF4AYGUY8G1iPkSramNoGC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
