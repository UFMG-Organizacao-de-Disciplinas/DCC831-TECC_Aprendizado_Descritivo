# Aprendizado Descritivo - Renato Vimieiro - 2025

## Aula 01 | 18/03/2025 | Apresenta√ß√£o do curso - [JV: Cheguei atrasado]

### Slide 1

#### Introdu√ß√£o

...

#### Aprendizado Preditivo

- O que desejamos √© receber informa√ß√µes e conseguir retornar um r√≥tulo.
- Nem todos de aprendizado de m√°quina levam em considera√ß√£o os r√≥tulos.
- Aprendizado supervisionado preditivo
- [JV: n√£o anotei sobre o que seria o supervisionado]
- Se tiver que explicar como funciona, a√≠ √© a √°rea de Explanable AI.
- Aqui √© mais importante o resultado do que o processo.

---

- Queremos um modelo que seja fiel √† realidade.

---

[(Tratamento)] --> Contru√ß√£o --> Avalia√ß√£o --> Uso (Modelo)
Avalia√ß√£o --> [(Valida√ß√£o)]

```mermaid
flowchart TD
  Trat[(Tratamento)]
  Cons[Constru√ß√£o]
  Ava[Avalia√ß√£o]
  Uso[Uso (Modelo)]
  Val[(Valida√ß√£o)]
  Trat --> Cons
  Cons --> Ava
  Ava --> Uso
  Ava --> Val
```

- Aqui, n√£o queremos obter nenhum entendimento sobre o que j√° vimos antes, mas sim, ter meios de prever como ser√£o classificados os pr√≥ximos itens que veremos.

---

- Exemplo: c√¢ncer de mama
  - Separando mulheres cuja quimioterapia foram eficazes e as que n√£o foram.
  - Analisar quais os mapeamentos gen√©ticos delas
  - Verificar de que forma h√° uma rela√ß√£o entre os mapeamentos gen√©ticos e a efic√°cia da quimioterapia.
  - E com isso, tentar predizer se a quimioterapia ser√° eficaz ou n√£o para uma nova paciente.
- No aprendizado n√£o supervisionado n√£o h√° r√≥tulos.
- Uma tarefa de aprendizado n√£o-supervisionado bastante popular √© a de agrupamento (clustering)
  - Um dos mais conhecidos √© o k-menas

#### Aprendizado Descritivo

- > A premissa √© "eu n√£o sei nada sobre os dados", "ent√£o preciso encontrar um modelo que descreva os dados"
- "Estat√≠stica n√£o √© boa para descrever grafos"
- O Descritivo √© t√£o importante quanto prever coisas, mas atuam em momentos diferentes.

---

- O aprendizado descritivo leva a descoberta en ovos conhecimentos. Estando entre minera√ß√£o de dados e aprendizado de m√°quina.
- Busca responder "o qu√™ aconteceu?"
- Descrevem situa√ß√µes passadas e com isso auxiliam no processo de **tomada de decis√£o**.

- 4 paradigmas cient√≠ficos
  - Indu√ß√£o, te√≥rico, ...
- Antes viam o fen√¥menos, criavam teorias, tentavam provar que as teorias se aplicavam.
- Atualmente usamos um modelo baseado em dados
  - Veem como os dados se comportas, criam teorias e tentam provar que os dados se comportam de acordo com a teoria.

---

- Uma das coisas feitas nessa disciplina √© a busca por regularidades de acontecimentos em conjuntos.
- H√° uma interse√ß√£o bem grande entre aprendizado descritivo e Minera√ß√£o de Dados.

---

- > What Wal-Mart knows about Customers' Habits
- Eles avaliaram de que forma os usu√°rios se comportavam em rela√ß√£o a desastres naturais e o que compravam nas cidades que tavam para ser afetadas.
  - A decis√£o trivial era considerar que faria sentido estocar pilha, √°gua mineral, lanterna e produtos n√£o espec√≠ficos.
  - Eles descobriram que houve um aumento de 7x nas vendas de Pop Tarts sabor morando, e o campe√£o dos aumentos foi a cerveja.
  - Nessa situa√ß√£o eles mais queriam descrever o passado do que predizer o futuro.
- Embora seja abordado como preditivo, na pr√°tica seria um exemplo de aprendizado descritivo supervisionado.
- Exceptional Model Mining
  - Busca-se encontrar quais conjuntos de itens s√£o n√£o-usualmente comprados qunado algum evento ocorre.

D√∫vida: Como fazer para discernimos se um aumento, como no caso do Pop Tarts foi de fato devido aos furac√µes ou se calhou de, nesses dois mesmos intervalos de tempo, foram ve√≠culados an√∫ncios desse produto; sendo ent√£o apenas uma coincid√™ncia?

Resposta: D√° para tentar refinar a forma de an√°lise e o c√°lculo da fun√ß√£o objetivo. Por√©m, devido ao car√°ter qualitativo, √© dif√≠cil de se ter certeza de que essa atipicidade nessa busca por padr√µes at√≠picos sejam separados.

#### Aprendizado Descritivo X Preditivo

...

---

...

---

No modelo Preditivo, tenta-se definir limites para que pr√≥ximos itens sejam classificados de acordo com o que foi visto anteriormente.

J√° no Descritivo, tenta-se encontrar padr√µes que descrevam o que foi visto anteriormente.

---

No exemplo apontado pode-se separar as distribui√ß√µes dos pontos em 4 quadrantes, isso baseado na estimativa do que j√° ocorreu antes, busca ent√£o estimar onde estar√£o posicionados os quadradinhos azuis e as bolinhas vermelhas.

√â importante tamb√©m identificar quais s√£o as regularidades existentes em certos padr√µes irregulares.

---

√Äs vezes usa-se o mesmo modelo entre preditivo e descritivo, por√©m, um pra descrever e o outro pra predizer.

---

#### Coment√°rios sobre o curso

Nesse curso busca-se a parte teoria dos algoritmos, n√£o necessariamente em sua aplica√ß√£o.

Ela √© te√≥rica, densa em algoritmo, e a aplica√ß√£o em c√≥digo √© m√≠nimo.

Busca-se "botar uma lupa" sobre a descoberta de padr√µes.

A primeira parte ser√° toda n√£o-supervisionada.

A busca de padr√µes em grafos pode ser usada na √°rea de f√°rmacos para encontrar quais sub-estruturas s√£o as mais frequentes em determinados rem√©dios para determinada infermidade?

A segunda parte ser√° de aprendizado supervisionado.

Por volta de 20 de maio tem uma prova.

Haver√° um tipo de "roleplaying" das atividades. Ele separou as salas em 8 grupos. Um grupo era o "historiador" (buscava entender qual era o contexto), o outro era o "metodologista" (tentatva entender como o algoritmo funcionava), "Aplica√ß√µes", "Coletar as apresenta√ß√µes e redigir", "Publicar o resumo em um site da turma". O "hacker" √© quem busca os c√≥digos existentes, tenta entender, fazer funcionar e documentar como fez funcionar.

"Daqui para baixo √© a parte mais recente, talvez mais p√≥s gradua√ß√£o, ou coisas que n√£o est√£o nos livros".

Cada grupo vai rotacionar em cada uma das tarefas. Ser√£o 9 artigos no total que leremos.

Os Semin√°rios (aplica√ß√µes), veremos de fato aplica√ß√µes

O que ele quer com o projeto? Uma intera√ß√£o maior com o professor. A parte mais pr√°tica da disciplina.

Na parte de ... ser√° o ... que foi quem inventou.

Na parte de supervisionado: Sebastian Ventura e Jos√© Maria Luna 2018; Guozhu Dong and James Bailey 2012;

## Aula 02 | 20/03/2025 | Aprendizado descritivo x preditivo

### Slide - Aprendizado Descritivo

#### Introdu√ß√£o - Aula 2

- Minera√ß√£o de itens alguma coisa

---

- Kosinski (2013) coletavam dados das personalidades atrav√©s do MyPersonality.
  - Esc√¢ndalo do Cambridge Analytica
- Buscava predizer a personalidade baseado nas curtidas feitas no Facebook

---

- Provost e Foster (2013) utilizaram os dados para demonstrar a modelagem descritiva e as informa√ß√µes √∫teis.
- Alguns exemplos de regras:
  - Selena Gomez -> Demi Lovato
  - Linking Park & Disturbed & System of a Down & Korn -> Slipknot
  - SpongeBob SquarePants & Converse [JV: empresa do All Stars] -> Patrick Star
  - Skittles & Mountain Dew -> Gatorade

[JV: Offtopic: o diretor da Google daqui fez doutorado no PPGCC]

---

- A ideia de itens em cesta de compra pode ser generalizada para itens virtuais
- Busca-se encontrar co-ocorr√™ncias de itens de an√°lise
- Dados os limites √©ticos, pode-se usar essa an√°lise para se atingir diversos objetivos.

##### Itemset e Tidsets

- Os "produtos" da cesta de compras s√£o chamados de "Itens".
  - $I = {x_1, x_2, \dots, x_m}$
- Esses elementos ser√£o as **vari√°veis de an√°lise**
- Um conjunto $X \subseteq I$ √© chamado de **Itemset**
- Um itemset de tamanho $k$ √© chamado de **k-itemset**
- Denotamos o conjunto de todos os **k-itemsets** por $I^(k)$
- Todas as "Transa√ß√µes" ser√£o identificadas por IDs, ou ent√£o, **TID** (Transaction ID)
- O conjunto $T = {t_1, t_2, \dots, t_n}$ √© o conjunto das transa√ß√µes.

---

- O Conjunto $Y \subseteq T$ √© chamado de **Tidset**
- √â v√°lido assumir que _itemsets_ e _tidsets_ s√£o ordenados por ordem lexicogr√°fica dos itens e transa√ß√µes. (N√£o importa qual ordem, mas est√£o de algum modo ordenados)
- Cada transa√ß√£o consiste de um identificador (TID) e um conjunto de itens
  - Ent√£o, cada transa√ß√£o √© um par $(t, X)$ em que $t \in T$ e $X \subseteq I$
- Formalmente, um conjunto de dados ser√° uma tripla $(T, I, D)$
  - T: Transa√ß√µes ou objetos
  - I: Atributos ou itens
  - D: Rela√ß√£o bin√°ria entre eles.
  - $T$ e $I$ s√£o os conjuntos de tids e itens
  - $D \subsetq T \times I$ √© a rela√ß√£o bin√°ria em que $(t, i) \in D$...

---

- Podemos estender a defini√ß√£o tamb√©m para conjuntos de itens
- Dizemos que $t$ cont√©m um itemset X sse $\forall i \in X (t, i) \in D$

Ou seja: $t$ cont√©m $X$ se $|X - t| = 0$

---

- Dado um itemset $X$, podemos querer saber o conjunto de transa√ß√µes que o cont√©m.
- Esse conjunto √© chamado de **extens√£o** ou **cobertura** de $X$
- Ele √© definido pela seguinte fun√ß√£o:
  - $c: P(I) \to P(T)$
  - $c(X) = {t \in T | \forall i \in X(t, i) \in D}$
- Dado um tidset Y, podemos querer saber o maior conjunto de itens comuns √†s transa√ß√µes de Y.
- Esse conjunto √© chamado de **intens√£o** (N√£o √© inten√ß√£o!) de Y.
- Ele √© definido por
  - $i: P(T) \to P(I)$
  - $i(Y) = {x \in I | \forall t \in Y(t, x) \in D}$

O uso de extens√£o e intens√£o v√™m da ideia filos√≥fica e semi√≥tica de que a extens√£o √© o conjunto de coisas que se encaixam em uma defini√ß√£o, enquanto a intens√£o √© a defini√ß√£o em si.

---

|  TID | Muesli | Oats | Milk | Yoghut | BIscuits |  Tea |
| ---: | -----: | ---: | ---: | -----: | -------: | ---: |
|    1 |      1 |    0 |      |        |          |    1 |
|    2 |      0 |    1 |      |        |          |    0 |
|    3 |      0 |    0 |      |        |          |    1 |
|    4 |      1 |    0 |      |        |          |    0 |
|    5 |      0 |    1 |      |        |          |    1 |
|    6 |      1 |    0 |      |        |          |    1 |

Intens√£o: conjunto de itens comuns a todos os elementos de um determinado conjunto de transa√ß√µes.

Extens√£o: o conjunto de transa√ß√µes que contenham um determinado conjunto de itens.

Intens√£o: a interse√ß√£o das linhas
Extens√£o: a interse√ß√£o das colunas

##### Representa√ß√µes de conjuntos de dados

- Podemos enxergar o conjunto de dados como um conjunto de transa√ß√µes e suas respectivas intens√µes
  - Conjunto de $(t, i(t))$
  - Essa representa√ß√£o √© chamada de **Horizontal**
- Similarmente podemos enxergar o conjunto de dados como um conjunto de itens e suas cobrerturas
  - Conjunto de $(x, c(x))$
  - Essa representa√ß√£o √© chamada de **Vertical**

---

- Horizontal

|    t |                       i(t) |
| ---: | -------------------------: |
|    1 | Muesli, Milk, Yoghurt, Tea |
|    2 |                 Oats, Milk |
|    3 |        Milk, Biscuits, Tea |
|    4 |            Muesli, Yoghurt |
|    5 |            Oats, Milk, Tea |
|    6 |          Muesli, Milk, Tea |

- Vertical

|    x |  Muesli | Oats |          Milk | Yoghurt | Biscuits |        Tea |
| ---: | ------: | ---: | ------------: | ------: | -------: | ---------: |
| t(x) | 1, 4, 6 | 2, 5 | 1, 2, 3, 5, 6 |    1, 4 |        3 | 1, 3, 5, 6 |

|        x |          t(x) |
| -------: | ------------: |
|   Muesli |       1, 4, 6 |
|     Oats |          2, 5 |
|     Milk | 1, 2, 3, 5, 6 |
|  Yoghurt |          1, 4 |
| Biscuits |             3 |
|      Tea |    1, 3, 5, 6 |

##### Conjuntos de itens frequentes e Regras de Associa√ß√£o

- A identifica√ß√£o de regras tais como as que vimos no exemplo no in√≠cio da aula, em geral, envolvem duas etapas
  - Minera√ß√£o de conjuntos de itens frequentes
  - Descoberta de regras de associa√ß√µes interessantes
- A primeira tende a ser computacionalmente mais intensa. Por este motivo recebe mais aten√ß√£o dos pesquisadores.
- Nos concentraremos nessa tarefa.

##### Minera√ß√£o de Conjuntos de itens frequentes

- Uma defini√ß√£o de "regra interessante" √© ela ocorrer com certa frequ√™ncia.
- Ent√£o √© necess√°rio definir um limiar entre o que √© frequente e o que √© infrequente
  - Esse limiar √© chamado de suporte m√≠nimo (minsup)
- O suporte de um itemset √© o tamanho de sua cobertura
  - sup(X) = |c(X)|
- Como essa defini√ß√£o √© dependente do contexto, admite-se tamb√©m a defini√ß√£o do suporte relativo
  - rsup(X) = |c(X)| / |T|

---

Dessa forma, dizemos que um itemset √© frequente sse $sup(X) \geq minsup$

---

Rela√ß√£o de ordem parcial: $X \subseteq Y \leftrightarrow X \leq Y$

Conjunto pot√™ncia: √© o conjunto de todos os poss√≠veis subconjuntos de um conjunto.

```mermaid
flowchart TD
  null[["\null"]]
  null --> A & B & C
  A --> AB & AC
  B --> AB & BC
  C --> AC & BC
```

- O espa√ßo de busca do problema √© o conjunto pot√™ncia do conjunto de itens
- Se considerarmos a rela√ß√£o de subconjuntos como uma rela√ß√£o de ordem parcial, temos que o espa√ßo de busca √© estruturado como um reticulado
  - Esse reticulado pode ser visualizado como um grafo, onde somente as rela√ß√µes diretas s√£o representadas
  - Ou seja, se $A \subseteq B \land |A| = |B| - 1$, ent√£o existe uma aresta entre A e B no diagrama

Aquele diagrama explica bastante o que que isso quis dizer. Basicamente, no conjunto pot√™ncia, cada n√≠vel vai ter um elemento a mais que o n√≠vel anterior.

- ...

###### Algoritmo Ing√™nuo

Complexidade do algoritmo: $O(2^I \cdot T \cdot I)$

- // ALGORITHM 8.1. Algoritm BruteForce
- int BruteForce(D, I, minsup):
  - F \leftarrow // set of frequent itemsets
    - for each X \subseteq I do
      - sup(X) \leftarrow ComputeSupport (X, D)
      - if sup(X) \leq minsup then
        - F \leftarrow F \cup {(X, sup(X))}
    - return F

- ComputeSupport(X, D):
  - sup(X) \leftarrow 0
  - for each (t, i(t)) \in D do
    - if X \subseteq i(t) then
      - sup(X) \leftarrow sup(X) + 1
- return sup(X)

---

...

---

- A complexidade do espa√ßo de busca √© inerente ao problema. Contudo o algoritmo √© ineficiente mesmo em espa√ßos pequenos
- Note que o conjunto de dados n√£o √© mantido em mem√≥ria, portanto a computa√ß√£o do suporte torna o algoritmo impratic√°vel
- Os algoritmos mais "sofisticados" atacam majoritariamente o problema de computa√ß√£o de suporte, evitando computa√ß√µes desnecess√°ris, e/ou adotando estrat√©gias mais eficientes para comput√°-lo.

Ent√£o temos dois problemas principais: reduzir o espa√ßo de busca e reduzir a complexidade para calcular o suporte.

##### Leitura

- Se√ß√µes 8.1 e 8.2 Zaki e Meira
- Se√ß√µes 6.1 e 5.2 (Introduction to Data Mining)

## Aula 03 | 25/03/2025 | Minera√ß√£o de conjuntos de itens - Faltei - Minera√ß√£o de itens frequentes: Apriori e Eclat

### Slide: aula03-apriori_eclat (Aula 03)

#### Introdu√ß√£o (Aula 03)

- Como vimos na aula anterior, o principal problema do algoritmo ing√™nuo para minera√ß√£o de conjuntos de itens frequentes era a replica√ß√£o de esfor√ßos para avaliar o suporte dos candidatos
- As m√∫ltiplas passadas no conjunto de dados (armazenado em mem√≥ria secund√°ria) torna o algoritmo impratic√°vel at√© mesmo para pequenos volumes
- Os algoritmos que veremos hoje exploram propriedades do problema para amortizar o custo da computa√ß√£o de suporte, e evitar retrabalho na avalia√ß√£o dos candidatos

---
---

O que √© mesmo o suporte? ü§î

- Recapitulando da aula anterior, o Suporte aparantemente √© um encurtamento para o "Suporte M√≠nimo" (minsup) que √© o limiar que define se determinado item √© frequente o bastante ou n√£o.
  - Esse valor √© dado pela seguinte f√≥rmula:
    - $sup(X) = |c(X)|$, onde $c(X)$ √© a cobertura do itemset X.

Mas o que √© mesmo a cobertura?

- A cobertura √© o conjunto de transa√ß√µes que cont√©m um itemset X. Ou seja, √© o conjunto de transa√ß√µes que cont√©m todos os itens do itemset X.

#### Apriori [1]

- O Apriori foi proposto por Rakesh Agrawal e Ramakrishnan Srikant em 1994
  - O artigo possui mais de 30K cita√ß√µes
- Os autores √† √©poca trabalhavam no projeto da IBM para o Wal-Mart
- A ideia central √© evitar computa√ß√µes desnecess√°rias para candidatos infrequentes
- Isso √© viabilizado pela propriedade de **anti-monotonicidade** da fun√ß√£o suporte
- Essa √© uma das propriedades mais importantes para a √°rea

##### Anti-monotonicidade do suporte

- Considere dois itemsets $A$ e $B$ quaisquer. Se $A \subseteq B$, ent√£o $sup(A) \geq sup(B)$.
- Essa observa√ß√£o nos diz que a cobertura de conjuntos de itens √©, no m√°ximo, t√£o grande quanto a de seus subconjuntos
  - No caso mais simples, um conjunto de dois itens n√£o pode ocorrer em mais transa√ß√µes que cada um dos itens individualmente
- Consequentemente, se o itemset A √© infrequente, B tamb√©m ser√°.
- Isso define a propriedade de anti-monotonicidade da fun√ß√£o suporte, tamb√©m conhecida como a propriedade do Apriori
  - **Todo superconjunto de um conjunto infrequente √© infrequente**
  - **Todo subconjunto de um conjunto frequente √© frequente**

---
---

Muito interessante isso da√≠ de cima.

Basicamente entendemos que $sup(X=\{A, B\}) \geq sup(Y=\{A, B, C\})$ com isso, se X √© frequente, nada garante que Y tamb√©m seja. Por√©m, se Y √© frequente, isso garante que todos os poss√≠veis subconjuntos de Y tamb√©m ser√£o frequentes.

#### Apriori [2]

- O Apriori utiliza uma busca em largura no espa√ßo de busca para minerar os padr√µes
  - Frequentemente, o termo usado na literatura √© abordagem por n√≠veis (level-wise approach)
- A busca inicia com a identifica√ß√£o dos itens frequentes
- Depois, os conjuntos de tamanho k s√£o explorados antes dos de tamanho k+1
- Assim como o algoritmo ing√™nuo, ele tamb√©m opera em duas etapas:
  - Gera√ß√£o de candidatos
  - C√¥mputo do suporte e elimina√ß√£o dos infrequentes

---

- A gera√ß√£o dos candidatos √© feita a partir dos conjuntos frequentes encontrados na fase anterior
- Conjuntos compartilhando um prefixo de k-1 itens s√£o combinados para gerar candidatos de tamanho k+1
  - Novamente, assume-se que eles s√£o ordenados pela ordem lexicogr√°fica
- Candidatos que possuam algum subconjunto infrequente s√£o descartados imediatamente
  - A propriedade do Apriori √© empregada
- Os suportes dos candidatos s√£o atualizados com uma √∫nica passada no conjunto de dados
  - Subconjuntos de tamanho k de cada transa√ß√£o s√£o usados para atualizar o suporte dos candidatos

---

- **APRIORI** $(D, \mathcal{I}, minsup)$:
  - $\mathcal{F} \leftarrow \emptyset$
  - $\mathcal{C}^{(1)} \leftarrow \{\emptyset\}$ // Initial prefix tree with single items
  - **foreach** $i \in \mathcal{I}$ **do** Add $i$ as child of $\emptyset$ in $\mathcal{C}^{(1)}$ with $sup(i) \leftarrow 0$
  - $k \leftarrow 1$ // $k$ denotes the level
  - **while** $\mathcal{C}^{(k)} \neq \emptyset$ **do**
    - ComputeSupport $(\mathcal{C}^{(k)}, D)$
    - **foreach** _leaf_ $X \in \mathcal{C}^{(k)}$ **do**
      - **if** $sup(X) \geq minsup$ **then** $\mathcal{F} \leftarrow \mathcal{F} \cup \{(X, sup(X))\}$
      - **else** remove $X$ from $\mathcal{C}^{(k)}$
    - $\mathcal{C}^{(k+1)} \leftarrow$ ExtendPRefixTree($\mathcal{C}^{(k)}$)
    - $k \leftarrow k+1$
  - **return** $\mathcal{F}^{(k)}$

---

- ComputeSupport $(C^{(k)}, D)$:
  - ...

- ExtendPrefixTree $()$:
  - ...
  - **return** $C^{(k)}$

---

Exemplo (minsup=3):

$$
\begin{bmatrix}
  TID & Muesli & Oats & Milk & Yoghurt & Biscuits & Tea \\
  1 & 1 & 0 & 1 & 1 & 0 & 1 \\
  2 & 0 & 1 & 1 & 0 & 0 & 0 \\
  3 & 0 & 0 & 1 & 0 & 1 & 1 \\
  4 & 1 & 0 & 0 & 1 & 0 & 0 \\
  5 & 0 & 1 & 1 & 0 & 0 & 1 \\
  6 & 1 & 0 & 1 & 0 & 0 & 1 \\
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
  TID & Muesli & Milk & Tea \\
  1 & 1 & 1 & 1 \\
  2 & 0 & 1 & 0 \\
  3 & 0 & 1 & 1 \\
  4 & 1 & 0 & 0 \\
  5 & 0 & 1 & 1 \\
  6 & 1 & 1 & 1 \\
\end{bmatrix}
$$

---

- O n√∫mero de passadas √© drasticamente reduzido em rela√ß√£o ao algoritmo ing√™nuo
  - $O(2^I) \rightarrow O(I)$
- As podas baseadas na anti-monotonicidade do suporte tamb√©m s√£o bastante efetivas na pr√°tica
- O algoritmo tamb√©m apresenta alguns problemas:
  - Busca em largura requer que todos os candidatos de um n√≠vel sejam mantidos em mem√≥ria. Esse custo √© proibitivo em alguns (muitos) casos
  - Tanto a contagem do suporte quanto a poda do Apriori podem ser consideravelmente caras, dependendo da implementa√ß√£o

---

- O custo de mem√≥ria √© inerente √† abordagem, e n√£o podemos fazer muita coisa para melhor√°-lo
- O custo da contagem e verifica√ß√£o pode ser atenuado, usando estruturas de dados mais ‚Äòsofisticadas‚Äô
- Existem duas abordagens mais comuns:
  - Usar uma √°rvore hash
  - Usar uma √°rvore de prefixos (Trie)
- Na primeira abordagem, cada n√≥ folha armazena um conjunto de candidatos/conjuntos frequentes
  - O n√∫mero de compara√ß√µes √© reduzido
- Na segunda abordagem, os n√≥s da Trie armazenam os candidatos/conjuntos frequentes. Os subconjuntos das transa√ß√µes s√£o usadas para index√°-la e atualizar o suporte

---

- A redu√ß√£o do suporte m√≠nimo tem um impacto muito grande no custo computacional do algoritmo
  - O tamanho dos candidatos aumenta -> Mais candidatos s√£o avaliados em cada n√≠vel -> o tamanho dos conjuntos frequentes aumenta -> mais n√≠veis s√£o explorados

---

- A densidade da base de dados tamb√©m tem muito impacto no custo
  - Transa√ß√µes passam a ter mais itens
  - Isso tem duas implica√ß√µes: tamanho m√©dio dos itemsets aumentam; mais subconjuntos s√£o gerados durante a contagem do suporte $\binom{|t|}{k}$

## Aula 04 | 27/03/2025 | Minera√ß√£o de conjuntos de itens

### Aula passada

- Algoritmo apriori
- Frequente, infrequente.
- Como calcula o suporte
  - A partir dos itens frequentes: tabelas de 0 e 1.
  - Pra isso usaca √°rvore de ???
  - Para cada transa√ß√£o gerava os itemsets de tamanho k, ia na √°rvore ???
  - E incrementava o suporte daquela chave

[JV: escrevi o que ele t√° falando, mas n√£o t√¥ entendendo]

- Duas coisas influenciam o desempenho do algoritmo
  1. Ele falou algo
  2. Se o BD √© denso, as transa√ß√µes s√£o mais largas.
- $\binom{|t|}{k}$
- Quando √© esparso, funciona bem. Quando √© denso que come√ßa a dar problema.

Eu t√¥ achando que se eu compro $J = {A, B, C}$, Ent√£o o conjunto pot√™ncia dele √© $P(J) = {\emptyset, A, B, C, AB, AC, BC, ABC}$, e ent√£o, incrementaria 1 para um desses grupos

- C√°lculo de suporte:
  - Para cada um dos itemsets tem que verificar se ele t√° na √°rvore K(?)

- Se os itemsets est√£o em mem√≥ria...
- Se quero gerar o itemset XY partindo de $X \cup Y$, posso dizer que o suporte ser√° $|c(X) \cap c(Y)|$

### Slide: aula03-apriori_eclat (Aula 04)

#### Eclat (Equivalence Class Transformation)

Dada a representa√ß√£o vertical dos dados, consigo calcular o suporte por essa intercess√£o.

- Dadas as defici√™ncias do Apriori, M. Zaki prop√¥s, em 2000, o algoritmo Equivalence Class Transformation (Eclat)
- A proposta do algoritmo √© ‚Äòeliminar‚Äô a necessidade de passadas no conjunto de dados para computar o suporte
- Para isso, ele parte de uma representa√ß√£o vertical dos dados, e se baseia no fato de que a cobertura da uni√£o de dois itemsets √© a interse√ß√£o de suas coberturas

Problema: como mantenho todos os itemsets gerados em mem√≥ria?

---

- Ou seja, a ideia central do algoritmo tentar manter os tidsets em mem√≥ria principal para computar o suporte dos itemsets atrav√©s de interse√ß√µes desses conjuntos
- Contudo, todos os tidsets podem n√£o caber na mem√≥ria principal. Assim, √© necess√°rio algum mecanismo que possibilite a divis√£o do espa√ßo de busca em subproblemas independentes que caibam na mem√≥ria
- A divis√£o √© feita conforme uma rela√ß√£o de equival√™ncia estabelecida sobre os candidatos

Tenta manter tudo na mem√≥ria principal

A ideia √© partir o problema em subproblemas e trazer esses subproblemas pra mem√≥ria.

Surgiu atrav√©s da cria√ß√£o de uma rela√ß√£o de equival√™ncia entre os itemsets

---

Cria-se uma rela√ß√£o de equival√™ncia pelos prefixos.

Diz-se que dois itemsets s√£o equivalentes se o prefixos dos dois s√£o iguais.

Consideremos que temos o seguinte conjunto pot√™ncia: $P(I) = {\emptyset, A, B, C, AB, AC, BC, ABC}$. Na forma de representa√ß√£o, seria como se agrup√°ssemos os dados em grupos de prefixos:

- A: {A, AB, AC, ABC}
- B: {B, BC}
- C: {C}

E ent√£o seriam varridos de C para A.

Poderia-se tamb√©m fazer subgrupos de subgrupos, dependendo do tamanho do conjunto de prefixos.

---

Ele faz uma busca em profundidade (DFS)

Ele faz subparti√ß√µes at√© que o n√∫mero de transa√ß√µes seja pequeno o suficiente para caber na mem√≥ria.

---

- Algoritmo 8.3 - Algoritmo ECLAT
- // Initial Call: $F \leftarrow 0, P \leftarrow {}$
- ECLAT (P, minsup, F):
- foreach
  - F
  - P0
  - foreach
    - X
    - ?
    - if sup()
      - po
  - if p neq 0 then ...

[JV: Droga, foquei em transcrever brevemente e esqueci de prestar aten√ß√£o na explica√ß√£o do professor]

P guarda todos os frequentes da chamada anterior. porque ele filtro toudos que s√£o infrequentes pelo minsup

para cada um dos frequentes dos candidatos, armazena no ocnjunto de itens frequentes globais

e a partir dele gera em profundidade a combina√ß√£o dele com todos os outros que v√™m pra frente.

Evitam redund√¢ncia: 1. parti√ß√µes; 2. Ordem sistem√°tica de combina√ß√£o dos itens.

1. A B C
2. A com B e C: AB AC
3. AB com AC: ABC
4. B com C: BC

##### Representa√ß√µes de conjuntos de dados - Aula 4

Pelo que eu t√¥ entendendo:

1. Come√ßa pegando todos os itens que tenham uma quantidade de transa√ß√µes maior que o minsup (o valor m√≠nimo aceit√°vel para que consideremos relevante)
2. Depois disso, come√ßamos fazendo a intercess√£o das transa√ß√µes entre o primeiro conjunto de itens que passou pela compara√ß√£o com o segundo conjunto.
3. Depois disso, v√™ se o resultado dessas intercess√µes √© grande o bastante.

Se $A \subseteq B$, ent√£o $c(B) \subseteq c(A)$ (Cobertura)

---

##### Diffsets e dEclat

Em bases de dados densos, varia bem pouco o suporte entre os itens. Ent√£o, faria mais sentido guardar s√≥ a diferen√ßa ao inv√©s de guardar o todo.

Ao inv√©s de chamar de tidset, passaram a chamar de diffset.

...

Pode-se armazenar em vetores de bits ao inv√©s de vetores de inteiros.

Usando o vetor de bits, √© como se fosse:

A = [00110, 01001, 01100, 00011]

E para calcular o suporte (?) cobetura(?)

basta fazer um c√°lculo r√°pido de 0 a 255 para dizer quantos bits est√£o ativos, e ent√£o fazer a contagem de bits ativos somando esses valores.

0 -> 1
1 -> 1
2 -> 1
...
255 -> ...

Outra forma de condensar √©: Se sei que um determinado conjunto √© grande o bastante, posso inferir que todos os que s√£o menores que eles tamb√©m s√£o grandes o bastante.

Se s√≥ √© guardado o valor das diferen√ßas, acaba sendo um problema fazer as intercess√µes.

---

- $C(PX) - C(PY)$
- $C(PX) - C(PY) \cup C(P) - C(P)$
- $C(PX) \cap \overline{C(PY)} \cup C(P) \cap \overline{C(P)}$
- $C(PX) \cup \overline{C(P)} \cap C(P) \cup \overline{C(P)}$

## Aula 05 | 01/04/2025 | Minera√ß√£o de conjuntos de itens

### Slide: aula03-apriori_eclat (Aula 05)

#### Diffsets e dEclat [Aula 05]

- Percebendo o problema de se manter os tidsets em mem√≥ria, Zaki e Gouda propuseram em 2001 (o artigo s√≥ foi publicado em 2003) uma solu√ß√£o alternativa
- Eles propuseram armazenar as diferen√ßas entre os tidsets dos membros de uma classe e dos prefixos que a definem
- Eles chamaram esse conjunto de **diffset**
- Formalmente, para um prefixo $P$ e um itemset $PX$, o diffset de $X$, $d(PX) = c(P) - c(PX)$
- Seriam armazenados, portanto, o suporte do itemset e seu diffset

---

- O fato √© que, se somente os diffsets s√£o armazenados, o suporte n√£o √© mais obtido como a cardinalidade desse conjunto
- Dessa forma, como calcular o suporte de um itemset $PXY$ obtido a partir de outros dois $PX$ e $PY$, usando somente seus diffsets?
- O suporte de $PX$ √© calculado pela diferen√ßa entre o suporte de $P$ e o diffset de $PX$, $sup(PX) = sup(PX) - |d(PXY)|$
  - Portanto, $sup(PXY) = sup(PX) - |d(PXY)|$
- A solu√ß√£o passa por computar o diffset de $PXY$. Por√©m, temos somente os diffsets de $PX$ e $PY$

---

- Por defini√ß√£o, ùëë ùëÉùëãùëå = ùëê ùëÉùëã ‚àí ùëê ùëÉùëãùëå = ùëê ùëÉùëã ‚àí ùëê ùëÉùëå
- Podemos adicionar, ao conjunto acima, o conjunto vazio (ùëê ùëÉ ‚àí ùëê ùëÉ ) sem alter√°-lo
- Logo, ùëë ùëÉùëãùëå = ùëê ùëÉùëã ‚àí ùëê ùëÉùëå + ùëê ùëÉ ‚àí ùëê ùëÉ = 3 4 ùëê ùëÉ ‚àí ùëê ùëÉùëå ‚àí ùëê ùëÉ ‚àí ùëê ùëÉùëã = ùëë ùëÉùëå ‚àí ùëë(ùëÉùëã)
- Em outras palavras, podemos usar os diffsets dos conjuntos base para calcular o diffset do novo candidato
- A variante do Eclat que usa diffsets ficou conhecida como dEclat

---
---

- $d(PXY) = c(PX) - c(PY) = c(PX) - c(PXY)$
- $c(PXY) = c(PX) \cap c(PY)$

"Diferen√ßa √© a mesma coisa que interse√ß√£o com complemento"

- $d(PXY) = ...$

Ele fez um monte de igualdades com opera√ß√µes de conjuntos.

- $d(PXY) =$
- $c(PX) - c(PY) =$
- $c(PX) - c(PXY) =$
- $[c(PX) - c(PXY)] \cup [c(P) - c(P)] =$
- $[c(PX) \cap \overline{c(PXY)}] \cup [c(P) \cap \overline{c(P)}]$
- $[c(PX) \cup c(P)] \cap [\overline{c(PY)} \cup \overline{c(P)}] \cap \overline{d(PX)}$
- $(c(PX) \cap \overline{c(PY)}) \cup (c(P) \cap \overline{c(P)}) \cup (c(P) \cap \overline{c(PY)}) \cup (c(P) \cap \overline{c(P)} \cap \overline{d(PX)})$
- $(Q \subseteq d(PY)) \cup (\emptyset) \cup (d(PY)) \cup (\emptyset) \cap (\overline{d(PX)})$
- $d(PY) \cap \overline{d(PX)} =$
- $d(PY) - d(PX)$

---

- $d(PX) = ...$

---

- $P = \{1, 2, 3, 4, 5\}$
- $X = \{1, 3, 5\}$
- $Y = \{2, 3, 4\}$
- $PX = \{1, 3, 5\}$
- $PY = \{2, 3, 4\}$
- $\overline{PY} = \{1, 5\}$
- $PX \cap \overline{PY} = \{1, 5\}$

---

- ALGORITHM 8.4. Algorithm dEclat
  - ...

---

- Essa abordagem se mostrou muito eficiente para conjuntos densos
- Por√©m, em conjuntos esparsos, o algoritmo original √© a melhor op√ß√£o

Para bases esparsas: eClat; para bases densas: dEclat

#### Leitura (Aula 05)

- Se√ß√µes 8.1, 8.2 (Zaki e Meira)
- Se√ß√µes 6.1, 6.2 (Introduction to Data Mining)
- Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3): 372-390 (2000)
- Zaki, M.J., Gouda, K.: Fast vertical mining using diffsets. Technical Report 01-1, Computer Science Dept., Rensselaer Polytechnic Institute (March 2001) 10
- Christian Borgelt. Efficient Implementations of Apriori and Eclat. Workshop of Frequent Item Set Mining Implementations (FIMI 2003, Melbourne, FL, USA).

Boa parte da explica√ß√£o est√£o nos artigos. A implementa√ß√£o t√° no Borgelt.

### Slide: aula04-FPGrowth (Aula 05)

#### Recapitulando

- Apriori: reduzir o n√∫mero de passsadas em disco
- Eclat: trazer pra mem√≥ria e assim reduzir o n√∫mero de passadas em disco, tendendo a zero.

#### Introdu√ß√£o (Aula 05)

- Nessa aula, veremos outro algoritmo que usa proje√ß√µes para reduzir o n√∫mero de passadas para computa√ß√£o dos conjuntos de itens frequentes
- O algoritmo FP-Growth (Frequent Pattern Growth) adota uma estrat√©gia dividir-e-conquistar para reduzir o custo computacional
- Ele, ao contr√°rio do Apriori, n√£o se baseia na gera√ß√£o de candidatos
- Os padr√µes s√£o constru√≠dos ao longo do processamento em profundidade
- Esse algoritmo √©, talvez, o algoritmo sequencial mais eficiente para busca de conjuntos de itens frequentes

A ideia √© evitar ter que computar os candidatos.

#### FP-Growth

- O FP-Growth foi proposto em 2000 por Jiawei Han, Jian Pei e Yiwen Yin
- O algoritmo atacou dois problemas presentes nas abordagens iniciais:
  1. Repetidas passadas sobre a base de dados; e
  2. Gera√ß√£o de candidatos [Mais cr√≠tico]
- O primeiro problema, como j√° discutimos, √© cr√≠tico pelo custo computacional inerente √† leitura em mem√≥ria secund√°ria
- O segundo problema est√° relacionado √† gera√ß√£o de candidatos desnecess√°rios
  - Muitos s√£o descartados pela propriedade do Apriori

##### FP-Tree [√Årvore de Prefixos]

- O FP-Growth possui algumas similaridades ao Eclat:
  - Ambos adotam a estrat√©gia de busca em profundidade
  - Ambos adotam proje√ß√µes dos dados com o intuito de traz√™-los para mem√≥ria principal e reduzir o custo computacional
- O FP-Growth, no entanto, usa uma estrutura de dados diferente para suportar a busca pelos padr√µes
  - Uma √°rvore de prefixos chamada FP-Tree
- A busca pelos padr√µes se d√° inteiramente atrav√©s da √°rvore sem a necessidade de se voltar √† base de dados
- Dessa forma, a primeira tarefa do algoritmo √© construir essa estrutura

"A partir da Base de Dados, como fazer a √°rvore de prefixos?"

---

- A constru√ß√£o da FP-Tree ocorre em duas fases
- Primeiro, o algoritmo varre a base de dados para computar a frequ√™ncia individual de cada item
  - Itens infrequentes s√£o descartados, uma vez que n√£o podem formar padr√µes frequentes
- Segundo, o algoritmo percorre novamente a base processando as transa√ß√µes ordenadas pela frequ√™ncia dos itens
  - Os itens nas transa√ß√µes s√£o ordenados em ordem decrescente de frequ√™ncia e os infrequentes s√£o filtrados
- As transa√ß√µes s√£o ent√£o inseridas na √°rvore enquanto processadas
  - Itens s√£o n√≥s da √°rvore
  - Cada n√≥ armazena um item e sua frequ√™ncia (n√∫mero de transa√ß√µes que o cont√©m)

Basicamente, ele limpa os infrequentes, e depois disso, vai inserindo as transa√ß√µes em uma √°rvore.

---

- Para facilitar a busca pelos padr√µes, a √°rvore √© equipada com uma estrutura adicional para localizar a ocorr√™ncia dos itens e sua frequ√™ncia
- Exemplo

$$
\begin{bmatrix}
  TID & Muesli (a) & Oats (b) & Milk (c) & Yoghurt (d) & Biscuits (e) & Tea (f) \\
  1 & 1 & 0 & 1 & 1 & 0 & 1\\
  2 & 0 & 1 & 1 & 0 & 0 & 0\\
  3 & 0 & 0 & 1 & 0 & 1 & 1\\
  4 & 1 & 0 & 0 & 1 & 0 & 0\\
  5 & 0 & 1 & 1 & 0 & 0 & 1\\
  6 & 1 & 0 & 1 & 0 & 0 & 1\\
\end{bmatrix}
$$

minsup = 2

Em ordem de maior suporte pra menor suporte: cfabd

1. cfad
2. cb
3. cf
4. ad
5. cfb
6. cfa

Obs.: Ignoram-se os itens infrequentes.

#### Minera√ß√£o dos padr√µes [Aula 05]

- A minera√ß√£o dos padr√µes se inicia uma vez que a FP-Tree tenha sido constru√≠da
- A constru√ß√£o agora ocorre aumentando-se prefixos dos padr√µes em ordem crescente de suporte
- As transa√ß√µes que satisfa√ßam (cont√©m) o padr√£o sendo constru√≠do s√£o projetadas em uma nova √°rvore
- Itens podem se tornar infrequentes nessa nova base e s√£o descartados
- Os padr√µes encontrados nessa nova √°rvore devem incluir o prefixo que a gerou
- O algoritmo segue com as extens√µes recursivamente at√© que um √∫nico ramo seja obtido
  - Se a √°rvore possui um √∫nico ramo, os padr√µes obten√≠veis s√£o todas as combina√ß√µes dos n√≥s

Para se minerar as transa√ß√µes de volta, percorremos a lista de itens e ent√£o subimos dele at√© a raiz.

Partindo do item menos frequente e indo pro item mais frequente, fazemos proje√ß√µes da √°rvore.

Essas proje√ß√µes s√£o sub-√°rvores da √°rvore original.

No caso do d, percorrerei todos os n√≥s da lista encadeada de de d's, indo dele at√© a raiz. A jun√ß√£o de todos os n√≥s que eu passar, formar√° uma nova √°rvore. E essa ser√° a proje√ß√£o do item d.

Mas ainda n√£o entendi o que precisa ser feito ap√≥s essa primeira proje√ß√£o.

---

- Exemplo

$$
\begin{bmatrix}
  Item & Freq & Link
  c & 5 & \\
  f & 4 & \\
  a & 3 & \\
  b & 2 & \\
  d & 2 & \\
\end{bmatrix}
$$

```mermaid
flowchart LR
  Vazio(("`$\emptyset$`")) --> C((c:5))
  C --> F((f:4))
  F --> A2((a:2))
  A2 --> D1((d:1))
  F --> B1((b:1))
  C --> B2((b:1))
  Vazio --> A1((a:1))
  A1 --> D2((d:1))
```

H√° tamb√©m uma lista encadeada para todos os n√≥s com ocorr√™ncias de um mesmo item.

A lista encadeada serve para podermos percorrer todos os n√≥s de um mesmo item e calcularmos sua frequ√™ncia.

...

#### Quest√µes de implementa√ß√£o

- E se a FP-tree n√£o couber na mem√≥ria?
  - A solu√ß√£o √© particionar/projetar a base de dados em mem√≥ria secund√°ria antes de iniciar a constru√ß√£o da √°rvore
- Como construir a FP-tree de forma eficiente?
  - Solu√ß√£o proposta por Christian Borgelt otimiza mem√≥ria e tempo
  - Representa√ß√£o b√°sica dos dados: lista de vetores de inteiros
  - Dados (proje√ß√µes) s√£o carregados inteiramente para mem√≥ria
  - Lista √© seccionada com base no k-√©simo item; um n√≥ √© criado para cada se√ß√£o
  - N√≥s t√™m tamanho fixo (20 bytes em 32bits; 40 em 64bits)
    - 1x identificador de item
    - 1x contador de frequ√™ncia
    - 1x ponteiro para n√≥ pai
    - 1x ponteiro para pr√≥xima ocorr√™ncia do item
    - 1x ponteiro para n√≥ auxiliar

---

- Implementa√ß√£o tradicional, conforme descri√ß√£o do algoritmo, evita carregar dados para mem√≥ria
  - Por√©m, n√≥s ter√£o tamanho vari√°vel ou desperdi√ßam mem√≥ria (ponteiros para filhos que nunca ocorrem)
  - Melhora gerenciamento de mem√≥ria; grandes blocos podem ser alocados de uma vez e gerenciados internamente
  - Al√©m disso, ponteiros para pais s√£o mais √∫teis que ponteiros para filhos durante execu√ß√£o

---

- Proje√ß√µes s√£o executadas com dois la√ßos
  - Um la√ßo externo percorre o n√≠vel mais baixo (elemento condicionante da proje√ß√£o)
  - La√ßo interno percorre a os ramos origin√°rios do n√≥ folha
- A nova √°rvore √© constru√≠da como uma ‚Äòsombra‚Äô da original
  - N√≥s s√£o duplicados conforme s√£o visitados (ponteiro auxiliar mant√©m elo de liga√ß√£o entre original e c√≥pia para atualiza√ß√µes necess√°rias durante constru√ß√£o)
  - Frequ√™ncia do n√≥ folha √© propagada para cima
- A sombra √© destacada da √°rvore original em uma segunda passada pelos n√≥s
- N√≥s infrequentes podem ser removidos e n√≥s com mesmo r√≥tulo mesclados

---

...

---

#### Leitura [Aula 05]

- Se√ß√£o 6.6 Intro to Data Mining
- Se√ß√£o 8.2.3 Zaki e Meira
- [Borgelt, C. (2005) An Implementation of the FP-growth Algorithm][LinkFPGrowth]

[LinkFPGrowth]: <https://borgelt.net/papers/fpgrowth.pdf>

## Aula 06 | 03/04/2025 | Minera√ß√£o de conjuntos de itens

## Aula 07 | 08/04/2025 | Minera√ß√£o de sequ√™ncias

### Aula 08 | 10/04/2025 | Minera√ß√£o de sequ√™ncias

### Aula 09 | 15/04/2025 | Minera√ß√£o de grafos

### Aula 10 | 17/04/2025 | Minera√ß√£o de grafos

### Aula 11 | 22/04/2025 | Regras de associa√ß√£o e m√©tricas de qualidade

### Aula 12 | 24/04/2025 | Aprendizado descritivo supervisionado: padr√µes emergentes, contrastantes e descoberta de subgrupos

### Aula 13 | 29/04/2025 | Descoberta de subgrupos

### Aula 14 | 06/05/2025 | Descoberta de subgrupos

### Aula 15 | 08/05/2025 | Descoberta de subgrupos

### Aula 16 | 13/05/2025 | Minera√ß√£o de modelos excepcionais

### Aula 17 | 15/05/2025 | Minera√ß√£o de modelos excepcionais

### Aula 18 | 20/05/2025 | Minera√ß√£o de modelos excepcionais

### Aula 19 | 22/05/2025 | Semin√°rios (Padr√µes Frequentes)

### Aula 20 | 27/05/2025 | Semin√°rios (Padr√µes Frequentes)

### Aula 21 | 29/05/2025 | Semin√°rios (Padr√µes Frequentes)

### Aula 22 | 03/06/2025 | Semin√°rios (SD)

### Aula 23 | 05/06/2025 | Semin√°rios (SD)

### Aula 24 | 10/06/2025 | Semin√°rios (SD)

### Aula 25 | 12/06/2025 | Semin√°rios (aplica√ß√µes)

### Aula 26 | 17/06/2025 | Semin√°rios (aplica√ß√µes)

### Aula 27 | 24/06/2025 | Semin√°rios (aplica√ß√µes)

### Aula 28 | 26/06/2025 | Projeto

### Aula 29 | 01/07/2025 | Projeto

### Aula 30 | 03/07/2025 | Projeto
