# Aprendizado Descritivo - Renato Vimieiro - 2025

‚Äò‚Äô

## Aula 01 | 18/03/2025 | Apresenta√ß√£o do curso - [JV: Cheguei atrasado]

### Slide 1 - aula01-intro

#### Introdu√ß√£o

- Quando falamos sobre aprendizado de m√°quina e minera√ß√£o de dados, frequentemente associamos essas express√µes a predi√ß√£o de valores
- Mais especificamente, temos a ideia de que aprendizado de m√°quina (AM) se resume a, dada uma entrada X, encontrar uma fun√ß√£o f(X) que retorne o valor de uma vari√°vel alvo Y
  - Quando a vari√°vel alvo Y √© categ√≥rica, chamamos o problema de **classifica√ß√£o**
  - Quando a vari√°vel alvo Y √© cont√≠nua, chamamos o problema de **regress√£o**
- Assim, o senso comum define AM como aprender uma fun√ß√£o f capaz de predizer valores para dados ainda n√£o coletados
- Essa defini√ß√£o, embora restritiva, √© correta para a classe de tarefas elencadas acima, chamadas de **aprendizado preditivo**

#### Aprendizado Preditivo

- Mais especificamente ainda, esse imagin√°rio popular define o que conhecemos por aprendizado supervisionado de modelos preditivos
- De maneira mais formal, o objetivo dessas t√©cnicas √© aprender uma fun√ß√£o cujo dom√≠nio √© um conjunto de inst√¢ncias $\mathcal{X}$, e contradom√≠nio um conjunto de sa√≠das $\mathcal{Y}$
- Para tal, o algoritmo recebe um conjunto de pares $(x, l(x)) \in \mathcal{X} \times \mathcal{L}$
  - A fun√ß√£o $l(x)$ retorna o valor de sa√≠da esperado para a inst√¢ncia x
- Como o algoritmo obt√©m o modelo guiado por esse conjunto de entrada (treinamento), a tarefa √© classificada como 'supervisionada', j√° que $l(x)$ faz o papel de 'professor'
- Da mesma forma, como o modelo aprendido √© usado para predizer valores de sa√≠da de novas inst√¢ncias, ele √© chamado de preditivo

- JV
  - O que desejamos √© receber informa√ß√µes e conseguir retornar um r√≥tulo.
  - Nem todos de aprendizado de m√°quina levam em considera√ß√£o os r√≥tulos.
  - Aprendizado supervisionado preditivo
  - [JV: n√£o anotei sobre o que seria o supervisionado]
  - Se tiver que explicar como funciona, a√≠ √© a √°rea de Explanable AI.
  - Aqui √© mais importante o resultado do que o processo.

---

- Como queremos que o modelo seja fiel √† realidade e, portanto, consiga capturar a rela√ß√£o entre inst√¢ncia (entrada) e sa√≠da, √© comum, nesse cen√°rio, avaliarmos sua qualidade comparando os valores obtidos com os verdadeiros para um conjunto de inst√¢ncias chamado de conjunto de valida√ß√£o
- Ou seja, nesse tipo de tarefa temos o seguinte fluxo de trabalho:

---

```mermaid
flowchart LR
  Trat[(Tratamento)]
  Cons[Constru√ß√£o]
  Ava[Avalia√ß√£o]
  Uso["Uso (Modelo)"]
  Val[(Valida√ß√£o)]
  Trat --> Cons
  Cons --> Ava
  Ava --> Uso
  Ava --> Cons
  Val --> Ava
```

Aqui, n√£o queremos obter nenhum entendimento sobre o que j√° vimos antes, mas sim, ter meios de prever como ser√£o classificados os pr√≥ximos itens que veremos.

---

- Embora m√©todos de regress√£o e classifica√ß√£o (aprendizado supervisionado) sejam os -ais populares em AM, existem outras abordagens preditivas que n√£o requerem a vari√°vel -lvo para ajustarem modelos
- T√©cnicas que n√£o utilizam essas vari√°veis s√£o classificadas como **n√£o-supervisionadas**
- Uma tarefa de aprendizado n√£o-supervisionado bastante popular √© a de agrupamento (clustering)
- Essa tarefa consiste em encontrar subgrupos de elementos homog√™neos nos dados

---
---

- [JV]
  - Exemplo: c√¢ncer de mama
    - Separando mulheres cuja quimioterapia foram eficazes e as que n√£o foram.
    - Analisar quais os mapeamentos gen√©ticos delas
    - Verificar de que forma h√° uma rela√ß√£o entre os mapeamentos gen√©ticos e a efic√°cia da quimioterapia.
    - E com isso, tentar predizer se a quimioterapia ser√° eficaz ou n√£o para uma nova paciente.
  - No aprendizado n√£o supervisionado n√£o h√° r√≥tulos.
  - Uma tarefa de aprendizado n√£o-supervisionado bastante popular √© a de agrupamento (clustering)
    - Um dos mais conhecidos √© o k-menas

---

- Em outras palavras, a tarefa de agrupamento consiste em detectar grupos de inst√¢ncias que sejam mais parecidas entre si do que com as de outros grupos
- Sob a √≥tica de aprendizado preditivo, a tarefa consiste em encontrar uma fun√ß√£o $q: \mathcal{X} \rightarrow \mathcal{C}$, cujo dom√≠nio $\mathcal{X}$ √© um conjunto de inst√¢ncias, e o contradom√≠nio ùíû um conjunto de grupos (clusters)
- Note que a tarefa se assemelha √† de classifica√ß√£o (j√° que os r√≥tulos dos grupos s√£o categ√≥ricos), por√©m o ajuste do modelo n√£o leva em considera√ß√£o r√≥tulos pr√©-definidos
- Um exemplo de m√©todo aprendizado preditivo n√£o-supervisionado √© o K-Means
  - O modelo s√£o os centroides e a dist√¢ncia euclidiana
  - Novas inst√¢ncias s√£o alocadas nos clusters de cujos centroides elas sejam mais pr√≥ximas de acordo com a dist√¢ncia euclidiana

#### Aprendizado Descritivo

- Aprendizado descritivo tem como objetivo central obter uma descri√ß√£o para os dados
  - Isto √©, o objetivo √© encontrar um **modelo descritivo** para os dados
- Dessa forma, podemos apontar a primeira diferen√ßa para aprendizado preditivo:
  - N√£o temos mais a necessidade de dividir o conjunto de inst√¢ncias em treinamento e valida√ß√£o
- A divis√£o entre treinamento e valida√ß√£o n√£o faz mais sentido, pois queremos obter um modelo para os dados que temos em m√£os
- Consequentemente, a avalia√ß√£o dos resultados (modelos) se torna mais dif√≠cil, j√° que n√£o temos mais uma ‚Äòverdade absoluta‚Äô para compararmos as sa√≠das

- JV
  - > A premissa √© "eu n√£o sei nada sobre os dados", "ent√£o preciso encontrar um modelo que descreva os dados"
  - "Estat√≠stica n√£o √© boa para descrever grafos"
  - O Descritivo √© t√£o importante quanto prever coisas, mas atuam em momentos diferentes.

---

- Por outro lado, segundo Flach (2012), *o aprendizado descritivo leva √† descoberta genu√≠na de novos conhecimentos, e, dessa forma, est√° situado entre as √°reas de minera√ß√£o de dados e aprendizado de m√°quina*
- O objetivo de se buscar um modelo descritivo dos dados se justifica nas situa√ß√µes em que se quer responder perguntas do tipo ‚Äúo qu√™ aconteceu?‚Äù
- Ou seja, esses modelos descrevem situa√ß√µes passadas e, assim, auxiliam no processo de tomada de decis√£o Aprendizado

- JV
  - O aprendizado descritivo leva a descoberta e novos conhecimentos. Estando entre minera√ß√£o de dados e aprendizado de m√°quina.
  - Busca responder "o qu√™ aconteceu?"
  - Descrevem situa√ß√µes passadas e com isso auxiliam no processo de **tomada de decis√£o**.

  - 4 paradigmas cient√≠ficos
    - Indu√ß√£o, te√≥rico, ...
  - Antes viam o fen√¥menos, criavam teorias, tentavam provar que as teorias se aplicavam.
  - Atualmente usamos um modelo baseado em dados
    - Veem como os dados se comportas, criam teorias e tentam provar que os dados se comportam de acordo com a teoria.

---

- Considere a seguinte situa√ß√£o em que um grande *Market place* deseja reduzir os custos de distribui√ß√£o dos produtos que ele vende
- Uma pr√°tica muito utilizada atualmente √© manter centros de distribui√ß√£o regionais para estocar produtos vendidos frequentemente, reduzindo o custo e tempo de transporte, e, consequentemente, aumentando a satisfa√ß√£o dos clientes
- Apesar da estocagem de produtos populares nas regionais ser uma decis√£o trivial, ela pode ser aprimorada analisando-se o hist√≥rico de vendas
- Nesse hist√≥rico, podemos encontrar itens menos populares que, com certa frequ√™ncia, s√£o adquiridos junto com os mais populares
  - Isso nos permite decidir estocar tamb√©m esses produtos menos populares, em menor quantidade, mas evitando, assim, um custo maior de se enviar tais produtos individualmente de centros mais distantes

- JV
  - Uma das coisas feitas nessa disciplina √© a busca por regularidades de acontecimentos em conjuntos.
  - H√° uma interse√ß√£o bem grande entre aprendizado descritivo e Minera√ß√£o de Dados.

---

- Considere um segundo caso real em que executivos do Wal-Mart utilizaram de AM para aumentar as vendas diante da amea√ßa do fura√ß√£o Frances em 2004
- Enquanto o furac√£o atravessava o Caribe, os executivos queriam prever os produtos que seus clientes consumiam diante de cat√°strofes

- JV
  - > What Wal-Mart knows about Customers' Habits
  - Eles avaliaram de que forma os usu√°rios se comportavam em rela√ß√£o a desastres naturais e o que compravam nas cidades que tavam para ser afetadas.
    - A decis√£o trivial era considerar que faria sentido estocar pilha, √°gua mineral, lanterna e produtos n√£o espec√≠ficos.
    - Eles descobriram que houve um aumento de 7x nas vendas de Pop Tarts sabor morando, e o campe√£o dos aumentos foi a cerveja.
    - Nessa situa√ß√£o eles mais queriam descrever o passado do que predizer o futuro.
  - Embora seja abordado como preditivo, na pr√°tica seria um exemplo de aprendizado descritivo supervisionado.
  - Exceptional Model Mining
    - Busca-se encontrar quais conjuntos de itens s√£o n√£o-usualmente comprados qunado algum evento ocorre.

D√∫vida: Como fazer para discernimos se um aumento, como no caso do Pop Tarts foi de fato devido aos furac√µes ou se calhou de, nesses dois mesmos intervalos de tempo, foram ve√≠culados an√∫ncios desse produto; sendo ent√£o apenas uma coincid√™ncia?

Resposta: D√° para tentar refinar a forma de an√°lise e o c√°lculo da fun√ß√£o objetivo. Por√©m, devido ao car√°ter qualitativo, √© dif√≠cil de se ter certeza de que essa atipicidade nessa busca por padr√µes at√≠picos sejam separados.

---

- O objetivo dos executivos do Wal-Mart era abastecer as lojas da Fl√≥rida, que estava no caminho do furac√£o, e assim aumentar as vendas
- Novamente, a decis√£o trivial seria aumentar o estoque de pilhas, √°gua mineral, lanternas, e produtos n√£o perec√≠veis
- A an√°lise do hist√≥rico de vendas, nesse caso, poderia retornar que as lojas venderam todo o estoque de DVDs de um g√™nero espec√≠fico de filmes
- No entanto, ao analisar os dados, eles chegaram √† conclus√£o de que havia um aumento de 7x nas vendas de Pop Tarts sabor morango, e o campe√£o do aumento de vendas era cerveja

---

- Apesar do caso ter sido abordado como um exemplo de aprendizado preditivo na mat√©ria, ele √© um exemplo claro de aprendizado descritivo supervisionado
- Especificamente, ele √© um caso claro da aplica√ß√£o de excepcional model mining
  - Queremos encontrar padr√µes de vendas n√£o usuais, isto √©, detectar aumento da venda de produtos que esteja correlacionado positivamente a um evento de interesse
- O √∫nico por√©m √© que essa tarefa foi inicialmente proposta somente em 2008, 4 anos ap√≥s o evento!

#### Aprendizado Descritivo X Preditivo

- Os exemplos ilustram bem a aplicabilidade do aprendizado descritivo:
  - Como dito, eles revelam o que ocorreu no passado e auxiliam na tomada de decis√£o de eventos futuros
- De forma an√°loga, os modelos preditivos utilizam dados hist√≥ricos para prever o comportamento de dados futuros
- Note que a diferen√ßa, portanto, √© bem t√™nue entre as duas abordagens
  - A diferen√ßa mais aparente √© a interven√ß√£o humana no primeiro, contra um certo automatismo da segunda

---

- Um exemplo dessa diferen√ßa t√™nue entre as duas abordagens √© justamente a tarefa de agrupamento
- Incialmente apresentamos clustering como uma tarefa de aprendizado preditivo n√£o-supervisionado
- A mesma tarefa, por√©m, pode ser apresentada como descritiva
- O objetivo no agrupamento descritivo √© obter uma fun√ß√£o $q: \mathcal{D} \to \mathcal{C}$, que mapeia as inst√¢ncias coletadas no conjunto de dados $\mathcal{D}$ a grupos espec√≠ficos (clusters) $\mathcal{C}$
  - A diferen√ßa aqui √© que assumimos como dom√≠nio apenas o conjunto de inst√¢ncias em m√£os, e n√£o toda popula√ß√£o de inst√¢ncias poss√≠veis
  - Ou seja, o resultado do nosso agrupamento √© ‚Äòapenas‚Äô uma divis√£o das inst√¢ncias em grupos, permitindo a an√°lise de similaridade entre elas; n√£o estamos interessados em alocar novas inst√¢ncias aos grupos encontrados

---

[Imagem: Agrupamento preditivo//Agrupamento Descritivo]

No modelo Preditivo, tenta-se definir limites para que pr√≥ximos itens sejam classificados de acordo com o que foi visto anteriormente.

J√° no Descritivo, tenta-se encontrar padr√µes que descrevam o que foi visto anteriormente.

---

- Considere agora um exemplo mais relacionado ao segundo estudo de caso discutido anteriormente
- Suponha que nosso conjunto de entrada rotulado seja o seguinte
- Podemos tra√ßar dois objetivos aqui:
  - Separar c√≠rculos de quadrados, para classificar novos pontos como um ou outro
  - Buscar padr√µes nos dados para entender as diferen√ßas entre c√≠rculos e quadrados

[Imagem: Distribui√ß√£o de quadrados azuis e c√≠rculos vermelhos]

No exemplo apontado pode-se separar as distribui√ß√µes dos pontos em 4 quadrantes, isso baseado na estimativa do que j√° ocorreu antes, busca ent√£o estimar onde estar√£o posicionados os quadradinhos azuis e as bolinhas vermelhas.

√â importante tamb√©m identificar quais s√£o as regularidades existentes em certos padr√µes irregulares.

---

- O primeiro objetivo induz uma abordagem preditiva
- Logo, o abordamos como um problema de classifica√ß√£o

√Äs vezes usa-se o mesmo modelo entre preditivo e descritivo, por√©m, um pra descrever e o outro pra predizer.

[Imagem: Distribui√ß√£o dos pontos]

---

- O segundo objetivo induz a uma abordagem descritiva
- Logo, abordamos o problema como uma tarefa de descoberta de subgrupos
- [JV]
  - $\sigma_1 \equiv x_1 \in [0.3, 0.7) \wedge x_2 \geq 0.9$
  - $\sigma_2 \equiv x_1 \in [0.275, 0.7) \wedge x_2 \leq 0.1$

[Imagem: Definindo grupo independente]

---

- Esse √∫ltimo exemplo mostra uma caracter√≠stica que frequentemente diferencia as duas abordagens
- As abordagens preditivas buscam ajustar modelos que aprendam as regularidades globais dos dados
  - No exemplo, encontrar as fronteiras que separam c√≠rculos de quadrados
- As abordagens descritivas, por outro lado, ajustam modelos que aprendam regularidades locais dos dados, i.e., padr√µes v√°lidos apenas a subgrupos espec√≠ficos
  - No exemplo, intervalos das vari√°veis que descrevem subgrupos com uma distribui√ß√£o n√£o usual de c√≠rculos e quadrados Leitura

#### Leitura - Aula 01

- Cap√≠tulo 3, Flach (2012)

#### Coment√°rios sobre o curso

Nesse curso busca-se a parte teoria dos algoritmos, n√£o necessariamente em sua aplica√ß√£o.

Ela √© te√≥rica, densa em algoritmo, e a aplica√ß√£o em c√≥digo √© m√≠nimo.

Busca-se "botar uma lupa" sobre a descoberta de padr√µes.

A primeira parte ser√° toda n√£o-supervisionada.

A busca de padr√µes em grafos pode ser usada na √°rea de f√°rmacos para encontrar quais sub-estruturas s√£o as mais frequentes em determinados rem√©dios para determinada infermidade?

A segunda parte ser√° de aprendizado supervisionado.

Por volta de 20 de maio tem uma prova.

Haver√° um tipo de "roleplaying" das atividades. Ele separou as salas em 8 grupos. Um grupo era o "historiador" (buscava entender qual era o contexto), o outro era o "metodologista" (tentatva entender como o algoritmo funcionava), "Aplica√ß√µes", "Coletar as apresenta√ß√µes e redigir", "Publicar o resumo em um site da turma". O "hacker" √© quem busca os c√≥digos existentes, tenta entender, fazer funcionar e documentar como fez funcionar.

"Daqui para baixo √© a parte mais recente, talvez mais p√≥s gradua√ß√£o, ou coisas que n√£o est√£o nos livros".

Cada grupo vai rotacionar em cada uma das tarefas. Ser√£o 9 artigos no total que leremos.

Os Semin√°rios (aplica√ß√µes), veremos de fato aplica√ß√µes

O que ele quer com o projeto? Uma intera√ß√£o maior com o professor. A parte mais pr√°tica da disciplina.

Na parte de ... ser√° o ... que foi quem inventou.

Na parte de supervisionado: Sebastian Ventura e Jos√© Maria Luna 2018; Guozhu Dong and James Bailey 2012;

## Aula 02 | 20/03/2025 | Aprendizado descritivo x preditivo

### Slide - Aprendizado Descritivo

#### Introdu√ß√£o - Aula 2

- Minera√ß√£o de itens alguma coisa

---

- Kosinski (2013) coletavam dados das personalidades atrav√©s do MyPersonality.
  - Esc√¢ndalo do Cambridge Analytica
- Buscava predizer a personalidade baseado nas curtidas feitas no Facebook

---

- Provost e Foster (2013) utilizaram os dados para demonstrar a modelagem descritiva e as informa√ß√µes √∫teis.
- Alguns exemplos de regras:
  - Selena Gomez -> Demi Lovato
  - Linking Park & Disturbed & System of a Down & Korn -> Slipknot
  - SpongeBob SquarePants & Converse [JV: empresa do All Stars] -> Patrick Star
  - Skittles & Mountain Dew -> Gatorade

[JV: Offtopic: o diretor da Google daqui fez doutorado no PPGCC]

---

- A ideia de itens em cesta de compra pode ser generalizada para itens virtuais
- Busca-se encontrar co-ocorr√™ncias de itens de an√°lise
- Dados os limites √©ticos, pode-se usar essa an√°lise para se atingir diversos objetivos.

##### Itemset e Tidsets

- Os "produtos" da cesta de compras s√£o chamados de "Itens".
  - $I = {x_1, x_2, \dots, x_m}$
- Esses elementos ser√£o as **vari√°veis de an√°lise**
- Um conjunto $X \subseteq I$ √© chamado de **Itemset**
- Um itemset de tamanho $k$ √© chamado de **k-itemset**
- Denotamos o conjunto de todos os **k-itemsets** por $I^(k)$
- Todas as "Transa√ß√µes" ser√£o identificadas por IDs, ou ent√£o, **TID** (Transaction ID)
- O conjunto $T = {t_1, t_2, \dots, t_n}$ √© o conjunto das transa√ß√µes.

---

- O Conjunto $Y \subseteq T$ √© chamado de **Tidset**
- √â v√°lido assumir que _itemsets_ e _tidsets_ s√£o ordenados por ordem lexicogr√°fica dos itens e transa√ß√µes. (N√£o importa qual ordem, mas est√£o de algum modo ordenados)
- Cada transa√ß√£o consiste de um identificador (TID) e um conjunto de itens
  - Ent√£o, cada transa√ß√£o √© um par $(t, X)$ em que $t \in T$ e $X \subseteq I$
- Formalmente, um conjunto de dados ser√° uma tripla $(T, I, D)$
  - T: Transa√ß√µes ou objetos
  - I: Atributos ou itens
  - D: Rela√ß√£o bin√°ria entre eles.
  - $T$ e $I$ s√£o os conjuntos de tids e itens
  - $D \subsetq T \times I$ √© a rela√ß√£o bin√°ria em que $(t, i) \in D$...

---

- Podemos estender a defini√ß√£o tamb√©m para conjuntos de itens
- Dizemos que $t$ cont√©m um itemset X sse $\forall i \in X (t, i) \in D$

Ou seja: $t$ cont√©m $X$ se $|X - t| = 0$

---

- Dado um itemset $X$, podemos querer saber o conjunto de transa√ß√µes que o cont√©m.
- Esse conjunto √© chamado de **extens√£o** ou **cobertura** de $X$
- Ele √© definido pela seguinte fun√ß√£o:
  - $c: P(I) \to P(T)$
  - $c(X) = {t \in T | \forall i \in X(t, i) \in D}$
- Dado um tidset Y, podemos querer saber o maior conjunto de itens comuns √†s transa√ß√µes de Y.
- Esse conjunto √© chamado de **intens√£o** (N√£o √© inten√ß√£o!) de Y.
- Ele √© definido por
  - $i: P(T) \to P(I)$
  - $i(Y) = {x \in I | \forall t \in Y(t, x) \in D}$

O uso de extens√£o e intens√£o v√™m da ideia filos√≥fica e semi√≥tica de que a extens√£o √© o conjunto de coisas que se encaixam em uma defini√ß√£o, enquanto a intens√£o √© a defini√ß√£o em si.

---

|  TID | Muesli | Oats | Milk | Yoghut | BIscuits |  Tea |
| ---: | -----: | ---: | ---: | -----: | -------: | ---: |
|    1 |      1 |    0 |      |        |          |    1 |
|    2 |      0 |    1 |      |        |          |    0 |
|    3 |      0 |    0 |      |        |          |    1 |
|    4 |      1 |    0 |      |        |          |    0 |
|    5 |      0 |    1 |      |        |          |    1 |
|    6 |      1 |    0 |      |        |          |    1 |

Intens√£o: conjunto de itens comuns a todos os elementos de um determinado conjunto de transa√ß√µes.

Extens√£o: o conjunto de transa√ß√µes que contenham um determinado conjunto de itens.

Intens√£o: a interse√ß√£o das linhas
Extens√£o: a interse√ß√£o das colunas

##### Representa√ß√µes de conjuntos de dados

- Podemos enxergar o conjunto de dados como um conjunto de transa√ß√µes e suas respectivas intens√µes
  - Conjunto de $(t, i(t))$
  - Essa representa√ß√£o √© chamada de **Horizontal**
- Similarmente podemos enxergar o conjunto de dados como um conjunto de itens e suas cobrerturas
  - Conjunto de $(x, c(x))$
  - Essa representa√ß√£o √© chamada de **Vertical**

---

- Horizontal

|    t |                       i(t) |
| ---: | -------------------------: |
|    1 | Muesli, Milk, Yoghurt, Tea |
|    2 |                 Oats, Milk |
|    3 |        Milk, Biscuits, Tea |
|    4 |            Muesli, Yoghurt |
|    5 |            Oats, Milk, Tea |
|    6 |          Muesli, Milk, Tea |

- Vertical

|    x |  Muesli | Oats |          Milk | Yoghurt | Biscuits |        Tea |
| ---: | ------: | ---: | ------------: | ------: | -------: | ---------: |
| t(x) | 1, 4, 6 | 2, 5 | 1, 2, 3, 5, 6 |    1, 4 |        3 | 1, 3, 5, 6 |

|        x |          t(x) |
| -------: | ------------: |
|   Muesli |       1, 4, 6 |
|     Oats |          2, 5 |
|     Milk | 1, 2, 3, 5, 6 |
|  Yoghurt |          1, 4 |
| Biscuits |             3 |
|      Tea |    1, 3, 5, 6 |

##### Conjuntos de itens frequentes e Regras de Associa√ß√£o

- A identifica√ß√£o de regras tais como as que vimos no exemplo no in√≠cio da aula, em geral, envolvem duas etapas
  - Minera√ß√£o de conjuntos de itens frequentes
  - Descoberta de regras de associa√ß√µes interessantes
- A primeira tende a ser computacionalmente mais intensa. Por este motivo recebe mais aten√ß√£o dos pesquisadores.
- Nos concentraremos nessa tarefa.

##### Minera√ß√£o de Conjuntos de itens frequentes

- Uma defini√ß√£o de "regra interessante" √© ela ocorrer com certa frequ√™ncia.
- Ent√£o √© necess√°rio definir um limiar entre o que √© frequente e o que √© infrequente
  - Esse limiar √© chamado de suporte m√≠nimo (minsup)
- O suporte de um itemset √© o tamanho de sua cobertura
  - sup(X) = |c(X)|
- Como essa defini√ß√£o √© dependente do contexto, admite-se tamb√©m a defini√ß√£o do suporte relativo
  - rsup(X) = |c(X)| / |T|

---

Dessa forma, dizemos que um itemset √© frequente sse $sup(X) \geq minsup$

---

Rela√ß√£o de ordem parcial: $X \subseteq Y \leftrightarrow X \leq Y$

Conjunto pot√™ncia: √© o conjunto de todos os poss√≠veis subconjuntos de um conjunto.

```mermaid
flowchart TD
  null[["\null"]]
  null --> A & B & C
  A --> AB & AC
  B --> AB & BC
  C --> AC & BC
```

- O espa√ßo de busca do problema √© o conjunto pot√™ncia do conjunto de itens
- Se considerarmos a rela√ß√£o de subconjuntos como uma rela√ß√£o de ordem parcial, temos que o espa√ßo de busca √© estruturado como um reticulado
  - Esse reticulado pode ser visualizado como um grafo, onde somente as rela√ß√µes diretas s√£o representadas
  - Ou seja, se $A \subseteq B \land |A| = |B| - 1$, ent√£o existe uma aresta entre A e B no diagrama

Aquele diagrama explica bastante o que que isso quis dizer. Basicamente, no conjunto pot√™ncia, cada n√≠vel vai ter um elemento a mais que o n√≠vel anterior.

- ...

###### Algoritmo Ing√™nuo

Complexidade do algoritmo: $O(2^I \cdot T \cdot I)$

- **// ALGORITHM 8.1. Algoritm BruteForce**
  - **BruteForce** $(D, \mathcal{I}, minsup)$:
    - $\mathcal{F} \leftarrow \emptyset$ // set of frequent itemsets
      - **foreach** $X \subseteq \mathcal{I}$ **do**
        - $sup(X) \leftarrow ComputeSupport (X, D)$
        - **if** $sup(X) \leq minsup$ **then**
          - $\mathcal{F} \leftarrow \mathcal{F} \cup {(X, sup(X))}$
      - **return** $\mathcal{F}$

  - **ComputeSupport** $(X, D)$:
    - $sup(X) \leftarrow 0$
    - **foreach** $\langle t, i(t) \rangle \in D$ **do**
      - **if** $X \subseteq i(t)$ **then**
        - $sup(X) \leftarrow sup(X) + 1$
  - **return** $sup(X)$

---

- A computa√ß√£o do suporte de um itemset requer uma passada sobre o conjunto de dados, ou seja, requer tempo $O(|T|)$
- Verificar se uma dada transa√ß√£o cont√©m um itemset requer tempo $O(|I|)$
- Portanto, o custo total de computa√ß√£o do suporte √© $O(IT)$
- O espa√ßo de busca, por sua vez, √© o conjunto pot√™ncia de $I$. Logo, a complexidade do algoritmo ing√™nuo √© $O(2^I I T)$

---

- A complexidade do espa√ßo de busca √© inerente ao problema. Contudo o algoritmo √© ineficiente mesmo em espa√ßos pequenos
- Note que o conjunto de dados n√£o √© mantido em mem√≥ria, portanto a computa√ß√£o do suporte torna o algoritmo impratic√°vel
- Os algoritmos mais "sofisticados" atacam majoritariamente o problema de computa√ß√£o de suporte, evitando computa√ß√µes desnecess√°ris, e/ou adotando estrat√©gias mais eficientes para comput√°-lo.

Ent√£o temos dois problemas principais: reduzir o espa√ßo de busca e reduzir a complexidade para calcular o suporte.

##### Leitura

- Se√ß√µes 8.1 e 8.2 Zaki e Meira
- Se√ß√µes 6.1 e 5.2 (Introduction to Data Mining)

## Aula 03 | 25/03/2025 | Minera√ß√£o de conjuntos de itens - Faltei - Minera√ß√£o de itens frequentes: Apriori e Eclat

### Slide: aula03-apriori_eclat (Aula 03)

#### Introdu√ß√£o (Aula 03)

- Como vimos na aula anterior, o principal problema do algoritmo ing√™nuo para minera√ß√£o de conjuntos de itens frequentes era a replica√ß√£o de esfor√ßos para avaliar o suporte dos candidatos
- As m√∫ltiplas passadas no conjunto de dados (armazenado em mem√≥ria secund√°ria) torna o algoritmo impratic√°vel at√© mesmo para pequenos volumes
- Os algoritmos que veremos hoje exploram propriedades do problema para amortizar o custo da computa√ß√£o de suporte, e evitar retrabalho na avalia√ß√£o dos candidatos

---
---

O que √© mesmo o suporte? ü§î

- Recapitulando da aula anterior, o Suporte aparantemente √© um encurtamento para o "Suporte M√≠nimo" (minsup) que √© o limiar que define se determinado item √© frequente o bastante ou n√£o.
  - Esse valor √© dado pela seguinte f√≥rmula:
    - $sup(X) = |c(X)|$, onde $c(X)$ √© a cobertura do itemset $X$.

Mas o que √© mesmo a cobertura?

- A cobertura √© o conjunto de transa√ß√µes que cont√©m um itemset $X$. Ou seja, √© o conjunto de transa√ß√µes que cont√©m todos os itens do itemset $X$.

#### Apriori

- O Apriori foi proposto por Rakesh Agrawal e Ramakrishnan Srikant em 1994
  - O artigo possui mais de 30K cita√ß√µes
- Os autores √† √©poca trabalhavam no projeto da IBM para o Wal-Mart
- A ideia central √© evitar computa√ß√µes desnecess√°rias para candidatos infrequentes
- Isso √© viabilizado pela propriedade de **anti-monotonicidade** da fun√ß√£o suporte
- Essa √© uma das propriedades mais importantes para a √°rea

##### Anti-monotonicidade do suporte

- Considere dois itemsets $A$ e $B$ quaisquer. Se $A \subseteq B$, ent√£o $sup(A) \geq sup(B)$.
- Essa observa√ß√£o nos diz que a cobertura de conjuntos de itens √©, no m√°ximo, t√£o grande quanto a de seus subconjuntos
  - No caso mais simples, um conjunto de dois itens n√£o pode ocorrer em mais transa√ß√µes que cada um dos itens individualmente
- Consequentemente, se o itemset A √© infrequente, B tamb√©m ser√°.
- Isso define a propriedade de anti-monotonicidade da fun√ß√£o suporte, tamb√©m conhecida como a propriedade do Apriori
  - **Todo superconjunto de um conjunto infrequente √© infrequente**
  - **Todo subconjunto de um conjunto frequente √© frequente**

---
---

Muito interessante isso da√≠ de cima.

Basicamente entendemos que $sup(X=\{A, B\}) \geq sup(Y=\{A, B, C\})$ com isso, se X √© frequente, nada garante que Y tamb√©m seja. Por√©m, se Y √© frequente, isso garante que todos os poss√≠veis subconjuntos de Y tamb√©m ser√£o frequentes.

#### Apriori [2]

- O Apriori utiliza uma busca em largura no espa√ßo de busca para minerar os padr√µes
  - Frequentemente, o termo usado na literatura √© abordagem por n√≠veis (level-wise approach)
- A busca inicia com a identifica√ß√£o dos itens frequentes
- Depois, os conjuntos de tamanho $k$ s√£o explorados antes dos de tamanho $k+1$
- Assim como o algoritmo ing√™nuo, ele tamb√©m opera em duas etapas:
  - Gera√ß√£o de candidatos
  - C√¥mputo do suporte e elimina√ß√£o dos infrequentes

---

- A gera√ß√£o dos candidatos √© feita a partir dos conjuntos frequentes encontrados na fase anterior
- Conjuntos compartilhando um prefixo de $k-1$ itens s√£o combinados para gerar candidatos de tamanho $k+1$
  - Novamente, assume-se que eles s√£o ordenados pela ordem lexicogr√°fica
- Candidatos que possuam algum subconjunto infrequente s√£o descartados imediatamente
  - A propriedade do Apriori √© empregada
- Os suportes dos candidatos s√£o atualizados com uma √∫nica passada no conjunto de dados
  - Subconjuntos de tamanho $k$ de cada transa√ß√£o s√£o usados para atualizar o suporte dos candidatos

---

- **APRIORI** $(D, \mathcal{I}, minsup)$:
  - $\mathcal{F} \leftarrow \emptyset$
  - $\mathcal{C}^{(1)} \leftarrow \{\emptyset\}$ `// Initial prefix tree with single items`
  - **foreach** $i \in \mathcal{I}$ **do** Add $i$ as child of $\emptyset$ in $\mathcal{C}^{(1)}$ with $sup(i) \leftarrow 0$
  - $k \leftarrow 1$ `// k denotes the level`
  - **while** $\mathcal{C}^{(k)} \neq \emptyset$ **do**
    - ComputeSupport $(\mathcal{C}^{(k)}, D)$
    - **foreach** _leaf_ $X \in \mathcal{C}^{(k)}$ **do**
      - **if** $sup(X) \geq minsup$ **then** $\mathcal{F} \leftarrow \mathcal{F} \cup \{(X, sup(X))\}$
      - **else** remove $X$ from $\mathcal{C}^{(k)}$
    - $\mathcal{C}^{(k+1)} \leftarrow$ ExtendPrefixTree($\mathcal{C}^{(k)}$)
    - $k \leftarrow k+1$
  - **return** $\mathcal{F}^{(k)}$

---

- ComputeSupport $(\mathcal{C}^{(k)}, D)$:
  - **foreach** $\langle t, i(t) \rangle \in D$ **do**
    - **foreach** k-subset $X \subseteq i(t)$ **do**
      - **if** $X \in \mathcal{C}^{(k)}$ **then** $sup(X) \leftarrow sup(X) + 1$

- ExtendPrefixTree $(\mathcal{C}^{(k)})$:
  - **foreach** leaf $X_a \in \mathcal{C}^{(k)}$ **do**
    - **foreach** leaf $X_b \in SIBLING(X_a)$, such that $b > a$ **do**
      - $X_{ab} \leftarrow X_a \cup X_b$ `// prune candidate if there are any infrequent subsets`
      - **if** $X_j \in \mathcal{C}^{(k)}$, **for all** $X_j \subset X_{ab}$, such that $|X_j| = |X_{ab}|-1$ **then**
        - Add $X_{ab}$ as child of $X_a$ with $sup(X_{ab}) \leftarrow 0$
    - **if** _no extensions from_ $X_a$ **then**
      - Remove $X_a$, and all ancestors of $X_a with no extensions, from $\mathcal{C}^{(k)}$
  - **return** $\mathcal{C}^{(k)}$

---

- Exemplo (minsup=3):

$$
\begin{bmatrix}
  TID & Muesli & Oats & Milk & Yoghurt & Biscuits & Tea \\
  1 & 1 & 0 & 1 & 1 & 0 & 1 \\
  2 & 0 & 1 & 1 & 0 & 0 & 0 \\
  3 & 0 & 0 & 1 & 0 & 1 & 1 \\
  4 & 1 & 0 & 0 & 1 & 0 & 0 \\
  5 & 0 & 1 & 1 & 0 & 0 & 1 \\
  6 & 1 & 0 & 1 & 0 & 0 & 1 \\
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
  TID & Muesli & Milk & Tea \\
  1 & 1 & 1 & 1 \\
  2 & 0 & 1 & 0 \\
  3 & 0 & 1 & 1 \\
  4 & 1 & 0 & 0 \\
  5 & 0 & 1 & 1 \\
  6 & 1 & 1 & 1 \\
\end{bmatrix}
$$

---

- O n√∫mero de passadas √© drasticamente reduzido em rela√ß√£o ao algoritmo ing√™nuo
  - $O(2^I) \rightarrow O(I)$
- As podas baseadas na anti-monotonicidade do suporte tamb√©m s√£o bastante efetivas na pr√°tica
- O algoritmo tamb√©m apresenta alguns problemas:
  - Busca em largura requer que todos os candidatos de um n√≠vel sejam mantidos em mem√≥ria. Esse custo √© proibitivo em alguns (muitos) casos
  - Tanto a contagem do suporte quanto a poda do Apriori podem ser consideravelmente caras, dependendo da implementa√ß√£o

---

- O custo de mem√≥ria √© inerente √† abordagem, e n√£o podemos fazer muita coisa para melhor√°-lo
- O custo da contagem e verifica√ß√£o pode ser atenuado, usando estruturas de dados mais 'sofisticadas'
- Existem duas abordagens mais comuns:
  - Usar uma √°rvore hash
  - Usar uma √°rvore de prefixos (Trie)
- Na primeira abordagem, cada n√≥ folha armazena um conjunto de candidatos/conjuntos frequentes
  - O n√∫mero de compara√ß√µes √© reduzido
- Na segunda abordagem, os n√≥s da Trie armazenam os candidatos/conjuntos frequentes. Os subconjuntos das transa√ß√µes s√£o usadas para index√°-la e atualizar o suporte

---

- A redu√ß√£o do suporte m√≠nimo tem um impacto muito grande no custo computacional do algoritmo
  - O tamanho dos candidatos aumenta -> Mais candidatos s√£o avaliados em cada n√≠vel -> o tamanho dos conjuntos frequentes aumenta -> mais n√≠veis s√£o explorados

[Imagem(a): Number of candidate itemsets]

[Imagem(b): Number of frequent itemsets]

---

- A densidade da base de dados tamb√©m tem muito impacto no custo
  - Transa√ß√µes passam a ter mais itens
  - Isso tem duas implica√ß√µes: tamanho m√©dio dos itemsets aumentam; mais subconjuntos s√£o gerados durante a contagem do suporte $\binom{|t|}{k}$

[Imagem(a): Number of candidate itemsets]

[Imagem(b): Number of frequent itemsets]

## Aula 04 | 27/03/2025 | Minera√ß√£o de conjuntos de itens

### Aula passada

- Algoritmo apriori
- Frequente, infrequente.
- Como calcula o suporte
  - A partir dos itens frequentes: tabelas de 0 e 1.
  - Pra isso usaca √°rvore de ???
  - Para cada transa√ß√£o gerava os itemsets de tamanho k, ia na √°rvore ???
  - E incrementava o suporte daquela chave

[JV: escrevi o que ele t√° falando, mas n√£o t√¥ entendendo]

- Duas coisas influenciam o desempenho do algoritmo
  1. Ele falou algo
  2. Se o BD √© denso, as transa√ß√µes s√£o mais largas.
- $\binom{|t|}{k}$
- Quando √© esparso, funciona bem. Quando √© denso que come√ßa a dar problema.

Eu t√¥ achando que se eu compro $J = \{A, B, C\}$, Ent√£o o conjunto pot√™ncia dele √© $P(J) = \{\emptyset, A, B, C, AB, AC, BC, ABC\}$, e ent√£o, incrementaria 1 para um desses grupos

- C√°lculo de suporte:
  - Para cada um dos itemsets tem que verificar se ele t√° na √°rvore K(?)

- Se os itemsets est√£o em mem√≥ria...
- Se quero gerar o itemset $XY$ partindo de $X \cup Y$, posso dizer que o suporte ser√° $|c(X) \cap c(Y)|$

### Slide: aula03-apriori_eclat (Aula 04)

#### Eclat (Equivalence Class Transformation)

Dada a representa√ß√£o vertical dos dados, consigo calcular o suporte por essa intercess√£o.

- Dadas as defici√™ncias do Apriori, M. Zaki prop√¥s, em 2000, o algoritmo Equivalence Class Transformation (Eclat)
- A proposta do algoritmo √© 'eliminar' a necessidade de passadas no conjunto de dados para computar o suporte
- Para isso, ele parte de uma representa√ß√£o vertical dos dados, e se baseia no fato de que a cobertura da uni√£o de dois itemsets √© a interse√ß√£o de suas coberturas

Problema: como mantenho todos os itemsets gerados em mem√≥ria?

---

- Ou seja, a ideia central do algoritmo tentar manter os tidsets em mem√≥ria principal para computar o suporte dos itemsets atrav√©s de interse√ß√µes desses conjuntos
- Contudo, todos os tidsets podem n√£o caber na mem√≥ria principal. Assim, √© necess√°rio algum mecanismo que possibilite a divis√£o do espa√ßo de busca em subproblemas independentes que caibam na mem√≥ria
- A divis√£o √© feita conforme uma rela√ß√£o de equival√™ncia estabelecida sobre os candidatos

Tenta manter tudo na mem√≥ria principal

A ideia √© partir o problema em subproblemas e trazer esses subproblemas pra mem√≥ria.

Surgiu atrav√©s da cria√ß√£o de uma rela√ß√£o de equival√™ncia entre os itemsets

---

- Seja $p: P(I) \times \mathbb{N} \rightarrow P(I)$ uma fun√ß√£o prefixo. $p(X, k) = X[1:k]$.
- A rela√ß√£o $\theta_k \subseteq P(I) \times P(I), A \theta_k B \equiv p(A, k) = p(B, k)$, √© uma rela√ß√£o de equival√™ncia
- Dessa forma, ela induz uma parti√ß√£o dos conjuntos de itens em classes de equival√™ncia, onde todos os elementos compartilham um certo prefixo
- Por exemplo, todos os conjuntos que cont√™m o item Muesli pertencem √† classe de equival√™ncia $[Muesli]_{\theta_1}$
- Intuitivamente, essas classes servem como proje√ß√µes do conjunto de dados, em que somente as transa√ß√µes contendo aquele prefixo s√£o consideradas

Cria-se uma rela√ß√£o de equival√™ncia pelos prefixos.

Diz-se que dois itemsets s√£o equivalentes se o prefixos dos dois s√£o iguais.

Consideremos que temos o seguinte conjunto pot√™ncia: $P(I) = \{\emptyset, A, B, C, AB, AC, BC, ABC\}$. Na forma de representa√ß√£o, seria como se agrup√°ssemos os dados em grupos de prefixos:

- $A: \{A, AB, AC, ABC\}$
- $B: \{B, BC\}$
- $C: \{C\}$

E ent√£o seriam varridos de C para A.

Poderia-se tamb√©m fazer subgrupos de subgrupos, dependendo do tamanho do conjunto de prefixos.

---

Ele faz uma busca em profundidade (DFS)

Ele faz subparti√ß√µes at√© que o n√∫mero de transa√ß√µes seja pequeno o suficiente para caber na mem√≥ria.

---

- **Algoritmo 8.3 - Algoritmo ECLAT**
- // Initial Call: $\mathcal{F} \leftarrow \emptyset, P \leftarrow \{ \langle i, t(i) \rangle | i \in \mathcal{I}, |t(i)| \geq minsup \} $
- ECLAT $(P, minsup, \mathcal{F})$:
  - **foreach** $\langle X_a, t(X_a) \rangle \in P$ **do**
    - $\mathcal{F} \leftarrow \mathcal{F} \cup \{(X_a, sup(X_a))\}$
    - $P_a \leftarrow \emptyset$
    - **foreach** $\langle X_b, t(X_b) \rangle \in P$, with $X_b > X_a$ **do**
      - $X_{ab} = X_a \cup X_b$
      - $t(X_{ab}) = t(X_a) \cap t(X_b)$
      - **if** $sup(X_{ab}) \geq minsup$ **then**
        - $P_a \leftarrow P_a \cup \{ \langle X_{ab}, t(X_{ab}) \rangle \}$
    - **if** $P_a \neq \emptyset$ **then** ECLAT $(P_a, minsup, \mathcal{F})$

[JV: Droga, foquei em transcrever brevemente e esqueci de prestar aten√ß√£o na explica√ß√£o do professor]

P guarda todos os frequentes da chamada anterior. porque ele filtro toudos que s√£o infrequentes pelo minsup

para cada um dos frequentes dos candidatos, armazena no ocnjunto de itens frequentes globais

e a partir dele gera em profundidade a combina√ß√£o dele com todos os outros que v√™m pra frente.

Evitam redund√¢ncia: 1. parti√ß√µes; 2. Ordem sistem√°tica de combina√ß√£o dos itens.

1. A B C
2. A com B e C: AB AC
3. AB com AC: ABC
4. B com C: BC

##### Representa√ß√µes de conjuntos de dados (Aula 4)

$$
\begin{bmatrix}
  TID & Muesli & Oats & Milk & Yoghurt & Biscuits & Tea \\
  1 & 1 & 0 & 1 & 1 & 0 & 1\\
  2 & 0 & 1 & 1 & 0 & 0 & 0\\
  3 & 0 & 0 & 1 & 0 & 1 & 1\\
  4 & 1 & 0 & 0 & 1 & 0 & 0\\
  5 & 0 & 1 & 1 & 0 & 0 & 1\\
  6 & 1 & 0 & 1 & 0 & 0 & 1\\
\end{bmatrix}
\Rightarrow
[\emptyset]_{\theta_0}
\begin{bmatrix}
  X      & c(x)  \\
  Muesli & 146   \\
  Milk   & 12356 \\
  Tea    & 1356  \\
\end{bmatrix}
$$

Pelo que eu t√¥ entendendo:

1. Come√ßa pegando todos os itens que tenham uma quantidade de transa√ß√µes maior que o minsup (o valor m√≠nimo aceit√°vel para que consideremos relevante)
2. Depois disso, come√ßamos fazendo a intercess√£o das transa√ß√µes entre o primeiro conjunto de itens que passou pela compara√ß√£o com o segundo conjunto.
3. Depois disso, v√™ se o resultado dessas intercess√µes √© grande o bastante.

Se $A \subseteq B$, ent√£o $c(B) \subseteq c(A)$ (Cobertura)

---

#### Eclat (Equivalence Class Transformation) [2]

- O custo computacional do algoritmo est√° diretamente relacionado ao tamanho dos tidsets
  - O tempo de execu√ß√£o depende do c√°lculo da interse√ß√£o dos tidsets
- O custo de espa√ßo tamb√©m √© dependente do tamanho. Quanto mais denso o conjunto de dados, mais largos ser√£o os tidsets
- H√° duas formas de se implementar o algoritmo:
  - Usando vetores de bits
  - Usando vetores de Ids
- Os vetores de bits s√£o interessantes para o c√°lculo do suporte
  - Eles facilitam a computa√ß√£o da interse√ß√£o e o c√°lculo do tamanho do tidset pode ser feito usando uma tabela auxiliar (palavras de 16bits mapeadas para valores)
- Contudo, se o conjunto for esparso, isso representar√° um desperd√≠cio muito grande de espa√ßo. Ent√£o vetores de Ids se tornam mais interessantes.
  - As computa√ß√µes de interse√ß√£o s√£o feitas como na fun√ß√£o merge do mergesort

##### Diffsets e dEclat

Em bases de dados densos, varia bem pouco o suporte entre os itens. Ent√£o, faria mais sentido guardar s√≥ a diferen√ßa ao inv√©s de guardar o todo.

Ao inv√©s de chamar de tidset, passaram a chamar de diffset.

...

Pode-se armazenar em vetores de bits ao inv√©s de vetores de inteiros.

Usando o vetor de bits, √© como se fosse:

A = [00110, 01001, 01100, 00011]

E para calcular o suporte (?) cobetura(?)

basta fazer um c√°lculo r√°pido de 0 a 255 para dizer quantos bits est√£o ativos, e ent√£o fazer a contagem de bits ativos somando esses valores.

0 -> 1
1 -> 1
2 -> 1
...
255 -> ...

Outra forma de condensar √©: Se sei que um determinado conjunto √© grande o bastante, posso inferir que todos os que s√£o menores que eles tamb√©m s√£o grandes o bastante.

Se s√≥ √© guardado o valor das diferen√ßas, acaba sendo um problema fazer as intercess√µes.

---

- $C(PX) - C(PY)$
- $C(PX) - C(PY) \cup C(P) - C(P)$
- $C(PX) \cap \overline{C(PY)} \cup C(P) \cap \overline{C(P)}$
- $C(PX) \cup \overline{C(P)} \cap C(P) \cup \overline{C(P)}$

## Aula 05 | 01/04/2025 | Minera√ß√£o de conjuntos de itens

### Slide: aula03-apriori_eclat (Aula 05)

#### Diffsets e dEclat [Aula 05]

- Percebendo o problema de se manter os tidsets em mem√≥ria, Zaki e Gouda propuseram em 2001 (o artigo s√≥ foi publicado em 2003) uma solu√ß√£o alternativa
- Eles propuseram armazenar as diferen√ßas entre os tidsets dos membros de uma classe e dos prefixos que a definem
- Eles chamaram esse conjunto de **diffset**
- Formalmente, para um prefixo $P$ e um itemset $PX$, o diffset de $X$, $d(PX) = c(P) - c(PX)$
- Seriam armazenados, portanto, o suporte do itemset e seu diffset

---

- O fato √© que, se somente os diffsets s√£o armazenados, o suporte n√£o √© mais obtido como a cardinalidade desse conjunto
- Dessa forma, como calcular o suporte de um itemset $PXY$ obtido a partir de outros dois $PX$ e $PY$, usando somente seus diffsets?
- O suporte de $PX$ √© calculado pela diferen√ßa entre o suporte de $P$ e o diffset de $PX$, $sup(PX) = sup(PX) - |d(PXY)|$
  - Portanto, $sup(PXY) = sup(PX) - |d(PXY)|$
- A solu√ß√£o passa por computar o diffset de $PXY$. Por√©m, temos somente os diffsets de $PX$ e $PY$

---

- Por defini√ß√£o, $d(PXY) = c(PX) - c(PXY) = c(PX) - c(PY)$
- Podemos adicionar, ao conjunto acima, o conjunto vazio $(c(P) - c(P))$ sem alter√°-lo
- Logo, $d(PXY) = c(PX) - c(PY) + c(P) - c(P) - c(P) = (c(P)-c(PY)) - (c(P) - c(PX)) = d(PY) - d(PX)$
- Em outras palavras, podemos usar os diffsets dos conjuntos base para calcular o diffset do novo candidato
- A variante do Eclat que usa diffsets ficou conhecida como dEclat

---
---

- $d(PXY) = c(PX) - c(PY) = c(PX) - c(PXY)$
- $c(PXY) = c(PX) \cap c(PY)$

"Diferen√ßa √© a mesma coisa que interse√ß√£o com complemento"

- $d(PXY) = ...$

Ele fez um monte de igualdades com opera√ß√µes de conjuntos.

- $d(PXY) =$
- $c(PX) - c(PY) =$
- $c(PX) - c(PXY) =$
- $[c(PX) - c(PXY)] \cup [c(P) - c(P)] =$
- $[c(PX) \cap \overline{c(PXY)}] \cup [c(P) \cap \overline{c(P)}]$
- $[c(PX) \cup c(P)] \cap [\overline{c(PY)} \cup \overline{c(P)}] \cap \overline{d(PX)}$
- $(c(PX) \cap \overline{c(PY)}) \cup (c(P) \cap \overline{c(P)}) \cup (c(P) \cap \overline{c(PY)}) \cup (c(P) \cap \overline{c(P)} \cap \overline{d(PX)})$
- $(Q \subseteq d(PY)) \cup (\emptyset) \cup (d(PY)) \cup (\emptyset) \cap (\overline{d(PX)})$
- $d(PY) \cap \overline{d(PX)} =$
- $d(PY) - d(PX)$

---

- $d(PX) = ...$

---

- $P = \{1, 2, 3, 4, 5\}$
- $X = \{1, 3, 5\}$
- $Y = \{2, 3, 4\}$
- $PX = \{1, 3, 5\}$
- $PY = \{2, 3, 4\}$
- $\overline{PY} = \{1, 5\}$
- $PX \cap \overline{PY} = \{1, 5\}$

---
---

- **ALGORITHM 8.4. Algorithm dEclat**
  - ...

---

- Essa abordagem se mostrou muito eficiente para conjuntos densos
- Por√©m, em conjuntos esparsos, o algoritmo original √© a melhor op√ß√£o

Para bases esparsas: eClat; para bases densas: dEclat

#### Leitura (Aula 05)

- Se√ß√µes 8.1, 8.2 (Zaki e Meira)
- Se√ß√µes 6.1, 6.2 (Introduction to Data Mining)
- Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3): 372-390 (2000)
- Zaki, M.J., Gouda, K.: Fast vertical mining using diffsets. Technical Report 01-1, Computer Science Dept., Rensselaer Polytechnic Institute (March 2001) 10
- Christian Borgelt. Efficient Implementations of Apriori and Eclat. Workshop of Frequent Item Set Mining Implementations (FIMI 2003, Melbourne, FL, USA).

Boa parte da explica√ß√£o est√£o nos artigos. A implementa√ß√£o t√° no Borgelt.

### Slide: aula04-FPGrowth (Aula 05)

#### Recapitulando

- Apriori: reduzir o n√∫mero de passsadas em disco
- Eclat: trazer pra mem√≥ria e assim reduzir o n√∫mero de passadas em disco, tendendo a zero.

#### Introdu√ß√£o (Aula 05)

- Nessa aula, veremos outro algoritmo que usa proje√ß√µes para reduzir o n√∫mero de passadas para computa√ß√£o dos conjuntos de itens frequentes
- O algoritmo FP-Growth (Frequent Pattern Growth) adota uma estrat√©gia dividir-e-conquistar para reduzir o custo computacional
- Ele, ao contr√°rio do Apriori, n√£o se baseia na gera√ß√£o de candidatos
- Os padr√µes s√£o constru√≠dos ao longo do processamento em profundidade
- Esse algoritmo √©, talvez, o algoritmo sequencial mais eficiente para busca de conjuntos de itens frequentes

A ideia √© evitar ter que computar os candidatos.

#### FP-Growth

- O FP-Growth foi proposto em 2000 por Jiawei Han, Jian Pei e Yiwen Yin
- O algoritmo atacou dois problemas presentes nas abordagens iniciais:
  1. Repetidas passadas sobre a base de dados; e
  2. Gera√ß√£o de candidatos [Mais cr√≠tico]
- O primeiro problema, como j√° discutimos, √© cr√≠tico pelo custo computacional inerente √† leitura em mem√≥ria secund√°ria
- O segundo problema est√° relacionado √† gera√ß√£o de candidatos desnecess√°rios
  - Muitos s√£o descartados pela propriedade do Apriori

##### FP-Tree [√Årvore de Prefixos]

- O FP-Growth possui algumas similaridades ao Eclat:
  - Ambos adotam a estrat√©gia de busca em profundidade
  - Ambos adotam proje√ß√µes dos dados com o intuito de traz√™-los para mem√≥ria principal e reduzir o custo computacional
- O FP-Growth, no entanto, usa uma estrutura de dados diferente para suportar a busca pelos padr√µes
  - Uma √°rvore de prefixos chamada FP-Tree
- A busca pelos padr√µes se d√° inteiramente atrav√©s da √°rvore sem a necessidade de se voltar √† base de dados
- Dessa forma, a primeira tarefa do algoritmo √© construir essa estrutura

"A partir da Base de Dados, como fazer a √°rvore de prefixos?"

---

- A constru√ß√£o da FP-Tree ocorre em duas fases
- Primeiro, o algoritmo varre a base de dados para computar a frequ√™ncia individual de cada item
  - Itens infrequentes s√£o descartados, uma vez que n√£o podem formar padr√µes frequentes
- Segundo, o algoritmo percorre novamente a base processando as transa√ß√µes ordenadas pela frequ√™ncia dos itens
  - Os itens nas transa√ß√µes s√£o ordenados em ordem decrescente de frequ√™ncia e os infrequentes s√£o filtrados
- As transa√ß√µes s√£o ent√£o inseridas na √°rvore enquanto processadas
  - Itens s√£o n√≥s da √°rvore
  - Cada n√≥ armazena um item e sua frequ√™ncia (n√∫mero de transa√ß√µes que o cont√©m)

Basicamente, ele limpa os infrequentes, e depois disso, vai inserindo as transa√ß√µes em uma √°rvore.

---

- Para facilitar a busca pelos padr√µes, a √°rvore √© equipada com uma estrutura adicional para localizar a ocorr√™ncia dos itens e sua frequ√™ncia
- Exemplo

$$
\begin{bmatrix}
  TID & Muesli (a) & Oats (b) & Milk (c) & Yoghurt (d) & Biscuits (e) & Tea (f) \\
  1 & 1 & 0 & 1 & 1 & 0 & 1\\
  2 & 0 & 1 & 1 & 0 & 0 & 0\\
  3 & 0 & 0 & 1 & 0 & 1 & 1\\
  4 & 1 & 0 & 0 & 1 & 0 & 0\\
  5 & 0 & 1 & 1 & 0 & 0 & 1\\
  6 & 1 & 0 & 1 & 0 & 0 & 1\\
\end{bmatrix}
$$

minsup = 2

Em ordem de maior suporte pra menor suporte: cfabd

1. cfad
2. cb
3. cf
4. ad
5. cfb
6. cfa

Obs.: Ignoram-se os itens infrequentes.

#### Minera√ß√£o dos padr√µes [Aula 05]

- A minera√ß√£o dos padr√µes se inicia uma vez que a FP-Tree tenha sido constru√≠da
- A constru√ß√£o agora ocorre aumentando-se prefixos dos padr√µes em ordem crescente de suporte
- As transa√ß√µes que satisfa√ßam (cont√©m) o padr√£o sendo constru√≠do s√£o projetadas em uma nova √°rvore
- Itens podem se tornar infrequentes nessa nova base e s√£o descartados
- Os padr√µes encontrados nessa nova √°rvore devem incluir o prefixo que a gerou
- O algoritmo segue com as extens√µes recursivamente at√© que um √∫nico ramo seja obtido
  - Se a √°rvore possui um √∫nico ramo, os padr√µes obten√≠veis s√£o todas as combina√ß√µes dos n√≥s

Para se minerar as transa√ß√µes de volta, percorremos a lista de itens e ent√£o subimos dele at√© a raiz.

Partindo do item menos frequente e indo pro item mais frequente, fazemos proje√ß√µes da √°rvore.

Essas proje√ß√µes s√£o sub-√°rvores da √°rvore original.

No caso do d, percorrerei todos os n√≥s da lista encadeada de de d's, indo dele at√© a raiz. A jun√ß√£o de todos os n√≥s que eu passar, formar√° uma nova √°rvore. E essa ser√° a proje√ß√£o do item d.

Mas ainda n√£o entendi o que precisa ser feito ap√≥s essa primeira proje√ß√£o.

---

- Exemplo

$$
\begin{bmatrix}
  Item & Freq & Link \\
  c & 5 & \\
  f & 4 & \\
  a & 3 & \\
  b & 2 & \\
  d & 2 & \\
\end{bmatrix}
$$

```mermaid
flowchart LR
  Vazio(("$$\emptyset$$")) --> C(("$$c:5$$"))
  C --> F(("$$f:4$$"))
  F --> A2(("$$a:2$$"))
  A2 --> D1(("$$d:1$$"))
  F --> B1(("$$b:1$$"))
  C --> B2(("$$b:1$$"))
  Vazio --> A1(("$$a:1$$"))
  A1 --> D2(("$$d:1$$"))
```

H√° tamb√©m uma lista encadeada para todos os n√≥s com ocorr√™ncias de um mesmo item.

A lista encadeada serve para podermos percorrer todos os n√≥s de um mesmo item e calcularmos sua frequ√™ncia.

#### Continua na pr√≥xima aula

## Aula 06 | 03/04/2025 | Minera√ß√£o de conjuntos de itens

### Slide: aula04-FPGrowth (Aula 06)

#### FP-Growth (Aula 06)

- O FP-Growth foi proposto em 2000 por Jiawei Han, Jian Pei e Yiwen Yin
- O algoritmo atacou dois problemas presentes nas abordagens iniciais:
  1. Repetidas passadas sobre a base de dados; e
  2. Gera√ß√£o de candidatos [Mais cr√≠tico]
- O primeiro problema, como j√° discutimos, √© cr√≠tico pelo custo computacional inerente √† leitura em mem√≥ria secund√°ria
- O segundo problema est√° relacionado √† gera√ß√£o de candidatos desnecess√°rios
  - Muitos s√£o descartados pela propriedade do Apriori

##### FP-Tree [√Årvore de Prefixos] (Aula 06)

- O FP-Growth possui algumas similaridades ao Eclat:
  - Ambos adotam a estrat√©gia de busca em profundidade
  - Ambos adotam proje√ß√µes dos dados com o intuito de traz√™-los para mem√≥ria principal e reduzir o custo computacional
- O FP-Growth, no entanto, usa uma estrutura de dados diferente para suportar a busca pelos padr√µes
  - Uma √°rvore de prefixos chamada FP-Tree
- A busca pelos padr√µes se d√° inteiramente atrav√©s da √°rvore sem a necessidade de se voltar √† base de dados
- Dessa forma, a primeira tarefa do algoritmo √© construir essa estrutura

"A partir da Base de Dados, como fazer a √°rvore de prefixos?"

---

- A constru√ß√£o da FP-Tree ocorre em duas fases
- Primeiro, o algoritmo varre a base de dados para computar a frequ√™ncia individual de cada item
  - Itens infrequentes s√£o descartados, uma vez que n√£o podem formar padr√µes frequentes
- Segundo, o algoritmo percorre novamente a base processando as transa√ß√µes ordenadas pela frequ√™ncia dos itens
  - Os itens nas transa√ß√µes s√£o ordenados em ordem decrescente de frequ√™ncia e os infrequentes s√£o filtrados
- As transa√ß√µes s√£o ent√£o inseridas na √°rvore enquanto processadas
  - Itens s√£o n√≥s da √°rvore
  - Cada n√≥ armazena um item e sua frequ√™ncia (n√∫mero de transa√ß√µes que o cont√©m)

Basicamente, ele limpa os infrequentes, e depois disso, vai inserindo as transa√ß√µes em uma √°rvore.

---

- Para facilitar a busca pelos padr√µes, a √°rvore √© equipada com uma estrutura adicional para localizar a ocorr√™ncia dos itens e sua frequ√™ncia
- Exemplo

$$
\begin{bmatrix}
  TID & Muesli (a) & Oats (b) & Milk (c) & Yoghurt (d) & Biscuits (e) & Tea (f) \\
  1 & 1 & 0 & 1 & 1 & 0 & 1\\
  2 & 0 & 1 & 1 & 0 & 0 & 0\\
  3 & 0 & 0 & 1 & 0 & 1 & 1\\
  4 & 1 & 0 & 0 & 1 & 0 & 0\\
  5 & 0 & 1 & 1 & 0 & 0 & 1\\
  6 & 1 & 0 & 1 & 0 & 0 & 1\\
\end{bmatrix}
$$

#### Minera√ß√£o dos padr√µes [Aula 05] (Aula 06)

- A minera√ß√£o dos padr√µes se inicia uma vez que a FP-Tree tenha sido constru√≠da
- A constru√ß√£o agora ocorre aumentando-se prefixos dos padr√µes em ordem crescente de suporte
- As transa√ß√µes que satisfa√ßam (cont√©m) o padr√£o sendo constru√≠do s√£o projetadas em uma nova √°rvore
- Itens podem se tornar infrequentes nessa nova base e s√£o descartados
- Os padr√µes encontrados nessa nova √°rvore devem incluir o prefixo que a gerou
- O algoritmo segue com as extens√µes recursivamente at√© que um √∫nico ramo seja obtido
  - Se a √°rvore possui um √∫nico ramo, os padr√µes obten√≠veis s√£o todas as combina√ß√µes dos n√≥s

---

- Exemplo

$$
\begin{bmatrix}
  Item & Freq & Link \\
  c & 5 & \\
  f & 4 & \\
  a & 3 & \\
  b & 2 & \\
  d & 2 & \\
\end{bmatrix}
$$

```mermaid
flowchart LR
  Vazio(("$$\emptyset$$")) --> C(("$$c:5$$"))
  C --> F(("$$f:4$$"))
  F --> A2(("$$a:2$$"))
  A2 --> D1(("$$d:1$$"))
  F --> B1(("$$b:1$$"))
  C --> B2(("$$b:1$$"))
  Vazio --> A1(("$$a:1$$"))
  A1 --> D2(("$$d:1$$"))
```

H√° tamb√©m uma lista encadeada para todos os n√≥s com ocorr√™ncias de um mesmo item.

A lista encadeada serve para podermos percorrer todos os n√≥s de um mesmo item e calcularmos sua frequ√™ncia.

- [JV] Explica√ß√£o do Algoritmo
  - Para se minerar as transa√ß√µes de volta, percorremos a lista de itens e ent√£o subimos dele at√© a raiz.
  - Partindo do item menos frequente e indo pro item mais frequente, fazemos proje√ß√µes da √°rvore.
  - Essas proje√ß√µes s√£o sub-√°rvores da √°rvore original.
  - No caso do d, percorrerei todos os n√≥s da lista encadeada de de d's, indo dele at√© a raiz. A jun√ß√£o de todos os n√≥s que eu passar, formar√° uma nova √°rvore. E essa ser√° a proje√ß√£o do item d.
  - Mas ainda n√£o entendi o que precisa ser feito ap√≥s essa primeira proje√ß√£o.
- [JV] Explica√ß√£o 2
  - Primeiro filtra pelos itens frequentes, removendo os infrenquentes.
    - Ex: minsup = 2
  - Depois disso, ele faz a... "transposi√ß√£o horizontal(?)", ou seja, para cara transa√ß√£o, ele lista todos os itens frequentes que est√£o presentes nela.
  - Ent√£o ordena cada um desses itens por seu suporte.
    - Ex:
      - Em ordem de maior suporte pra menor suporte: cfabd; Obs.: Ignoram-se os itens infrequentes.
      1. cfad
      2. cb
      3. cf
      4. ad
      5. cfb
      6. cfa
  - Depois disso, ele vai inserindo esses itens em uma √°rvore de prefixos, em sequ√™ncia: do primeiro TID at√© o √∫ltimo TID, depois do primeiro item at√© o √∫ltimo item.
  - D√∫vida JV: Qual √© a sequ√™ncia para se percorrer as transa√ß√µes? Da primeira pra √∫ltima? Poder√≠amos ordenar as transa√ß√µes por suporte, e depois fazer a inser√ß√£o na √°rvore? Haveria benef√≠cio ao fazermos isso?
    - Resposta: Sim, a ideia √© percorrer as transa√ß√µes na ordem em que elas aparecem. Por√©m, existe sim benef√≠cio em ordenar, mas n√£o direi agora.
  - √Ä medida em que insere, atualiza a tabela de frequ√™ncia dos itens.
  - Depois de todos preenchidos, ele percorrer√° a tabela das frequ√™ncias, partindo do item menos frequente
  - Agora, percorrendo a lista encadeada do item menos frequente, ele vai subindo at√© a raiz, e ent√£o vai criando uma nova √°rvore de prefixos, que ser√° a proje√ß√£o dos sufixos do item menos frequente.
  - Por√©m, ao inv√©s de fazer uma lista dos sufixos, ele faz uma sub-√°rvore que representa todos os sufixos do item que estamos percorrendo. Por√©m, omitindo o item em si.
  - Ap√≥s criada essa sub-√°rvore, podaremos os itens que n√£o s√£o frequentes, segundo a sub-tabela de frequ√™ncias dessas sub-√°rvores.
  - Ent√£o √© feito um merge dos ramos que sobraram, somando os suportes dos itens que sobraram.
    - Eu estimo que esse merge seja feito partindo da raiz, e conferindo se todos os seus filhos n√£o diferentes entre si. Caso sejam iguais, eles s√£o mesclados e seus filhos tamb√©m, assim somando e unindo recursivamente.
  - Agora sim s√£o gerados os itemsets de padr√µes frequentes ao gerar todas as possibilidades partindo do conjunto vazio que √© a raiz, e parando em cada um dos n√≥s que sobraram.
    - **OBS.: ESSA ETAPA S√ì OCORRE CASO A √ÅRVORE TENHA SOMENTE UM RAMO. SE A √ÅRVORE TIVER MAIS DE UM RAMO, O ALGORITMO SE REPETE RECURSIVAMENTE, AT√â QUE A √ÅRVORE TENHA SOMENTE UM RAMO.**
    - Suponho tamb√©m que, na √∫ltima recurs√£o, o algoritmo, mesmo que sejam podados todos os n√≥s vis√≠veis (n√£o considerando o $\emptyset$), ele ainda assim gere os padr√µes frequentes, sendo ele o item da qual a √°rvore √© proje√ß√£o.

#### Quest√µes de implementa√ß√£o

- E se a FP-tree n√£o couber na mem√≥ria?
  - A solu√ß√£o √© particionar/projetar a base de dados em mem√≥ria secund√°ria antes de iniciar a constru√ß√£o da √°rvore
- Como construir a FP-tree de forma eficiente?
  - Solu√ß√£o proposta por Christian Borgelt otimiza mem√≥ria e tempo
  - Representa√ß√£o b√°sica dos dados: lista de vetores de inteiros
  - Dados (proje√ß√µes) s√£o carregados inteiramente para mem√≥ria
  - Lista √© seccionada com base no k-√©simo item; um n√≥ √© criado para cada se√ß√£o
  - N√≥s t√™m tamanho fixo (20 bytes em 32bits; 40 em 64bits)
    - 1x identificador de item
    - 1x contador de frequ√™ncia
    - 1x ponteiro para n√≥ pai
    - 1x ponteiro para pr√≥xima ocorr√™ncia do item
    - 1x ponteiro para n√≥ auxiliar

- [JV] Explica√ß√£o da cria√ß√£o
  - Ao inv√©s de fazer a FP-Tree, ele primeiro ordena todos as transa√ß√µes, ele, recursivamente:
    - agrupa elas pelo prefixo inicial. Cada prefixo inicial ser√° um n√≥ apontando ao seu pai. Inicialmente sendo o pai o n√≥ raiz, o $\emptyset$.
    - E ent√£o repete isso para cada um dos sufixos que sobraram, at√© que n√£o haja mais sufixos.

---

- Implementa√ß√£o tradicional, conforme descri√ß√£o do algoritmo, evita carregar dados para mem√≥ria
  - Por√©m, n√≥s ter√£o tamanho vari√°vel ou desperdi√ßam mem√≥ria (ponteiros para filhos que nunca ocorrem)
  - Melhora gerenciamento de mem√≥ria; grandes blocos podem ser alocados de uma vez e gerenciados internamente
  - Al√©m disso, ponteiros para pais s√£o mais √∫teis que ponteiros para filhos durante execu√ß√£o

---

- Proje√ß√µes s√£o executadas com dois la√ßos
  - Um la√ßo externo percorre o n√≠vel mais baixo (elemento condicionante da proje√ß√£o)
  - La√ßo interno percorre a os ramos origin√°rios do n√≥ folha
- A nova √°rvore √© constru√≠da como uma 'sombra da original
  - N√≥s s√£o duplicados conforme s√£o visitados (ponteiro auxiliar mant√©m elo de liga√ß√£o entre original e c√≥pia para atualiza√ß√µes necess√°rias durante constru√ß√£o)
  - Frequ√™ncia do n√≥ folha √© propagada para cima
- A sombra √© destacada da √°rvore original em uma segunda passada pelos n√≥s
- N√≥s infrequentes podem ser removidos e n√≥s com mesmo r√≥tulo mesclados

---

[Duas imagens do comparativo das efici√™ncias do FP-Growth, Eclat e Apriori]

---

#### Leitura [Aula 05]

- Se√ß√£o 6.6 Intro to Data Mining
- Se√ß√£o 8.2.3 Zaki e Meira
- [Borgelt, C. (2005) An Implementation of the FP-growth Algorithm][LinkFPGrowth]

[LinkFPGrowth]: <https://borgelt.net/papers/fpgrowth.pdf>

### Slide: aula05-repr-compactadas (Aula 06)

#### Introdu√ß√£o (Aula 06)

- Nessa aula, vamos discutir representa√ß√µes compactas para o conjunto de todos os conjuntos de itens frequentes de uma base de dados
- Representa√ß√µes compactas s√£o subconjuntos a partir dos quais √© poss√≠vel derivar todos os conjuntos de itens frequentes
- Para motivar a necessidade dessas representa√ß√µes, considere uma base de dados com somente duas transa√ß√µes e 100 itens:
  - $D = \{(0, a_{1}, a_{2}, \dots, a_{50}), (1, a_{1}, a_{2}, \dots, a_{100})\}$
- Se considerarmos um minsup=1, essa base ter√°
  - $\binom{100}{1} + \binom{100}{2} + \dots + \binom{100}{100} = 2^{100} - 1 \equiv 1.27E^{30}$

- [JV]
  - Podemos considerar que:
    - $01 = a_{1}a_{2}\dots a_{50}$
    - $ 1 = a_{1}a_{2}\dots a_{100}$
  - O que seriam representa√ß√µes compactas do conjunto de itemsets frequentes

---

- Nesse caso, o problema se torna incomput√°vel por qualquer das abordagens que vimos anteriormente
- Embora esse seja um caso extremo, n√£o √© raro que situa√ß√µes similares a essa ocorram na pr√°tica
  - √â poss√≠vel, por exemplo, que um subconjunto das transa√ß√µes e itens apresentem esse comportamento em uma base de dados maior
- Note que podemos particionar os itemsets frequentes em duas classes de equival√™ncia:
  - Os que ocorrem em ambas as transa√ß√µes; e
  - Os que ocorrem somente na segunda

---

- Os que ocorrem em ambas as transa√ß√µes, possuem cobertura $c(X)=01$, e, portanto, s√£o equivalentes a $a_{1}a_{2}\dots a_{50}$
- Os que ocorrem somente na segunda transa√ß√£o, possuem cobertura $c(X)=1$, e, portanto, s√£o equivalentes a $a_{1}a_{2}\dots a_{100}$
  - Al√©m disso, se estiv√©ssemos somente interessados nos itemsets frequentes sem a informa√ß√£o da frequ√™ncia, todos seriam equivalentes a esse itemset
- Em outras palavras, os mais de $10^{30}$ itemsets que seriam retornados por qualquer dos algoritmos vistos poderiam ser representados somente por esses dois conjuntos
- Esses conjuntos formam, dessa forma, uma representa√ß√£o compacta de todo o conjunto de itemsets frequentes
- Em particular, eles est√£o relacionados a dois tipos de representa√ß√µes compactas que veremos nessa aula
  - Conjuntos frequentes **m√°ximos**
  - Conjuntos frequentes **fechados**

#### Representa√ß√µes compactas

- O particionamento dos itemsets no exemplo anterior se deu pelos tidsets que compunham suas extens√µes
- De fato, o racioc√≠nio se aplica a qualquer base de dados. Ou seja, podemos particionar os itemsets conforme sua cobertura
- Dentro de cada classe de equival√™ncia, podemos ordenar os elementos conforme a rela√ß√£o de subconjunto
  - O maior elemento da classe √© chamado de conjunto fechado ou **closed itemset**
    - [JV:] O maior item poss√≠vel dos itemsets. Exemplo: $a_{1}a_{2}\dots a_{50}$ || $P = c(i(P))$
  - Os menores elementos da classe s√£o chamados de **minimal generators**
    - [JV:] Exemplo: cada um dos itenzinhos que foram concatenados. ($a_{1}, a_{2}, \dots, a_{50}$) || $X = i(c(X))$
- Os maiores elementos entre todos os conjuntos fechados s√£o chamados de conjuntos frequentes m√°ximos (**maximal itemsets**)
  - [JV:] O maior itemset poss√≠vel entre todos os conjuntos fechados. Exemplo: $a_{1}a_{2}\dots a_{100}$ || "S√£o os maiores itemsets frequentes"

---

- Os conjuntos m√°ximos s√£o os maiores itemsets frequentes
- Eles definem a 'borda entre o que √© frequente e infrequente
- Como, por defini√ß√£o, n√£o existem conjuntos frequentes maiores que eles, **todos os conjuntos frequentes podem ser derivados a partir dos conjuntos m√°ximos**
- No entanto, o c√°lculo do suporte n√£o pode ser obtido diretamente desses itemsets, sendo necess√°ria uma nova passada na base de dados para comput√°-lo
- O itemset $a_{1}a_{2} \dots a_{100}$ no nosso exemplo inicial √© um conjunto frequente m√°ximo

---

- Essa necessidade de novas passadas na base de dados para computar o suporte dos itemsets frequentes a partir dos m√°ximos torna a representa√ß√£o incompleta
- Os conjuntos fechados, por outro lado, s√£o uma representa√ß√£o completa, j√° que tanto os itemsets quanto seu suporte podem ser derivados desses conjuntos
- Como dito, todo conjunto fechado d√° origem a uma classe de equival√™ncia
  - $[X] = \{Y \subseteq I | c(Y) = c(X)\} = \{Y \subseteq I | i(c(Y)) = X\}$
- Assim, podemos verificar o suporte de um itemset frequente a partir dos conjuntos fechados da seguinte forma
  - $sup(ùëã) = max \{sup(Y) | Y \in \mathcal{C} \wedge X \subseteq Y\}$
  - Em outras palavras, basta encontrarmos a classe de equival√™ncia √† qual o itemset pertence; todo itemset frequente ou √© fechado ou pertence √† classe de equival√™ncia de algum conjunto fechado, como o suporte √© anti-monot√¥nico, se ele n√£o for fechado, ele pertence √† classe do de maior suporte.

---

- Exemplo: minsup=1

|  TID | Muesli (m) | Oats (o) | Milk (m) | Yoghurt (y) |
| ---: | ---------: | -------: | -------: | ----------: |
|    1 |          1 |        0 |        1 |           1 |
|    2 |          0 |        1 |        1 |           0 |
|    3 |          0 |        0 |        1 |           0 |
|    4 |          1 |        0 |        0 |           1 |
|    5 |          0 |        1 |        1 |           0 |
|    6 |          1 |        0 |        1 |           0 |

---

```mermaid
flowchart LR
  
  %% Styles
  classDef Infrequent fill:#ffffff, color:#000000;
  classDef Frequent fill:#fffba6, color:#000000;
  classDef Maximal fill:#fffba6, color:#000000, stroke-width:5px, stroke:#000000;

  %% V√©rtices
  MOKY(("MOKY $$\{\}$$")):::Infrequent
  MOK(("MOK $$\{\}$$")):::Infrequent
  MOY(("MOY $$\{\}$$")):::Infrequent
  MKY(("MKY $$1$$")):::Maximal
  OKY(("OKY $$\{\}$$")):::Infrequent
  MO(("MO $$\{\}$$")):::Infrequent
  MK(("MK $$16$$")):::Frequent
  MY(("MY $$14$$")):::Frequent
  OK(("OK $$25$$")):::Maximal
  OY(("OY $$\{\}$$")):::Infrequent
  KY(("KY $$\{\}$$")):::Infrequent
  M(("M $$146$$")):::Frequent
  O(("O $$25$$")):::Frequent
  K(("K $$12356$$")):::Frequent
  Y(("Y $$14$$")):::Frequent
  Vazio(("$$\emptyset$$")):::Frequent

  %% Label
  Freq[Frequent Itemsets]:::Infrequent
  Infreq[Infrequent Itemsets]:::Frequent
  Max[Maximal Itemsets]:::Maximal

  %% Arestas
  MOKY --- MOK & MOY & MKY & OKY
  MOK --- MO & MK & OK
  MOY --- MO & MY & OY
  MKY --- MK & MY & KY
  OKY --- OK & OY & KY
  MO --- M & O
  MK --- M & K
  MY --- M & Y
  OK --- O & K
  OY --- O & Y
  KY --- K & Y
  M --- Vazio
  O --- Vazio
  K --- Vazio
  Y --- Vazio
```

---

```mermaid
flowchart LR
  
  %% Styles
  classDef Infrequent fill:#ffffff, color:#000000;
  classDef Frequent1 fill:#fffba6, color:#000000;
  classDef Frequent2 fill:#fffba6, color:#000000;
  classDef Frequent3 fill:#fffba6, color:#000000;
  classDef Maximal fill:#fffba6, color:#000000, stroke-width:5px, stroke:#000000;

  %% V√©rtices
  MOKY(("MOKY $$\{\}$$")):::Infrequent
  MOK(("MOK $$\{\}$$")):::Infrequent
  MOY(("MOY $$\{\}$$")):::Infrequent
  MKY(("MKY $$1$$")):::Maximal
  OKY(("OKY $$\{\}$$")):::Infrequent
  MO(("MO $$\{\}$$")):::Infrequent
  MK(("MK $$16$$")):::Frequent
  MY(("MY $$14$$")):::Frequent
  OK(("OK $$25$$")):::Maximal
  OY(("OY $$\{\}$$")):::Infrequent
  KY(("KY $$\{\}$$")):::Infrequent
  M(("M $$146$$")):::Frequent
  O(("O $$25$$")):::Frequent
  K(("K $$12356$$")):::Frequent
  Y(("Y $$14$$")):::Frequent
  Vazio(("$$\emptyset$$")):::Frequent

  %% Label
  Freq[Frequent Itemsets]:::Infrequent
  Infreq[Infrequent Itemsets]:::Frequent
  Max[Maximal Itemsets]:::Maximal

  %% Arestas
  MOKY --- MOK & MOY & MKY & OKY
  MOK --- MO & MK & OK
  MOY --- MO & MY & OY
  MKY --- MK & MY & KY
  OKY --- OK & OY & KY
  MO --- M & O
  MK --- M & K
  MY --- M & Y
  OK --- O & K
  OY --- O & Y
  KY --- K & Y
  M --- Vazio
  O --- Vazio
  K --- Vazio
  Y --- Vazio
```

Os azuis e verdes s√£o classes de equival√™ncia.

#### Algoritmos para encontrar representa√ß√µes compactas

- Os exemplos mostram que as representa√ß√µes compactas apresentam vantagens sobre o conjunto de todos os itemsets frequentes
- No entanto, se usarmos os algoritmos vistos anteriormente para encontrar essas representa√ß√µes, n√£o temos ganho computacional algum em rela√ß√£o ao problema anterior
  - Pode ser que a minera√ß√£o desses padr√µes continue invi√°vel
- Existem diversos algoritmos espec√≠ficos para minera√ß√£o de itemsets m√°ximos e fechados
- Veremos um representante de cada desses algoritmos

"Voc√™ consegue encontrar todos ..."

##### DCI_Closed

- O algoritmo DCI_Closed foi proposto em 2004 por C. Lucchese, S. Orlando e R. Perego
- Ele tamb√©m adota uma representa√ß√£o vertical da base de dados para facilitar a verifica√ß√£o dos conjuntos fechados
- O algoritmo explora o espa√ßo de busca usando uma estrat√©gia dividir- e-conquistar
- Os autores demonstraram que o problema pode ser decomposto em subproblemas independentes, permitindo inclusive uma solu√ß√£o paralela

---

- A ideia central do algoritmo √© 'escalar' o reticulado de itemsets, percorrendo cada classe de equival√™ncia uma √∫nica vez
- Somente um candidato de cada classe √© avaliado para computar o seu conjunto fechado
- Novamente, assume-se uma ordem lexicogr√°fica sobre os itens da base, e sua extens√£o sobre os itemsets
  - Qualquer ordem serve, inclusive a ordem sobre os r√≥tulos dos itens
  - Essa ordem ser√° representado por $\prec$
- Novos candidatos s√£o gerados a partir dos conjuntos fechados obtidos, estendendo-os com itens ainda n√£o investigados
  - Esses candidatos s√£o chamados de **geradores**
  - Formalmente, um gerador √© um conjunto $X = Y_i$, para um conjunto fechado $Y$ e um item $i$

---

- Um gerador $X = Y_i$ √© dito ordem-conservante sse $i \prec (i(c(X)) - X)$
  - Em palavras, $X$ √© ordem-conservante se todo item que tiver que ser adicionado a X para obter o conjunto fechado for maior que $i$
- Teorema 1: Para todo conjunto fechado $Y \neq i(c(\emptyset))$, existe uma sequ√™ncia de $n \geq 1$ extens√µes (items) $i_0 \prec i_1 \prec \dots \prec i_{n-1}$ tais que $gen_0 = Y_0i_0$, $gen_1 = Y_1i_1$, $gen_{n-1} = Y_{n-1}i_{n-1}$, em que todos os $gen_k$ s√£o ordem- conservantes, $Y_0 = i(c(\emptyset))$, $Y_{j+1} = i(c(Y_ji_j))$ e $Y_n=Y$.
- Corol√°rio: Essa sequ√™ncia √© √∫nica.

---

- O problema agora √© verificar se um gerador √© ordem-conservante
- Lema 1: Seja $gen = Y_i$, para um conjunto fechado $Y$ e item $i$. Se $\exists j $\prec$ i [j \notin gen \wedge c(gen) \subseteq c(j)]$, ent√£o $gen$ n√£o √© ordem-conservante.
  - Intuitivamente, $c(gen) \subseteq c(j)$ implica em $j \in i(c(gen))$, e como $j \notin gen$, $j \in i(c(gen)) - gen$; ou seja, $i \nprec i(c(gen)) - gen$
- Sendo assim, basta mantermos uma lista de elementos menores que $i$ n√£o pertencentes a $gen$ para verificarmos se ele √© ordem-conservante durante a execu√ß√£o do algoritmo
  - Essa lista √© chamada de **pre-set**
  - N√£o h√° necessidade de manter os conjuntos fechados em mem√≥ria!
- O espa√ßo de busca pode ser percorrido a partir de $i(c(\emptyset))$ e todos os itens frequentes como poss√≠veis extens√µes
  - Os geradores s√£o avaliados conforme a ordem lexicogr√°fica
  - Se encontrarmos um gerador n√£o ordem-conservante, podamos o ramo
  - Ap√≥s explorar o ramo com um item $i$, ele √© colocado no **pre-set**

---

- **procedure** $DCI\_Closed_d$ (CLOSED_SET, PRE_SET, POST_SET)
  - **while** POST_SET $neq \emptyset$ **do**
    - $i \leftarrow min_{\prec}$ (POST_SET)
    - POST_SET $leftarrow$ POST_SET \ $i$
    - $new\_gen \leftarrow$ CLOSED_SET $\cup i$ \\\\ Build a new generator
    - **if** $supp(new\_gen) \geq minsupp$ **then**
      - $\neg$ is_dup (new_gen, PRE_SET) **then** \\\\ if $new\_gen$ is both frequent and order preserving
      - CLOSED_SET $_{New} \leftarrow new\_gen$
      - POST_SET $_{New} \leftarrow \emptyset$
      - **for all** $j \in$ POST_SET **do** \\\\ Compute closure of $new\_gen$
        - **if** $g(new\_gen) \subseteq g(j)$ **then**
          - CLOSED_SET $_{New} \leftarrow$ CLOSED_SET $_{New} \cup j$
        - **else**
          - POST_SET $_{New} \leftarrow$ POST_SET$_{New} \cup j$
        - **end if**
      - **end for**
      - **Write Out** CLOSED_SET $_{New}$ _and its support_
      - DCI_Closed $_d$ CLOSED_SET $_{New}$, PRE_SET, POST_SET $_{New}$
      - PRE_SET $leftarrow$ PRE_SET $\cup i$
    - **end if**
  - **end while**
- **end procedure**

---

- **function** $is\_dup$ ($new\_gen$, PRE_SET) \\ Duplicate check
  - **for all** $j \in$ PRE_SET **do**
    - **if** $g(new\_gen) \subseteq g(j)$ **then**
      - **return** TRUE \\ $new\_gen$ is not order preserving
    - **end if**
  - **end for**
  - **return** FALSE
- **end function**

---

- Exemplo: minsup = 2

$$
\begin{bmatrix}
  TID & Muesli (a) & Oats (b) & Milk (c) & Yoghurt (d) & Biscuits (e) & Tea (f) \\
  1 & 1 & 0 & 1 & 1 & 0 & 1\\
  2 & 0 & 1 & 1 & 0 & 0 & 0\\
  3 & 0 & 0 & 1 & 0 & 1 & 1\\
  4 & 1 & 0 & 0 & 1 & 0 & 0\\
  5 & 0 & 1 & 1 & 0 & 0 & 1\\
  6 & 1 & 0 & 1 & 0 & 0 & 1\\
\end{bmatrix}
$$

##### MAFIA: Maximal Frequent Itemset Algorithm

- O algoritmo MAFIA foi proposto em 2001 por D. Burdick, M. Calimlim e J. Gehrke
- A ideia geral do algoritmo √© a de explorar o reticulado de itemsets com uma abordagem best-first/branch-and-bound
- Assim como o Eclat, o MAFIA tamb√©m assume uma representa√ß√£o vertical dos dados usando vetores de bits
- O algoritmo tamb√©m assume uma ordem lexicogr√°fica sobre os itens e a ordem parcial de subconjuntos entre os itemsets durante a explora√ß√£o

---

- Durante a explora√ß√£o do espa√ßo de busca, o algoritmo divide os itens em dois grupos
  - **Head**: contendo o r√≥tulo do n√≥ corrente (itemset) na explora√ß√£o
  - **Tail**: os itens que s√£o maiores que o maior elemento do Head (poss√≠veis extens√µes para o itemset)
- O conjunto de todos os itens que podem aparecer numa dada sub√°rvore √© a uni√£o entre o head e o tail (chamado de **HUT** - head union tail - pelos autores)
- Ao inv√©s de adotar uma explora√ß√£o puramente em profundidade, em cada n√≥, o algoritmo avalia os filhos imediatos para remover poss√≠veis extens√µes do tail
  - Eles chamam essa estrat√©gia de **reordenamento din√¢mico (dynamic reordering)**

---

- Ao explorar o n√≥ P na figura ao lado
  - Head = 1
  - Tail = 234
  - HUT = 1234
- Antes de explorar em profundidade o n√≥, o algoritmo avalia os filhos 12, 13 e 14
- Como s√≥ 12 √© frequente, 3 e 4 podem ser removidos do tail porque qualquer candidato dessa sub√°rvore que inclua esses itens ser√° infrequente
  - O ramo de 12 √© podado

---

- Note que, sempre que uma folha √© visitada, um candidato a itemset m√°ximo √© encontrado
  - Ele ser√° inclu√≠do na solu√ß√£o final somente se n√£o possuir um superconjunto j√° inclu√≠do
  - A visita√ß√£o em ordem lexicogr√°fica em profundidade garante que conjuntos n√£o tenham que ser removidos da solu√ß√£o final
- Outra poda vem do fato de que itemsets m√°ximos s√£o tamb√©m fechados e que, portanto, para um itemset $X$ (head) e $y \in X.tail$:
  - Se $c(X) \subseteq c(y)$, ent√£o $X_y \subseteq i(c(X))$ (isto √©, $y$ pertence ao conjunto fechado da classe √† qual $X$ pertence)
  - Nesse caso, $y$ pode ser incorporado ao head e removido do tail
- Essa poda √© chamada de Parent Equivalence Pruning (PEP)

---

- Finalmente, podemos usar a propriedade do Apriori para descartar um ramo baseado no HUT
  - Lembrando que HUT √© a uni√£o do head com o tail e representa o maior itemset que pode ser obtido a partir dessa sub√°rvore
- Se o HUT for subconjunto de algum itemset m√°ximo j√° descoberto anteriormente, sabemos que ele e todos os seus subconjuntos s√£o frequentes, e, al√©m disso, n√£o podem ser m√°ximos. Portanto, podemos podar a sub√°rvore.

---

- Pseudocode: _MAFIA_ (**C**, **MFI**, Boolean **IsHUT**)
  - name **HUT** = **C**.head $\cup$ **C**.tail
  - if **HUT** is in **MFI**
    - Stop generation of children and return
  - Count all children, use PEP to trim the tail, and reorder by increasing support
    - For each item **i** in **C**.trimmed_tail
      - **IsHUT** = whether **i** is in the first item in the trail
      - **newNode** = **C** $\cup$ **I**
      - _MAFIA_ (**newNode, MFI, IsHUT**)
    - if (**IsHUT** and all extensions are frequent)
      - Stop search and go back up subtree
    - if (**C** is a leaf and **C**.head is not in **MFI**)
      - Add **C**.head to **MFI**

---

- Exemplo: minsup = 2

| **TID** | **Muesli (a)** | **Oats (b)** | **Milk (c)** | **Yoghurt (d)** | **Biscuits (e)** | **Tea (f)** |
| ------: | -------------: | -----------: | -----------: | --------------: | ---------------: | ----------: |
|       1 |              1 |            0 |            1 |               1 |                0 |           1 |
|       2 |              0 |            1 |            1 |               0 |                0 |           0 |
|       3 |              0 |            0 |            1 |               0 |                1 |           1 |
|       4 |              1 |            0 |            0 |               1 |                0 |           0 |
|       5 |              0 |            1 |            1 |               0 |                0 |           1 |
|       6 |              1 |            0 |            1 |               0 |                0 |           1 |

##### Leitura (Aula 06)

- Se√ß√£o 9.1 Zaki e Meira
- Se√ß√£o 6.2.6 Han et al.
- Burdick, D., Calimlim, M., & Gehrke, J. (2001, April). Mafia: A maximal frequent itemset algorithm for transactional databases. In Proceedings 17th international conference on data engineering (pp. 443-452). IEEE.
- Lucchese, C., Orlando, S., & Perego, R. (2004, November). DCI Closed: A Fast and Memory Efficient Algorithm to Mine Frequent Closed Itemsets. In FIMI.

## Aula 07 | 08/04/2025 | Minera√ß√£o de sequ√™ncias

### Aula 08 | 10/04/2025 | Minera√ß√£o de sequ√™ncias

### Aula 09 | 15/04/2025 | Minera√ß√£o de grafos

### Aula 10 | 17/04/2025 | Minera√ß√£o de grafos

### Aula 11 | 22/04/2025 | Regras de associa√ß√£o e m√©tricas de qualidade

### Aula 12 | 24/04/2025 | Aprendizado descritivo supervisionado: padr√µes emergentes, contrastantes e descoberta de subgrupos

### Aula 13 | 29/04/2025 | Descoberta de subgrupos

### Aula 14 | 06/05/2025 | Descoberta de subgrupos

### Aula 15 | 08/05/2025 | Descoberta de subgrupos

### Aula 16 | 13/05/2025 | Minera√ß√£o de modelos excepcionais

### Aula 17 | 15/05/2025 | Minera√ß√£o de modelos excepcionais

### Aula 18 | 20/05/2025 | Minera√ß√£o de modelos excepcionais

### Aula 19 | 22/05/2025 | Semin√°rios (Padr√µes Frequentes)

### Aula 20 | 27/05/2025 | Semin√°rios (Padr√µes Frequentes)

### Aula 21 | 29/05/2025 | Semin√°rios (Padr√µes Frequentes)

### Aula 22 | 03/06/2025 | Semin√°rios (SD)

### Aula 23 | 05/06/2025 | Semin√°rios (SD)

### Aula 24 | 10/06/2025 | Semin√°rios (SD)

### Aula 25 | 12/06/2025 | Semin√°rios (aplica√ß√µes)

### Aula 26 | 17/06/2025 | Semin√°rios (aplica√ß√µes)

### Aula 27 | 24/06/2025 | Semin√°rios (aplica√ß√µes)

### Aula 28 | 26/06/2025 | Projeto

### Aula 29 | 01/07/2025 | Projeto

### Aula 30 | 03/07/2025 | Projeto
