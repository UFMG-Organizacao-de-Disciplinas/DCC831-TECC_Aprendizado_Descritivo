# Aprendizado Descritivo - Renato Vimieiro - 2025.1

<!-- ‚Äò‚Äô -->

## Aula 01 | 18/03/2025 | Apresenta√ß√£o do curso - [JV: Cheguei atrasado]

### Slide 1 - aula01-intro

#### Introdu√ß√£o

- Quando falamos sobre aprendizado de m√°quina e minera√ß√£o de dados, frequentemente associamos essas express√µes a predi√ß√£o de valores
- Mais especificamente, temos a ideia de que aprendizado de m√°quina (AM) se resume a, dada uma entrada X, encontrar uma fun√ß√£o f(X) que retorne o valor de uma vari√°vel alvo Y
  - Quando a vari√°vel alvo Y √© categ√≥rica, chamamos o problema de **classifica√ß√£o**
  - Quando a vari√°vel alvo Y √© cont√≠nua, chamamos o problema de **regress√£o**
- Assim, o senso comum define AM como aprender uma fun√ß√£o f capaz de predizer valores para dados ainda n√£o coletados
- Essa defini√ß√£o, embora restritiva, √© correta para a classe de tarefas elencadas acima, chamadas de **aprendizado preditivo**

#### Aprendizado Preditivo

- Mais especificamente ainda, esse imagin√°rio popular define o que conhecemos por aprendizado supervisionado de modelos preditivos
- De maneira mais formal, o objetivo dessas t√©cnicas √© aprender uma fun√ß√£o cujo dom√≠nio √© um conjunto de inst√¢ncias $\mathcal{X}$, e contradom√≠nio um conjunto de sa√≠das $\mathcal{Y}$
- Para tal, o algoritmo recebe um conjunto de pares $(x, l(x)) \in \mathcal{X} \times \mathcal{L}$
  - A fun√ß√£o $l(x)$ retorna o valor de sa√≠da esperado para a inst√¢ncia x
- Como o algoritmo obt√©m o modelo guiado por esse conjunto de entrada (treinamento), a tarefa √© classificada como 'supervisionada', j√° que $l(x)$ faz o papel de 'professor'
- Da mesma forma, como o modelo aprendido √© usado para predizer valores de sa√≠da de novas inst√¢ncias, ele √© chamado de preditivo

- JV
  - O que desejamos √© receber informa√ß√µes e conseguir retornar um r√≥tulo.
  - Nem todos de aprendizado de m√°quina levam em considera√ß√£o os r√≥tulos.
  - Aprendizado supervisionado preditivo
  - [JV: n√£o anotei sobre o que seria o supervisionado]
  - Se tiver que explicar como funciona, a√≠ √© a √°rea de Explanable AI.
  - Aqui √© mais importante o resultado do que o processo.

---

- Como queremos que o modelo seja fiel √† realidade e, portanto, consiga capturar a rela√ß√£o entre inst√¢ncia (entrada) e sa√≠da, √© comum, nesse cen√°rio, avaliarmos sua qualidade comparando os valores obtidos com os verdadeiros para um conjunto de inst√¢ncias chamado de conjunto de valida√ß√£o
- Ou seja, nesse tipo de tarefa temos o seguinte fluxo de trabalho:

---

```mermaid
flowchart LR
  Trat[(Tratamento)]
  Cons[Constru√ß√£o]
  Ava[Avalia√ß√£o]
  Uso["Uso (Modelo)"]
  Val[(Valida√ß√£o)]
  Trat --> Cons
  Cons --> Ava
  Ava --> Uso
  Ava --> Cons
  Val --> Ava
```

Aqui, n√£o queremos obter nenhum entendimento sobre o que j√° vimos antes, mas sim, ter meios de prever como ser√£o classificados os pr√≥ximos itens que veremos.

---

- Embora m√©todos de regress√£o e classifica√ß√£o (aprendizado supervisionado) sejam os -ais populares em AM, existem outras abordagens preditivas que n√£o requerem a vari√°vel -lvo para ajustarem modelos
- T√©cnicas que n√£o utilizam essas vari√°veis s√£o classificadas como **n√£o-supervisionadas**
- Uma tarefa de aprendizado n√£o-supervisionado bastante popular √© a de agrupamento (clustering)
- Essa tarefa consiste em encontrar subgrupos de elementos homog√™neos nos dados

---

---

- [JV]
  - Exemplo: c√¢ncer de mama
    - Separando mulheres cuja quimioterapia foram eficazes e as que n√£o foram.
    - Analisar quais os mapeamentos gen√©ticos delas
    - Verificar de que forma h√° uma rela√ß√£o entre os mapeamentos gen√©ticos e a efic√°cia da quimioterapia.
    - E com isso, tentar predizer se a quimioterapia ser√° eficaz ou n√£o para uma nova paciente.
  - No aprendizado n√£o supervisionado n√£o h√° r√≥tulos.
  - Uma tarefa de aprendizado n√£o-supervisionado bastante popular √© a de agrupamento (clustering)
    - Um dos mais conhecidos √© o k-menas

---

- Em outras palavras, a tarefa de agrupamento consiste em detectar grupos de inst√¢ncias que sejam mais parecidas entre si do que com as de outros grupos
- Sob a √≥tica de aprendizado preditivo, a tarefa consiste em encontrar uma fun√ß√£o $q: \mathcal{X} \rightarrow \mathcal{C}$, cujo dom√≠nio $\mathcal{X}$ √© um conjunto de inst√¢ncias, e o contradom√≠nio ùíû um conjunto de grupos (clusters)
- Note que a tarefa se assemelha √† de classifica√ß√£o (j√° que os r√≥tulos dos grupos s√£o categ√≥ricos), por√©m o ajuste do modelo n√£o leva em considera√ß√£o r√≥tulos pr√©-definidos
- Um exemplo de m√©todo aprendizado preditivo n√£o-supervisionado √© o K-Means
  - O modelo s√£o os centroides e a dist√¢ncia euclidiana
  - Novas inst√¢ncias s√£o alocadas nos clusters de cujos centroides elas sejam mais pr√≥ximas de acordo com a dist√¢ncia euclidiana

#### Aprendizado Descritivo

- Aprendizado descritivo tem como objetivo central obter uma descri√ß√£o para os dados
  - Isto √©, o objetivo √© encontrar um **modelo descritivo** para os dados
- Dessa forma, podemos apontar a primeira diferen√ßa para aprendizado preditivo:
  - N√£o temos mais a necessidade de dividir o conjunto de inst√¢ncias em treinamento e valida√ß√£o
- A divis√£o entre treinamento e valida√ß√£o n√£o faz mais sentido, pois queremos obter um modelo para os dados que temos em m√£os
- Consequentemente, a avalia√ß√£o dos resultados (modelos) se torna mais dif√≠cil, j√° que n√£o temos mais uma 'verdade absoluta' para compararmos as sa√≠das

- JV
  - > A premissa √© "eu n√£o sei nada sobre os dados", "ent√£o preciso encontrar um modelo que descreva os dados"
  - "Estat√≠stica n√£o √© boa para descrever grafos"
  - O Descritivo √© t√£o importante quanto prever coisas, mas atuam em momentos diferentes.

---

- Por outro lado, segundo Flach (2012), _o aprendizado descritivo leva √† descoberta genu√≠na de novos conhecimentos, e, dessa forma, est√° situado entre as √°reas de minera√ß√£o de dados e aprendizado de m√°quina_
- O objetivo de se buscar um modelo descritivo dos dados se justifica nas situa√ß√µes em que se quer responder perguntas do tipo ‚Äúo qu√™ aconteceu?‚Äù
- Ou seja, esses modelos descrevem situa√ß√µes passadas e, assim, auxiliam no processo de tomada de decis√£o Aprendizado

- JV

  - O aprendizado descritivo leva a descoberta e novos conhecimentos. Estando entre minera√ß√£o de dados e aprendizado de m√°quina.
  - Busca responder "o qu√™ aconteceu?"
  - Descrevem situa√ß√µes passadas e com isso auxiliam no processo de **tomada de decis√£o**.

  - 4 paradigmas cient√≠ficos
    - Indu√ß√£o, te√≥rico, ...
  - Antes viam o fen√¥menos, criavam teorias, tentavam provar que as teorias se aplicavam.
  - Atualmente usamos um modelo baseado em dados
    - Veem como os dados se comportas, criam teorias e tentam provar que os dados se comportam de acordo com a teoria.

---

- Considere a seguinte situa√ß√£o em que um grande _Market place_ deseja reduzir os custos de distribui√ß√£o dos produtos que ele vende
- Uma pr√°tica muito utilizada atualmente √© manter centros de distribui√ß√£o regionais para estocar produtos vendidos frequentemente, reduzindo o custo e tempo de transporte, e, consequentemente, aumentando a satisfa√ß√£o dos clientes
- Apesar da estocagem de produtos populares nas regionais ser uma decis√£o trivial, ela pode ser aprimorada analisando-se o hist√≥rico de vendas
- Nesse hist√≥rico, podemos encontrar itens menos populares que, com certa frequ√™ncia, s√£o adquiridos junto com os mais populares

  - Isso nos permite decidir estocar tamb√©m esses produtos menos populares, em menor quantidade, mas evitando, assim, um custo maior de se enviar tais produtos individualmente de centros mais distantes

- JV
  - Uma das coisas feitas nessa disciplina √© a busca por regularidades de acontecimentos em conjuntos.
  - H√° uma interse√ß√£o bem grande entre aprendizado descritivo e Minera√ß√£o de Dados.

---

- Considere um segundo caso real em que executivos do Wal-Mart utilizaram de AM para aumentar as vendas diante da amea√ßa do fura√ß√£o Frances em 2004
- Enquanto o furac√£o atravessava o Caribe, os executivos queriam prever os produtos que seus clientes consumiam diante de cat√°strofes

- JV
  - > What Wal-Mart knows about Customers' Habits
  - Eles avaliaram de que forma os usu√°rios se comportavam em rela√ß√£o a desastres naturais e o que compravam nas cidades que tavam para ser afetadas.
    - A decis√£o trivial era considerar que faria sentido estocar pilha, √°gua mineral, lanterna e produtos n√£o espec√≠ficos.
    - Eles descobriram que houve um aumento de 7x nas vendas de Pop Tarts sabor morando, e o campe√£o dos aumentos foi a cerveja.
    - Nessa situa√ß√£o eles mais queriam descrever o passado do que predizer o futuro.
  - Embora seja abordado como preditivo, na pr√°tica seria um exemplo de aprendizado descritivo supervisionado.
  - Exceptional Model Mining
    - Busca-se encontrar quais conjuntos de itens s√£o n√£o-usualmente comprados qunado algum evento ocorre.

D√∫vida: Como fazer para discernimos se um aumento, como no caso do Pop Tarts foi de fato devido aos furac√µes ou se calhou de, nesses dois mesmos intervalos de tempo, foram ve√≠culados an√∫ncios desse produto; sendo ent√£o apenas uma coincid√™ncia?

Resposta: D√° para tentar refinar a forma de an√°lise e o c√°lculo da fun√ß√£o objetivo. Por√©m, devido ao car√°ter qualitativo, √© dif√≠cil de se ter certeza de que essa atipicidade nessa busca por padr√µes at√≠picos sejam separados.

---

- O objetivo dos executivos do Wal-Mart era abastecer as lojas da Fl√≥rida, que estava no caminho do furac√£o, e assim aumentar as vendas
- Novamente, a decis√£o trivial seria aumentar o estoque de pilhas, √°gua mineral, lanternas, e produtos n√£o perec√≠veis
- A an√°lise do hist√≥rico de vendas, nesse caso, poderia retornar que as lojas venderam todo o estoque de DVDs de um g√™nero espec√≠fico de filmes
- No entanto, ao analisar os dados, eles chegaram √† conclus√£o de que havia um aumento de 7x nas vendas de Pop Tarts sabor morango, e o campe√£o do aumento de vendas era cerveja

---

- Apesar do caso ter sido abordado como um exemplo de aprendizado preditivo na mat√©ria, ele √© um exemplo claro de aprendizado descritivo supervisionado
- Especificamente, ele √© um caso claro da aplica√ß√£o de excepcional model mining
  - Queremos encontrar padr√µes de vendas n√£o usuais, isto √©, detectar aumento da venda de produtos que esteja correlacionado positivamente a um evento de interesse
- O √∫nico por√©m √© que essa tarefa foi inicialmente proposta somente em 2008, 4 anos ap√≥s o evento!

#### Aprendizado Descritivo X Preditivo

- Os exemplos ilustram bem a aplicabilidade do aprendizado descritivo:
  - Como dito, eles revelam o que ocorreu no passado e auxiliam na tomada de decis√£o de eventos futuros
- De forma an√°loga, os modelos preditivos utilizam dados hist√≥ricos para prever o comportamento de dados futuros
- Note que a diferen√ßa, portanto, √© bem t√™nue entre as duas abordagens
  - A diferen√ßa mais aparente √© a interven√ß√£o humana no primeiro, contra um certo automatismo da segunda

---

- Um exemplo dessa diferen√ßa t√™nue entre as duas abordagens √© justamente a tarefa de agrupamento
- Incialmente apresentamos clustering como uma tarefa de aprendizado preditivo n√£o-supervisionado
- A mesma tarefa, por√©m, pode ser apresentada como descritiva
- O objetivo no agrupamento descritivo √© obter uma fun√ß√£o $q: \mathcal{D} \to \mathcal{C}$, que mapeia as inst√¢ncias coletadas no conjunto de dados $\mathcal{D}$ a grupos espec√≠ficos (clusters) $\mathcal{C}$
  - A diferen√ßa aqui √© que assumimos como dom√≠nio apenas o conjunto de inst√¢ncias em m√£os, e n√£o toda popula√ß√£o de inst√¢ncias poss√≠veis
  - Ou seja, o resultado do nosso agrupamento √© 'apenas' uma divis√£o das inst√¢ncias em grupos, permitindo a an√°lise de similaridade entre elas; n√£o estamos interessados em alocar novas inst√¢ncias aos grupos encontrados

---

[Imagem: Agrupamento preditivo//Agrupamento Descritivo]

No modelo Preditivo, tenta-se definir limites para que pr√≥ximos itens sejam classificados de acordo com o que foi visto anteriormente.

J√° no Descritivo, tenta-se encontrar padr√µes que descrevam o que foi visto anteriormente.

---

- Considere agora um exemplo mais relacionado ao segundo estudo de caso discutido anteriormente
- Suponha que nosso conjunto de entrada rotulado seja o seguinte
- Podemos tra√ßar dois objetivos aqui:
  - Separar c√≠rculos de quadrados, para classificar novos pontos como um ou outro
  - Buscar padr√µes nos dados para entender as diferen√ßas entre c√≠rculos e quadrados

[Imagem: Distribui√ß√£o de quadrados azuis e c√≠rculos vermelhos]

No exemplo apontado pode-se separar as distribui√ß√µes dos pontos em 4 quadrantes, isso baseado na estimativa do que j√° ocorreu antes, busca ent√£o estimar onde estar√£o posicionados os quadradinhos azuis e as bolinhas vermelhas.

√â importante tamb√©m identificar quais s√£o as regularidades existentes em certos padr√µes irregulares.

---

- O primeiro objetivo induz uma abordagem preditiva
- Logo, o abordamos como um problema de classifica√ß√£o

√Äs vezes usa-se o mesmo modelo entre preditivo e descritivo, por√©m, um pra descrever e o outro pra predizer.

[Imagem: Distribui√ß√£o dos pontos]

---

- O segundo objetivo induz a uma abordagem descritiva
- Logo, abordamos o problema como uma tarefa de descoberta de subgrupos
- [JV]
  - $\sigma_1 \equiv x_1 \in [0.3, 0.7) \wedge x_2 \geq 0.9$
  - $\sigma_2 \equiv x_1 \in [0.275, 0.7) \wedge x_2 \leq 0.1$

[Imagem: Definindo grupo independente]

---

- Esse √∫ltimo exemplo mostra uma caracter√≠stica que frequentemente diferencia as duas abordagens
- As abordagens preditivas buscam ajustar modelos que aprendam as regularidades globais dos dados
  - No exemplo, encontrar as fronteiras que separam c√≠rculos de quadrados
- As abordagens descritivas, por outro lado, ajustam modelos que aprendam regularidades locais dos dados, i.e., padr√µes v√°lidos apenas a subgrupos espec√≠ficos
  - No exemplo, intervalos das vari√°veis que descrevem subgrupos com uma distribui√ß√£o n√£o usual de c√≠rculos e quadrados Leitura

#### Leitura - Aula 01

- Cap√≠tulo 3, Flach (2012)

#### Coment√°rios sobre o curso

Nesse curso busca-se a parte teoria dos algoritmos, n√£o necessariamente em sua aplica√ß√£o.

Ela √© te√≥rica, densa em algoritmo, e a aplica√ß√£o em c√≥digo √© m√≠nimo.

Busca-se "botar uma lupa" sobre a descoberta de padr√µes.

A primeira parte ser√° toda n√£o-supervisionada.

A busca de padr√µes em grafos pode ser usada na √°rea de f√°rmacos para encontrar quais sub-estruturas s√£o as mais frequentes em determinados rem√©dios para determinada infermidade?

A segunda parte ser√° de aprendizado supervisionado.

Por volta de 20 de maio tem uma prova.

Haver√° um tipo de "roleplaying" das atividades. Ele separou as salas em 8 grupos. Um grupo era o "historiador" (buscava entender qual era o contexto), o outro era o "metodologista" (tentatva entender como o algoritmo funcionava), "Aplica√ß√µes", "Coletar as apresenta√ß√µes e redigir", "Publicar o resumo em um site da turma". O "hacker" √© quem busca os c√≥digos existentes, tenta entender, fazer funcionar e documentar como fez funcionar.

"Daqui para baixo √© a parte mais recente, talvez mais p√≥s gradua√ß√£o, ou coisas que n√£o est√£o nos livros".

Cada grupo vai rotacionar em cada uma das tarefas. Ser√£o 9 artigos no total que leremos.

Os Semin√°rios (aplica√ß√µes), veremos de fato aplica√ß√µes

O que ele quer com o projeto? Uma intera√ß√£o maior com o professor. A parte mais pr√°tica da disciplina.

Na parte de ... ser√° o ... que foi quem inventou.

Na parte de supervisionado: Sebastian Ventura e Jos√© Maria Luna 2018; Guozhu Dong and James Bailey 2012;

## Aula 02 | 20/03/2025 | Aprendizado descritivo x preditivo

### Slide - aula02-FIM

#### Introdu√ß√£o - Aula 2

- Hoje iremos focar num dos modelos mais conhecidos de aprendizado descritivo: minera√ß√£o de regras de associa√ß√£o
- O problema foi inicialmente proposto pelos executivos do Wal-Mart para descoberta de padr√µes de consumo nos supermercados
- Por essa raz√£o, muitas vezes a √°rea √© tamb√©m conhecida, sobretudo em ingl√™s, como _Market basket analysis_
- Contudo, diversas aplica√ß√µes podem tirar proveito desse tipo de modelo
- Antes de entrarmos nos detalhes t√©cnicos, vamos analisar um estudo de caso

---

- Kosinski et al. (2013), um grupo de pesquisadores da Universidade de Cambridge, -oletaram dados sobre a personalidade e gostos de usu√°rios do Facebook atrav√©s do aplicativo MyPersonality
- O objetivo do trabalho foi demonstrar que 'curtidas' do Facebook poderiam ser usadas -ara predizer com acur√°cia informa√ß√µes sens√≠veis dos usu√°rios
- O app posteriormente foi relacionado ao esc√¢ndalo do Cambridge Analytica; e os dados em si s√£o carregados de controv√©rsia
- Embora seja um exemplo negativo, ele ilustra bem a utilidade da tarefa que estudaremos hoje

- JV
  - Kosinski (2013) coletavam dados das personalidades atrav√©s do MyPersonality.
    - Esc√¢ndalo do Cambridge Analytica
  - Buscava predizer a personalidade baseado nas curtidas feitas no Facebook

---

- Em resumo, os desenvolvedores do app coletaram uma s√©rie de dados de volunt√°rios, mas, em particular suas 'curtidas' no site
- Provost e Foster (2013) utilizaram esses dados para demonstrar como a modelagem descritiva traz informa√ß√µes √∫teis
- Seguem alguns exemplos de regras:

- JV
  - Provost e Foster (2013) utilizaram os dados para demonstrar a modelagem descritiva e as informa√ß√µes √∫teis.
  - Alguns exemplos de regras:
    - Selena Gomez -> Demi Lovato
    - Linking Park & Disturbed & System of a Down & Korn -> Slipknot
    - SpongeBob SquarePants & Converse [JV: empresa do All Stars] -> Patrick Star
    - Skittles & Mountain Dew -> Gatorade

[JV: Offtopic: o diretor da Google daqui fez doutorado no PPGCC]

---

- Note que a ideia de itens em uma cesta de compras da aplica√ß√£o original pode ser generalizada para itens virtuais
- O objetivo aqui √© encontrar co-ocorr√™ncias de itens de an√°lise que sejam interessantes
- O exemplo traz novamente padr√µes de consumo
  - Majoritariamente de consumo de m√∫sicas, mas, como intencionado pelo estudo original, revela tra√ßos de personalidade dos usu√°rios
- Respeitados os limites √©ticos e legais, essas informa√ß√µes s√£o √∫teis em diversos contextos: campanhas de marketing, desenvolvimento de produtos, ...

- JV
  - A ideia de itens em cesta de compra pode ser generalizada para itens virtuais
  - Busca-se encontrar co-ocorr√™ncias de itens de an√°lise
  - Dados os limites √©ticos, pode-se usar essa an√°lise para se atingir diversos objetivos.

#### Itemset e Tidsets

- Chamamos os elementos do conjunto $I = \{x_1, x_2, \dots, x_m\}$ de itens
- Esses elementos s√£o as vari√°veis de an√°lise que estamos considerando
- Um conjunto $X \subseteq I$ √© chamado de _itemset_
- Um itemset de tamanho k √© chamado de k-itemset
- Denotamos o conjunto de todos os k-itemsets por $I^{(k)}$
- Similarmente, como estamos lidando com 'transa√ß√µes', vamos identific√°-las individualmente por IDs, que ser√£o chamados de tids
- Logo, o conjunto $T = \{t_1, t_2, \dots, t_n\}$ √© o conjunto de transa√ß√µes consideradas, identificadas pelos seus respectivos tids

- JV
  - Os "produtos" da cesta de compras s√£o chamados de "Itens".
    - $I = \{x_1, x_2, \dots, x_m\}$
  - Esses elementos ser√£o as **vari√°veis de an√°lise**
  - Um conjunto $X \subseteq I$ √© chamado de **Itemset**
  - Um itemset de tamanho $k$ √© chamado de **k-itemset**
  - Denotamos o conjunto de todos os **k-itemsets** por $I^(k)$
  - Todas as "Transa√ß√µes" ser√£o identificadas por IDs, ou ent√£o, **TID** (Transaction ID)
  - O conjunto $T = \{t_1, t_2, \dots, t_n\}$ √© o conjunto das transa√ß√µes.

---

- O conjunto $Y \subseteq T$ √© chamado de _tidset_
- √â conveniente assumir que tanto os itemsets quanto os tidsets s√£o sempre armazenados ordenados pela ordem lexicogr√°fica dos itens e transa√ß√µes (seja ela qual for)
- Cada transa√ß√£o consiste de um identificador (tid) e um conjunto de itens
  - Ou seja, cada transa√ß√£o √© um par $(t, X)$ em que $t \in T$ e $X \subseteq I$
- Formalmente, um conjunto de dados ser√° uma tripla $(T, I, D)$

  - T e I s√£o os conjuntos de tids e itens
  - $D \subseteq T \times I$ √© uma rela√ß√£o bin√°ria em que $(t, i) \in D \bicond i \in X$ na transa√ß√£o $(t, X)$
  - Dizemos que a transa√ß√£o t **cont√©m** o item i

- JV
  - O Conjunto $Y \subseteq T$ √© chamado de **Tidset**
  - √â v√°lido assumir que _itemsets_ e _tidsets_ s√£o ordenados por ordem lexicogr√°fica dos itens e transa√ß√µes. (N√£o importa qual ordem, mas est√£o de algum modo ordenados)
  - Cada transa√ß√£o consiste de um identificador (TID) e um conjunto de itens
    - Ent√£o, cada transa√ß√£o √© um par $(t, X)$ em que $t \in T$ e $X \subseteq I$
  - Formalmente, um conjunto de dados ser√° uma tripla $(T, I, D)$
    - T: Transa√ß√µes ou objetos
    - I: Atributos ou itens
    - D: Rela√ß√£o bin√°ria entre eles.
    - $T$ e $I$ s√£o os conjuntos de tids e itens
    - $D \subsetq T \times I$ √© a rela√ß√£o bin√°ria em que $(t, i) \in D$...

---

- Podemos estender a defini√ß√£o tamb√©m para conjuntos de itens
- Dizemos que $t$ cont√©m um itemset X sse $\forall i \in X (t, i) \in D$

- Exemplo:
  - I = {muesli, oats, milk, yoghurt, biscuits, tea}
  - T = {1,2,3,4,5,6}
  - (1, {muesli, milk, yoghurt, tea})
  - 5 cont√©m {milk, tea}

Ou seja: $t$ cont√©m $X$ se $|X - t| = 0$

| TID | Muesli | Oats | Milk | Yoghurt | Biscuits | Tea |
| --: | -----: | ---: | ---: | ------: | -------: | --: |
|   1 |      1 |    0 |    1 |       1 |        0 |   1 |
|   2 |      0 |    1 |    1 |       0 |        0 |   0 |
|   3 |      0 |    0 |    1 |       0 |        1 |   1 |
|   4 |      1 |    0 |    0 |       1 |        0 |   0 |
|   5 |      0 |    1 |    1 |       0 |        0 |   1 |
|   6 |      1 |    0 |    1 |       0 |        0 |   1 |

---

- Dado um itemset $X$, podemos querer saber o conjunto de transa√ß√µes que o cont√©m.
- Esse conjunto √© chamado de **extens√£o** ou **cobertura** de $X$
- Ele √© definido pela seguinte fun√ß√£o:
  - $c: P(I) \to P(T)$
  - $c(X) = \{t \in T | \forall i \in X(t, i) \in D\}$
- Dado um tidset Y, podemos querer saber o maior conjunto de itens comuns √†s transa√ß√µes de Y.
- Esse conjunto √© chamado de **intens√£o** (N√£o √© inten√ß√£o!) de Y.
- Ele √© definido por
  - $i: P(T) \to P(I)$
  - $i(Y) = \{x \in I | \forall t \in Y(t, x) \in D\}$

O uso de extens√£o e intens√£o v√™m da ideia filos√≥fica e semi√≥tica de que a extens√£o √© o conjunto de coisas que se encaixam em uma defini√ß√£o, enquanto a intens√£o √© a defini√ß√£o em si.

---

- Exemplos:
  - $i(\{1, 5, 6\}) = \{milk, tea\}$
  - $c(\{milk, tea\}) = \{1, 3, 5, 6\}$
  - $c(\{muesly, oats\}) = ?$
  - $i({4, 5}) = ?$

| TID | Muesli | Oats | Milk | Yoghurt | Biscuits | Tea |
| --: | -----: | ---: | ---: | ------: | -------: | --: |
|   1 |      1 |    0 |    1 |       1 |        0 |   1 |
|   2 |      0 |    1 |    1 |       0 |        0 |   0 |
|   3 |      0 |    0 |    1 |       0 |        1 |   1 |
|   4 |      1 |    0 |    0 |       1 |        0 |   0 |
|   5 |      0 |    1 |    1 |       0 |        0 |   1 |
|   6 |      1 |    0 |    1 |       0 |        0 |   1 |

Intens√£o: conjunto de itens comuns a todos os elementos de um determinado conjunto de transa√ß√µes.

Extens√£o: o conjunto de transa√ß√µes que contenham um determinado conjunto de itens.

Intens√£o: a interse√ß√£o das linhas
Extens√£o: a interse√ß√£o das colunas

#### Representa√ß√µes de conjuntos de dados

- As fun√ß√µes de intens√£o e cobertura permitem representar de diferentes formas a defini√ß√£o de conjunto de dados apresentada anteriormente
- Por exemplo, podemos enxergar o conjunto de dados como um conjunto de transa√ß√µes e suas respectivas intens√µes
  - Ou seja, ele √© um conjunto de $(t, i(t))$
  - Essa representa√ß√£o √© chamada de **horizontal**
- Similarmente, podemos enxergar o conjunto de dados como um conjunto de itens e suas coberturas
  - Ou seja, como um conjunto de $(x, c(x))$
  - Essa representa√ß√£o √© chamada de **vertical**

---

- Horizontal

| **t** |                   **i(t)** |
| ----: | -------------------------: |
|     1 | Muesli, Milk, Yoghurt, Tea |
|     2 |                 Oats, Milk |
|     3 |        Milk, Biscuits, Tea |
|     4 |            Muesli, Yoghurt |
|     5 |            Oats, Milk, Tea |
|     6 |          Muesli, Milk, Tea |

|    **t** | 1                          | 2          | 3                   | 4               | 5               | 6                 |
| -------: | -------------------------- | ---------- | ------------------- | --------------- | --------------- | ----------------- |
| **i(t)** | Muesli, Milk, Yoghurt, Tea | Oats, Milk | Milk, Biscuits, Tea | Muesli, Yoghurt | Oats, Milk, Tea | Muesli, Milk, Tea |

- Vertical

|    **x** |  Muesli | Oats |          Milk | Yoghurt | Biscuits |        Tea |
| -------: | ------: | ---: | ------------: | ------: | -------: | ---------: |
| **t(x)** | 1, 4, 6 | 2, 5 | 1, 2, 3, 5, 6 |    1, 4 |        3 | 1, 3, 5, 6 |

|    **x** |      **t(x)** |
| -------: | ------------: |
|   Muesli |       1, 4, 6 |
|     Oats |          2, 5 |
|     Milk | 1, 2, 3, 5, 6 |
|  Yoghurt |          1, 4 |
| Biscuits |             3 |
|      Tea |    1, 3, 5, 6 |

#### Conjuntos de itens frequentes e Regras de Associa√ß√£o

- A identifica√ß√£o de regras tais como as que vimos no exemplo no in√≠cio da aula, em geral, envolvem duas etapas
  - Minera√ß√£o de conjuntos de itens frequentes
  - Descoberta de regras de associa√ß√£o interessantes
- A primeira parte √© computacionalmente mais intensa e, por esta raz√£o, √© a que recebeu mais aten√ß√£o dos pesquisadores
- Por isso, vamos inicialmente nos concentrar nessa tarefa

#### Minera√ß√£o de Conjuntos de itens frequentes

- Uma defini√ß√£o de "regra interessante" √© ela ocorrer com certa frequ√™ncia.
- Ent√£o √© necess√°rio definir um limiar entre o que √© frequente e o que √© infrequente
  - Esse limiar √© chamado de suporte m√≠nimo (minsup)
- O suporte de um itemset √© o tamanho de sua cobertura
  - $sup(X) = |c(X)|$
- Como essa defini√ß√£o √© dependente do contexto, admite-se tamb√©m a defini√ß√£o do suporte relativo
  - $rsup(X) = |c(X)| / |T|$

---

- Dessa forma, dizemos que um itemset √© frequente sse $sup(X) \geq minsup$
- Exemplos, considerando $minsup=2$:
  - $\{milk\}; sup(\{milk\}) = 5$
  - $\{milk, tea\}; sup(\{milk, tea\}) = 4$
  - $\{muesli, oats, milk\}$?
  - $\{muesli, milk\}$?

| TID | Muesli | Oats | Milk | Yoghurt | Biscuits | Tea |
| --: | -----: | ---: | ---: | ------: | -------: | --: |
|   1 |      1 |    0 |    1 |       1 |        0 |   1 |
|   2 |      0 |    1 |    1 |       0 |        0 |   0 |
|   3 |      0 |    0 |    1 |       0 |        1 |   1 |
|   4 |      1 |    0 |    0 |       1 |        0 |   0 |
|   5 |      0 |    1 |    1 |       0 |        0 |   1 |
|   6 |      1 |    0 |    1 |       0 |        0 |   1 |

---

- O espa√ßo de busca do problema √© o conjunto pot√™ncia do conjunto de itens
- Se considerarmos a rela√ß√£o de subconjuntos como uma rela√ß√£o de ordem parcial, temos que o espa√ßo de busca √© estruturado como um reticulado
  - Esse reticulado pode ser visualizado como um grafo, onde somente as rela√ß√µes diretas s√£o representadas
  - Ou seja, se $A \subseteq B \wedge |A| = |B| - 1$, ent√£o existe uma aresta entre A e B no diagrama
- Assim, a minera√ß√£o de conjunto de itens frequentes √© resolvida por uma 'simples' busca no reticulado associado
- Essa busca pode ser tanto uma busca em largura quanto em profundidade
  - De fato, existem abordagens baseadas em ambas as buscas
- No entanto, a maioria das abordagens compartilham a mesma estrutura de busca:
  - Identificam candidatos navegando o espa√ßo de busca
  - Computam o suporte desses candidatos, descartando os infrequentes

Rela√ß√£o de ordem parcial: $X \subseteq Y \leftrightarrow X \leq Y$

Conjunto pot√™ncia: √© o conjunto de todos os poss√≠veis subconjuntos de um conjunto.

```mermaid
flowchart TD
  null[["\null"]]
  null --> A & B & C
  A --> AB & AC
  B --> AB & BC
  C --> AC & BC
```

- JV
  - O espa√ßo de busca do problema √© o conjunto pot√™ncia do conjunto de itens
  - Se considerarmos a rela√ß√£o de subconjuntos como uma rela√ß√£o de ordem parcial, temos que o espa√ßo de busca √© estruturado como um reticulado
    - Esse reticulado pode ser visualizado como um grafo, onde somente as rela√ß√µes diretas s√£o representadas
    - Ou seja, se $A \subseteq B \land |A| = |B| - 1$, ent√£o existe uma aresta entre A e B no diagrama

Aquele diagrama explica bastante o que que isso quis dizer. Basicamente, no conjunto pot√™ncia, cada n√≠vel vai ter um elemento a mais que o n√≠vel anterior.

#### Algoritmo Ing√™nuo

- Vamos considerar uma abordagem ing√™nua para a minera√ß√£o de itens frequentes, antes de explorarmos outros mecanismos
- Independentemente da escolha da forma de busca, devemos enumerar os poss√≠veis candidatos, e, em seguida, computar seu suporte
- Especificamente, devemos enumerar cada itemset poss√≠vel; e depois verificar no conjunto de dados quais transa√ß√µes cont√™m esse itemset

---

Complexidade do algoritmo: $O(2^I \cdot T \cdot I)$

- **// ALGORITHM 8.1. Algoritm BruteForce**

  - **BruteForce** $(D, \mathcal{I}, minsup)$:

    - $\mathcal{F} \leftarrow \emptyset$ // set of frequent itemsets
      - **foreach** $X \subseteq \mathcal{I}$ **do**
        - $sup(X) \leftarrow ComputeSupport (X, D)$
        - **if** $sup(X) \leq minsup$ **then**
          - $\mathcal{F} \leftarrow \mathcal{F} \cup {(X, sup(X))}$
      - **return** $\mathcal{F}$

  - **ComputeSupport** $(X, D)$:
    - $sup(X) \leftarrow 0$
    - **foreach** $\langle t, i(t) \rangle \in D$ **do**
      - **if** $X \subseteq i(t)$ **then**
        - $sup(X) \leftarrow sup(X) + 1$
  - **return** $sup(X)$

---

- A computa√ß√£o do suporte de um itemset requer uma passada sobre o conjunto de dados, ou seja, requer tempo $O(|T|)$
- Verificar se uma dada transa√ß√£o cont√©m um itemset requer tempo $O(|I|)$
- Portanto, o custo total de computa√ß√£o do suporte √© $O(|T|)$
- O espa√ßo de busca, por sua vez, √© o conjunto pot√™ncia de $I$. Logo, a complexidade do algoritmo ing√™nuo √© $O(2^I \cdot I \cdot T)$

---

- A complexidade do espa√ßo de busca √© inerente ao problema. Contudo o algoritmo √© ineficiente mesmo em espa√ßos pequenos
- Note que o conjunto de dados n√£o √© mantido em mem√≥ria, portanto a computa√ß√£o do suporte torna o algoritmo impratic√°vel
- Os algoritmos mais "sofisticados" atacam majoritariamente o problema de computa√ß√£o de suporte, evitando computa√ß√µes desnecess√°ris, e/ou adotando estrat√©gias mais eficientes para comput√°-lo.

Ent√£o temos dois problemas principais: reduzir o espa√ßo de busca e reduzir a complexidade para calcular o suporte.

#### Leitura - Aula 02

- Se√ß√µes 8.1 e 8.2 Zaki e Meira
- Se√ß√µes 6.1 e 5.2 (Introduction to Data Mining)

## Aula 03 | 25/03/2025 | Minera√ß√£o de conjuntos de itens - Faltei - Minera√ß√£o de itens frequentes: Apriori e Eclat

### Slide: aula03-apriori_eclat (Aula 03)

#### Introdu√ß√£o (Aula 03)

- Como vimos na aula anterior, o principal problema do algoritmo ing√™nuo para minera√ß√£o de conjuntos de itens frequentes era a replica√ß√£o de esfor√ßos para avaliar o suporte dos candidatos
- As m√∫ltiplas passadas no conjunto de dados (armazenado em mem√≥ria secund√°ria) torna o algoritmo impratic√°vel at√© mesmo para pequenos volumes
- Os algoritmos que veremos hoje exploram propriedades do problema para amortizar o custo da computa√ß√£o de suporte, e evitar retrabalho na avalia√ß√£o dos candidatos

---

---

O que √© mesmo o suporte? ü§î

- Recapitulando da aula anterior, o Suporte aparantemente √© um encurtamento para o "Suporte M√≠nimo" (minsup) que √© o limiar que define se determinado item √© frequente o bastante ou n√£o.
  - Esse valor √© dado pela seguinte f√≥rmula:
    - $sup(X) = |c(X)|$, onde $c(X)$ √© a cobertura do itemset $X$.

Mas o que √© mesmo a cobertura?

- A cobertura √© o conjunto de transa√ß√µes que cont√©m um itemset $X$. Ou seja, √© o conjunto de transa√ß√µes que cont√©m todos os itens do itemset $X$.

#### Apriori

- O Apriori foi proposto por Rakesh Agrawal e Ramakrishnan Srikant em 1994
  - O artigo possui mais de 30K cita√ß√µes
- Os autores √† √©poca trabalhavam no projeto da IBM para o Wal-Mart
- A ideia central √© evitar computa√ß√µes desnecess√°rias para candidatos infrequentes
- Isso √© viabilizado pela propriedade de **anti-monotonicidade** da fun√ß√£o suporte
- Essa √© uma das propriedades mais importantes para a √°rea

##### Anti-monotonicidade do suporte

- Considere dois itemsets $A$ e $B$ quaisquer. Se $A \subseteq B$, ent√£o $sup(A) \geq sup(B)$.
- Essa observa√ß√£o nos diz que a cobertura de conjuntos de itens √©, no m√°ximo, t√£o grande quanto a de seus subconjuntos
  - No caso mais simples, um conjunto de dois itens n√£o pode ocorrer em mais transa√ß√µes que cada um dos itens individualmente
- Consequentemente, se o itemset A √© infrequente, B tamb√©m ser√°.
- Isso define a propriedade de anti-monotonicidade da fun√ß√£o suporte, tamb√©m conhecida como a propriedade do Apriori
  - **Todo superconjunto de um conjunto infrequente √© infrequente**
  - **Todo subconjunto de um conjunto frequente √© frequente**

---

---

Muito interessante isso da√≠ de cima.

Basicamente entendemos que $sup(X=\{A, B\}) \geq sup(Y=\{A, B, C\})$ com isso, se X √© frequente, nada garante que Y tamb√©m seja. Por√©m, se Y √© frequente, isso garante que todos os poss√≠veis subconjuntos de Y tamb√©m ser√£o frequentes.

#### Apriori [2]

- O Apriori utiliza uma busca em largura no espa√ßo de busca para minerar os padr√µes
  - Frequentemente, o termo usado na literatura √© abordagem por n√≠veis (level-wise approach)
- A busca inicia com a identifica√ß√£o dos itens frequentes
- Depois, os conjuntos de tamanho $k$ s√£o explorados antes dos de tamanho $k+1$
- Assim como o algoritmo ing√™nuo, ele tamb√©m opera em duas etapas:
  - Gera√ß√£o de candidatos
  - C√¥mputo do suporte e elimina√ß√£o dos infrequentes

---

- A gera√ß√£o dos candidatos √© feita a partir dos conjuntos frequentes encontrados na fase anterior
- Conjuntos compartilhando um prefixo de $k-1$ itens s√£o combinados para gerar candidatos de tamanho $k+1$
  - Novamente, assume-se que eles s√£o ordenados pela ordem lexicogr√°fica
- Candidatos que possuam algum subconjunto infrequente s√£o descartados imediatamente
  - A propriedade do Apriori √© empregada
- Os suportes dos candidatos s√£o atualizados com uma √∫nica passada no conjunto de dados
  - Subconjuntos de tamanho $k$ de cada transa√ß√£o s√£o usados para atualizar o suporte dos candidatos

---

- **APRIORI** $(D, \mathcal{I}, minsup)$:
  - $\mathcal{F} \leftarrow \emptyset$
  - $\mathcal{C}^{(1)} \leftarrow \{\emptyset\}$ `// Initial prefix tree with single items`
  - **foreach** $i \in \mathcal{I}$ **do** Add $i$ as child of $\emptyset$ in $\mathcal{C}^{(1)}$ with $sup(i) \leftarrow 0$
  - $k \leftarrow 1$ `// k denotes the level`
  - **while** $\mathcal{C}^{(k)} \neq \emptyset$ **do**
    - ComputeSupport $(\mathcal{C}^{(k)}, D)$
    - **foreach** _leaf_ $X \in \mathcal{C}^{(k)}$ **do**
      - **if** $sup(X) \geq minsup$ **then** $\mathcal{F} \leftarrow \mathcal{F} \cup \{(X, sup(X))\}$
      - **else** remove $X$ from $\mathcal{C}^{(k)}$
    - $\mathcal{C}^{(k+1)} \leftarrow$ ExtendPrefixTree($\mathcal{C}^{(k)}$)
    - $k \leftarrow k+1$
  - **return** $\mathcal{F}^{(k)}$

---

- ComputeSupport $(\mathcal{C}^{(k)}, D)$:

  - **foreach** $\langle t, i(t) \rangle \in D$ **do**
    - **foreach** k-subset $X \subseteq i(t)$ **do**
      - **if** $X \in \mathcal{C}^{(k)}$ **then** $sup(X) \leftarrow sup(X) + 1$

- ExtendPrefixTree $(\mathcal{C}^{(k)})$:
  - **foreach** leaf $X_a \in \mathcal{C}^{(k)}$ **do**
    - **foreach** leaf $X_b \in SIBLING(X_a)$, such that $b > a$ **do**
      - $X_{ab} \leftarrow X_a \cup X_b$ `// prune candidate if there are any infrequent subsets`
      - **if** $X_j \in \mathcal{C}^{(k)}$, **for all** $X_j \subset X_{ab}$, such that $|X_j| = |X_{ab}|-1$ **then**
        - Add $X_{ab}$ as child of $X_a$ with $sup(X_{ab}) \leftarrow 0$
    - **if** _no extensions from_ $X_a$ **then**
      - Remove $X_a$, and all ancestors of $X_a with no extensions, from $\mathcal{C}^{(k)}$
  - **return** $\mathcal{C}^{(k)}$

---

- Exemplo (minsup=3):

$$
\begin{bmatrix}
  TID & Muesli & Oats & Milk & Yoghurt & Biscuits & Tea \\
  1   &      1 &    0 &    1 &       1 &        0 &   1 \\
  2   &      0 &    1 &    1 &       0 &        0 &   0 \\
  3   &      0 &    0 &    1 &       0 &        1 &   1 \\
  4   &      1 &    0 &    0 &       1 &        0 &   0 \\
  5   &      0 &    1 &    1 &       0 &        0 &   1 \\
  6   &      1 &    0 &    1 &       0 &        0 &   1 \\
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
  TID & Muesli & Milk & Tea \\
    1 &      1 &    1 &   1 \\
    2 &      0 &    1 &   0 \\
    3 &      0 &    1 &   1 \\
    4 &      1 &    0 &   0 \\
    5 &      0 &    1 &   1 \\
    6 &      1 &    1 &   1 \\
\end{bmatrix}
$$

---

- O n√∫mero de passadas √© drasticamente reduzido em rela√ß√£o ao algoritmo ing√™nuo
  - $O(2^I) \rightarrow O(I)$
- As podas baseadas na anti-monotonicidade do suporte tamb√©m s√£o bastante efetivas na pr√°tica
- O algoritmo tamb√©m apresenta alguns problemas:
  - Busca em largura requer que todos os candidatos de um n√≠vel sejam mantidos em mem√≥ria. Esse custo √© proibitivo em alguns (muitos) casos
  - Tanto a contagem do suporte quanto a poda do Apriori podem ser consideravelmente caras, dependendo da implementa√ß√£o

---

- O custo de mem√≥ria √© inerente √† abordagem, e n√£o podemos fazer muita coisa para melhor√°-lo
- O custo da contagem e verifica√ß√£o pode ser atenuado, usando estruturas de dados mais 'sofisticadas'
- Existem duas abordagens mais comuns:
  - Usar uma √°rvore hash
  - Usar uma √°rvore de prefixos (Trie)
- Na primeira abordagem, cada n√≥ folha armazena um conjunto de candidatos/conjuntos frequentes
  - O n√∫mero de compara√ß√µes √© reduzido
- Na segunda abordagem, os n√≥s da Trie armazenam os candidatos/conjuntos frequentes. Os subconjuntos das transa√ß√µes s√£o usadas para index√°-la e atualizar o suporte

---

- A redu√ß√£o do suporte m√≠nimo tem um impacto muito grande no custo computacional do algoritmo
  - O tamanho dos candidatos aumenta -> Mais candidatos s√£o avaliados em cada n√≠vel -> o tamanho dos conjuntos frequentes aumenta -> mais n√≠veis s√£o explorados

[Imagem(a): Number of candidate itemsets]

[Imagem(b): Number of frequent itemsets]

---

- A densidade da base de dados tamb√©m tem muito impacto no custo
  - Transa√ß√µes passam a ter mais itens
  - Isso tem duas implica√ß√µes: tamanho m√©dio dos itemsets aumentam; mais subconjuntos s√£o gerados durante a contagem do suporte $\binom{|t|}{k}$

[Imagem(a): Number of candidate itemsets]

[Imagem(b): Number of frequent itemsets]

#### Eclat - Pr√≥xima aula

## Aula 04 | 27/03/2025 | Minera√ß√£o de conjuntos de itens

### Aula passada

- Algoritmo apriori
- Frequente, infrequente.
- Como calcula o suporte
  - A partir dos itens frequentes: tabelas de 0 e 1.
  - Pra isso usava √°rvore de prefixos
  - Para cada transa√ß√£o gerava os itemsets de tamanho k, ia na √°rvore de prefixos
  - E incrementava o suporte daquela chave

[JV: escrevi o que ele t√° falando, mas n√£o t√¥ entendendo]

- Duas coisas influenciam o desempenho do algoritmo
  1. Ele falou algo
  2. Se o BD √© denso, as transa√ß√µes s√£o mais largas.
- $\binom{|t|}{k}$
- Quando √© esparso, funciona bem. Quando √© denso que come√ßa a dar problema.

Eu t√¥ achando que se eu compro $J = \{A, B, C\}$, Ent√£o o conjunto pot√™ncia dele √© $P(J) = \{\emptyset, A, B, C, AB, AC, BC, ABC\}$, e ent√£o, incrementaria 1 para um desses grupos

- C√°lculo de suporte:

  - Para cada um dos itemsets tem que verificar se ele t√° na √°rvore K(?)

- Se os itemsets est√£o em mem√≥ria...
- Se quero gerar o itemset $XY$ partindo de $X \cup Y$, posso dizer que o suporte ser√° $|c(X) \cap c(Y)|$

### Slide: aula03-apriori_eclat (Aula 04)

#### Eclat (Equivalence Class Transformation)

Dada a representa√ß√£o vertical dos dados, consigo calcular o suporte por essa intercess√£o.

- Dadas as defici√™ncias do Apriori, M. Zaki prop√¥s, em 2000, o algoritmo Equivalence Class Transformation (Eclat)
- A proposta do algoritmo √© 'eliminar' a necessidade de passadas no conjunto de dados para computar o suporte
- Para isso, ele parte de uma representa√ß√£o vertical dos dados, e se baseia no fato de que a cobertura da uni√£o de dois itemsets √© a interse√ß√£o de suas coberturas

Problema: como mantenho todos os itemsets gerados em mem√≥ria?

---

- Ou seja, a ideia central do algoritmo tentar manter os tidsets em mem√≥ria principal para computar o suporte dos itemsets atrav√©s de interse√ß√µes desses conjuntos
- Contudo, todos os tidsets podem n√£o caber na mem√≥ria principal. Assim, √© necess√°rio algum mecanismo que possibilite a divis√£o do espa√ßo de busca em subproblemas independentes que caibam na mem√≥ria
- A divis√£o √© feita conforme uma rela√ß√£o de equival√™ncia estabelecida sobre os candidatos

Tenta manter tudo na mem√≥ria principal

A ideia √© partir o problema em subproblemas e trazer esses subproblemas pra mem√≥ria.

Surgiu atrav√©s da cria√ß√£o de uma rela√ß√£o de equival√™ncia entre os itemsets

---

- Seja $p: P(I) \times \mathbb{N} \rightarrow P(I)$ uma fun√ß√£o prefixo. $p(X, k) = X[1:k]$.
- A rela√ß√£o $\theta_k \subseteq P(I) \times P(I), A \theta_k B \equiv p(A, k) = p(B, k)$, √© uma rela√ß√£o de equival√™ncia
- Dessa forma, ela induz uma parti√ß√£o dos conjuntos de itens em classes de equival√™ncia, onde todos os elementos compartilham um certo prefixo
- Por exemplo, todos os conjuntos que cont√™m o item Muesli pertencem √† classe de equival√™ncia $[Muesli]_{\theta_1}$
- Intuitivamente, essas classes servem como proje√ß√µes do conjunto de dados, em que somente as transa√ß√µes contendo aquele prefixo s√£o consideradas

Cria-se uma rela√ß√£o de equival√™ncia pelos prefixos.

Diz-se que dois itemsets s√£o equivalentes se o prefixos dos dois s√£o iguais.

Consideremos que temos o seguinte conjunto pot√™ncia: $P(I) = \{\emptyset, A, B, C, AB, AC, BC, ABC\}$. Na forma de representa√ß√£o, seria como se agrup√°ssemos os dados em grupos de prefixos:

- $A: \{A, AB, AC, ABC\}$
- $B: \{B, BC\}$
- $C: \{C\}$

E ent√£o seriam varridos de C para A.

Poderia-se tamb√©m fazer subgrupos de subgrupos, dependendo do tamanho do conjunto de prefixos.

---

Ele faz uma busca em profundidade (DFS)

Ele faz subparti√ß√µes at√© que o n√∫mero de transa√ß√µes seja pequeno o suficiente para caber na mem√≥ria.

- Durante a busca em profundidade, o algoritmo particiona os conjuntos de itens conforme a rela√ß√£o de equival√™ncia e o n√≠vel da √°rvore
- O particionamento pode ser encerrado t√£o logo os tidsets caibam na mem√≥ria e as interse√ß√µes possam ser computadas facilmente
- Contudo, a estrat√©gia pode ser usada durante toda a execu√ß√£o do algoritmo
- O c√°lculo do suporte no algoritmo se restringe a calcular o tamanho do tidset

---

- **ALGORITHM 8.3. Algorithm ECLAT**
- // Initial Call: $\mathcal{F} \leftarrow \emptyset, P \leftarrow \{ \langle i, t(i) \rangle | i \in \mathcal{I}, |t(i)| \geq minsup \} $
- **ECLAT** $(P, minsup, \mathcal{F})$:
  - **foreach** $\langle X_a, t(X_a) \rangle \in P$ **do**
    - $\mathcal{F} \leftarrow \mathcal{F} \cup \{(X_a, sup(X_a))\}$
    - $P_a \leftarrow \emptyset$
    - **foreach** $\langle X_b, t(X_b) \rangle \in P$, with $X_b > X_a$ **do**
      - $X_{ab} = X_a \cup X_b$
      - $t(X_{ab}) = t(X_a) \cap t(X_b)$
      - **if** $sup(X_{ab}) \geq minsup$ **then**
        - $P_a \leftarrow P_a \cup \{ \langle X_{ab}, t(X_{ab}) \rangle \}$
    - **if** $P_a \neq \emptyset$ **then** ECLAT $(P_a, minsup, \mathcal{F})$

[JV: Droga, foquei em transcrever brevemente e esqueci de prestar aten√ß√£o na explica√ß√£o do professor]

P guarda todos os frequentes da chamada anterior. porque ele filtro toudos que s√£o infrequentes pelo minsup

para cada um dos frequentes dos candidatos, armazena no ocnjunto de itens frequentes globais

e a partir dele gera em profundidade a combina√ß√£o dele com todos os outros que v√™m pra frente.

Evitam redund√¢ncia: 1. parti√ß√µes; 2. Ordem sistem√°tica de combina√ß√£o dos itens.

1. A B C
2. A com B e C: AB AC
3. AB com AC: ABC
4. B com C: BC

##### Representa√ß√µes de conjuntos de dados (Aula 4)

$$
\begin{bmatrix}
  TID & Muesli & Oats & Milk & Yoghurt & Biscuits & Tea \\
  1 & 1 & 0 & 1 & 1 & 0 & 1\\
  2 & 0 & 1 & 1 & 0 & 0 & 0\\
  3 & 0 & 0 & 1 & 0 & 1 & 1\\
  4 & 1 & 0 & 0 & 1 & 0 & 0\\
  5 & 0 & 1 & 1 & 0 & 0 & 1\\
  6 & 1 & 0 & 1 & 0 & 0 & 1\\
\end{bmatrix}
\Rightarrow
[\emptyset]_{\theta_0}
\begin{bmatrix}
  X      & c(x)  \\
  Muesli & 146   \\
  Milk   & 12356 \\
  Tea    & 1356  \\
\end{bmatrix}
$$

Pelo que eu t√¥ entendendo:

1. Come√ßa pegando todos os itens que tenham uma quantidade de transa√ß√µes maior que o minsup (o valor m√≠nimo aceit√°vel para que consideremos relevante)
2. Depois disso, come√ßamos fazendo a intercess√£o das transa√ß√µes entre o primeiro conjunto de itens que passou pela compara√ß√£o com o segundo conjunto.
3. Depois disso, v√™ se o resultado dessas intercess√µes √© grande o bastante.

Se $A \subseteq B$, ent√£o $c(B) \subseteq c(A)$ (Cobertura)

#### Eclat (Equivalence Class Transformation) [2]

- O custo computacional do algoritmo est√° diretamente relacionado ao tamanho dos tidsets
  - O tempo de execu√ß√£o depende do c√°lculo da interse√ß√£o dos tidsets
- O custo de espa√ßo tamb√©m √© dependente do tamanho. Quanto mais denso o conjunto de dados, mais largos ser√£o os tidsets
- H√° duas formas de se implementar o algoritmo:
  - Usando vetores de bits
  - Usando vetores de Ids
- Os vetores de bits s√£o interessantes para o c√°lculo do suporte
  - Eles facilitam a computa√ß√£o da interse√ß√£o e o c√°lculo do tamanho do tidset pode ser feito usando uma tabela auxiliar (palavras de 16bits mapeadas para valores)
- Contudo, se o conjunto for esparso, isso representar√° um desperd√≠cio muito grande de espa√ßo. Ent√£o vetores de Ids se tornam mais interessantes.
  - As computa√ß√µes de interse√ß√£o s√£o feitas como na fun√ß√£o merge do mergesort

##### Diffsets e dEclat [Pr√≥xima aula]

- Percebendo o problema de se manter os tidsets em mem√≥ria, Zaki e Gouda propuseram em 2001 (o artigo s√≥ foi publicado em 2003) uma solu√ß√£o alternativa
- Eles propuseram armazenar as diferen√ßas entre os tidsets dos membros de uma classe e dos prefixos que a definem
- Eles chamaram esse conjunto de **diffset**
- Formalmente, para um prefixo P e um itemset PX, o diffset de X, $d(PX) = c(P) - c(PX)$
- Seriam armazenados, portanto, o suporte do itemset e seu diffset

Em bases de dados densos, varia bem pouco o suporte entre os itens. Ent√£o, faria mais sentido guardar s√≥ a diferen√ßa ao inv√©s de guardar o todo.

Ao inv√©s de chamar de tidset, passaram a chamar de diffset.

...

Pode-se armazenar em vetores de bits ao inv√©s de vetores de inteiros.

Usando o vetor de bits, √© como se fosse:

A = [00110, 01001, 01100, 00011]

E para calcular o suporte (?) cobetura(?)

basta fazer um c√°lculo r√°pido de 0 a 255 para dizer quantos bits est√£o ativos, e ent√£o fazer a contagem de bits ativos somando esses valores.

0 -> 1
1 -> 1
2 -> 1
...
255 -> ...

Outra forma de condensar √©: Se sei que um determinado conjunto √© grande o bastante, posso inferir que todos os que s√£o menores que eles tamb√©m s√£o grandes o bastante.

Se s√≥ √© guardado o valor das diferen√ßas, acaba sendo um problema fazer as intercess√µes.

---

- Por defini√ß√£o, $d(PXY) = c(PX) - c(PXY) = c(PX) - c(PY)$
- Podemos adicionar, ao conjunto acima, o conjunto vazio $(c(P) - c(P))$ sem alter√°-lo
- Logo, $d(PXY) = c(PX) - c(PY) + c(P) - c(P) - c(P) = (c(P)-c(PY)) - (c(P) - c(PX)) = d(PY) - d(PX)$
- Em outras palavras, podemos usar os diffsets dos conjuntos base para calcular o diffset do novo candidato
- A variante do Eclat que usa diffsets ficou conhecida como dEclat

$C(PX) - C(PY)$

$C(PX) - C(PY) \cup C(P) - C(P)$

$C(PX) \cap \overline{C(PY)} \cup C(P) \cap \overline{C(P)}$

$C(PX) \cup \overline{C(P)} \cap C(P) \cup \overline{C(P)}$

## Aula 05 | 01/04/2025 | Minera√ß√£o de conjuntos de itens

### Slide: aula03-apriori_eclat (Aula 05)

#### Diffsets e dEclat [Aula 05]

- Percebendo o problema de se manter os tidsets em mem√≥ria, Zaki e Gouda propuseram em 2001 (o artigo s√≥ foi publicado em 2003) uma solu√ß√£o alternativa
- Eles propuseram armazenar as diferen√ßas entre os tidsets dos membros de uma classe e dos prefixos que a definem
- Eles chamaram esse conjunto de **diffset**
- Formalmente, para um prefixo $P$ e um itemset $PX$, o diffset de $X$, $d(PX) = c(P) - c(PX)$
- Seriam armazenados, portanto, o suporte do itemset e seu diffset

---

- O fato √© que, se somente os diffsets s√£o armazenados, o suporte n√£o √© mais obtido como a cardinalidade desse conjunto
- Dessa forma, como calcular o suporte de um itemset $PXY$ obtido a partir de outros dois $PX$ e $PY$, usando somente seus diffsets?
- O suporte de $PX$ √© calculado pela diferen√ßa entre o suporte de $P$ e o diffset de $PX$, $sup(PX) = sup(PX) - |d(PXY)|$
  - Portanto, $sup(PXY) = sup(PX) - |d(PXY)|$
- A solu√ß√£o passa por computar o diffset de $PXY$. Por√©m, temos somente os diffsets de $PX$ e $PY$

---

- Por defini√ß√£o, $d(PXY) = c(PX) - c(PXY) = c(PX) - c(PY)$
- Podemos adicionar, ao conjunto acima, o conjunto vazio $(c(P) - c(P))$ sem alter√°-lo
- Logo, $d(PXY) = c(PX) - c(PY) + c(P) - c(P) - c(P) = (c(P)-c(PY)) - (c(P) - c(PX)) = d(PY) - d(PX)$
- Em outras palavras, podemos usar os diffsets dos conjuntos base para calcular o diffset do novo candidato
- A variante do Eclat que usa diffsets ficou conhecida como dEclat

---

---

- $d(PXY) = c(PX) - c(PY) = c(PX) - c(PXY)$
- $c(PXY) = c(PX) \cap c(PY)$

"Diferen√ßa √© a mesma coisa que interse√ß√£o com complemento"

- $d(PXY) = ...$

Ele fez um monte de igualdades com opera√ß√µes de conjuntos.

- $d(PXY) =$
- $c(PX) - c(PY) =$
- $c(PX) - c(PXY) =$
- $[c(PX) - c(PXY)] \cup [c(P) - c(P)] =$
- $[c(PX) \cap \overline{c(PXY)}] \cup [c(P) \cap \overline{c(P)}]$
- $[c(PX) \cup c(P)] \cap [\overline{c(PY)} \cup \overline{c(P)}] \cap \overline{d(PX)}$
- $(c(PX) \cap \overline{c(PY)}) \cup (c(P) \cap \overline{c(P)}) \cup (c(P) \cap \overline{c(PY)}) \cup (c(P) \cap \overline{c(P)} \cap \overline{d(PX)})$
- $(Q \subseteq d(PY)) \cup (\emptyset) \cup (d(PY)) \cup (\emptyset) \cap (\overline{d(PX)})$
- $d(PY) \cap \overline{d(PX)} =$
- $d(PY) - d(PX)$

---

- $d(PX) = ...$

---

- $P = \{1, 2, 3, 4, 5\}$
- $X = \{1, 3, 5\}$
- $Y = \{2, 3, 4\}$
- $PX = \{1, 3, 5\}$
- $PY = \{2, 3, 4\}$
- $\overline{PY} = \{1, 5\}$
- $PX \cap \overline{PY} = \{1, 5\}$

---

---

---

- **ALGORITHM 8.4. Algorithm dEclat**
  - //`Initial Call:` $\mathcal{F} \leftarrow \emptyset, P \leftarrow \{ \langle i, d(i), sup(i) \rangle | i \in \mathcal{I}, d(i) = \mathcal{T}\\t(i), sup(i) \geq minsup \}$
  - **dEclat** $(P, minsup, \mathcal{F})$:
    - **foreach** $\langle X_a, d(X_a), sup(X_a) \rangle \in P$ **do**
      - $\mathcal{F} \leftarrow \mathcal{F} \cup \{(X_a, sup(X_a))\}$
      - $P_a \leftarrow \emptyset$
      - **foreach** $\langle X_b, d(X_b), sup(X_b) \rangle \in P$, with $X_b > X_a$ **do**
        - $X_{ab} = X_a \cup X_b$
        - $d(X_{ab}) = d(X_b) \\ d(X_a)$
        - $sup(X_{ab}) = sup(X_a) - |d(X_{ab})|$
        - **if** $sup(X_{ab}) \geq minsup$ **then**
          - $P_a \leftarrow P_a \cup \{ \langle X_{ab}, d(X_{ab}), sup(X_{ab}) \rangle \}$
      - **if** $P_a \neq \emptyset$ **then** dEclat $(P_a, minsup, \mathcal{F})$

---

- Essa abordagem se mostrou muito eficiente para conjuntos densos
- Por√©m, em conjuntos esparsos, o algoritmo original √© a melhor op√ß√£o

Para bases esparsas: eClat; para bases densas: dEclat

[Imagem (a): Minimun Support (%) - Connect]

[Imagem (b): Minimun Support (%) - pumsb*]

[Imagem (c): Minimun Support (%) - T40I10D100K]

#### Leitura (Aula 05)

- Se√ß√µes 8.1, 8.2 (Zaki e Meira)
- Se√ß√µes 6.1, 6.2 (Introduction to Data Mining)
- Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3): 372-390 (2000)
- Zaki, M.J., Gouda, K.: Fast vertical mining using diffsets. Technical Report 01-1, Computer Science Dept., Rensselaer Polytechnic Institute (March 2001) 10
- Christian Borgelt. Efficient Implementations of Apriori and Eclat. Workshop of Frequent Item Set Mining Implementations (FIMI 2003, Melbourne, FL, USA).

Boa parte da explica√ß√£o est√£o nos artigos. A implementa√ß√£o t√° no Borgelt.

### Slide: aula04-FPGrowth (Aula 05)

#### Recapitulando (Aula 05)

- Apriori: reduzir o n√∫mero de passsadas em disco
- Eclat: trazer pra mem√≥ria e assim reduzir o n√∫mero de passadas em disco, tendendo a zero.

#### Introdu√ß√£o (Aula 05)

- Nessa aula, veremos outro algoritmo que usa proje√ß√µes para reduzir o n√∫mero de passadas para computa√ß√£o dos conjuntos de itens frequentes
- O algoritmo FP-Growth (Frequent Pattern Growth) adota uma estrat√©gia dividir-e-conquistar para reduzir o custo computacional
- Ele, ao contr√°rio do Apriori, n√£o se baseia na gera√ß√£o de candidatos
- Os padr√µes s√£o constru√≠dos ao longo do processamento em profundidade
- Esse algoritmo √©, talvez, o algoritmo sequencial mais eficiente para busca de conjuntos de itens frequentes

A ideia √© evitar ter que computar os candidatos.

#### FP-Growth

- O FP-Growth foi proposto em 2000 por Jiawei Han, Jian Pei e Yiwen Yin
- O algoritmo atacou dois problemas presentes nas abordagens iniciais:
  1. Repetidas passadas sobre a base de dados; e
  2. Gera√ß√£o de candidatos [Mais cr√≠tico]
- O primeiro problema, como j√° discutimos, √© cr√≠tico pelo custo computacional inerente √† leitura em mem√≥ria secund√°ria
- O segundo problema est√° relacionado √† gera√ß√£o de candidatos desnecess√°rios
  - Muitos s√£o descartados pela propriedade do Apriori

##### FP-Tree [√Årvore de Prefixos]

- O FP-Growth possui algumas similaridades ao Eclat:
  - Ambos adotam a estrat√©gia de busca em profundidade
  - Ambos adotam proje√ß√µes dos dados com o intuito de traz√™-los para mem√≥ria principal e reduzir o custo computacional
- O FP-Growth, no entanto, usa uma estrutura de dados diferente para suportar a busca pelos padr√µes
  - Uma √°rvore de prefixos chamada FP-Tree
- A busca pelos padr√µes se d√° inteiramente atrav√©s da √°rvore sem a necessidade de se voltar √† base de dados
- Dessa forma, a primeira tarefa do algoritmo √© construir essa estrutura

"A partir da Base de Dados, como fazer a √°rvore de prefixos?"

---

- A constru√ß√£o da FP-Tree ocorre em duas fases
- Primeiro, o algoritmo varre a base de dados para computar a frequ√™ncia individual de cada item
  - Itens infrequentes s√£o descartados, uma vez que n√£o podem formar padr√µes frequentes
- Segundo, o algoritmo percorre novamente a base processando as transa√ß√µes ordenadas pela frequ√™ncia dos itens
  - Os itens nas transa√ß√µes s√£o ordenados em ordem decrescente de frequ√™ncia e os infrequentes s√£o filtrados
- As transa√ß√µes s√£o ent√£o inseridas na √°rvore enquanto processadas
  - Itens s√£o n√≥s da √°rvore
  - Cada n√≥ armazena um item e sua frequ√™ncia (n√∫mero de transa√ß√µes que o cont√©m)

Basicamente, ele limpa os infrequentes, e depois disso, vai inserindo as transa√ß√µes em uma √°rvore.

---

- Para facilitar a busca pelos padr√µes, a √°rvore √© equipada com uma estrutura adicional para localizar a ocorr√™ncia dos itens e sua frequ√™ncia
- Exemplo

| **TID** | **Muesli (a)** | **Oats (b)** | **Milk (c)** | **Yoghurt (d)** | **Biscuits (e)** | **Tea (f)** |
| :------ | :------------: | :----------: | :----------: | :-------------: | :--------------: | :---------: |
| 1       |       1        |      0       |      1       |        1        |        0         |      1      |
| 2       |       0        |      1       |      1       |        0        |        0         |      0      |
| 3       |       0        |      0       |      1       |        0        |        1         |      1      |
| 4       |       1        |      0       |      0       |        1        |        0         |      0      |
| 5       |       0        |      1       |      1       |        0        |        0         |      1      |
| 6       |       1        |      0       |      1       |        0        |        0         |      1      |

minsup = 2

Em ordem de maior suporte pra menor suporte: cfabd

1. cfad
2. cb
3. cf
4. ad
5. cfb
6. cfa

Obs.: Ignoram-se os itens infrequentes.

#### Minera√ß√£o dos padr√µes [Aula 05]

- A minera√ß√£o dos padr√µes se inicia uma vez que a FP-Tree tenha sido constru√≠da
- A constru√ß√£o agora ocorre aumentando-se prefixos dos padr√µes em ordem crescente de suporte
- As transa√ß√µes que satisfa√ßam (cont√©m) o padr√£o sendo constru√≠do s√£o projetadas em uma nova √°rvore
- Itens podem se tornar infrequentes nessa nova base e s√£o descartados
- Os padr√µes encontrados nessa nova √°rvore devem incluir o prefixo que a gerou
- O algoritmo segue com as extens√µes recursivamente at√© que um √∫nico ramo seja obtido
  - Se a √°rvore possui um √∫nico ramo, os padr√µes obten√≠veis s√£o todas as combina√ß√µes dos n√≥s

Para se minerar as transa√ß√µes de volta, percorremos a lista de itens e ent√£o subimos dele at√© a raiz.

Partindo do item menos frequente e indo pro item mais frequente, fazemos proje√ß√µes da √°rvore.

Essas proje√ß√µes s√£o sub-√°rvores da √°rvore original.

No caso do d, percorrerei todos os n√≥s da lista encadeada de de d's, indo dele at√© a raiz. A jun√ß√£o de todos os n√≥s que eu passar, formar√° uma nova √°rvore. E essa ser√° a proje√ß√£o do item d.

Mas ainda n√£o entendi o que precisa ser feito ap√≥s essa primeira proje√ß√£o.

---

- Exemplo

$$
\begin{bmatrix}
  Item & Freq & Link \\
  c & 5 & \\
  f & 4 & \\
  a & 3 & \\
  b & 2 & \\
  d & 2 & \\
\end{bmatrix}
$$

```mermaid
flowchart LR
  Vazio(("$$\emptyset$$")) --> C(("$$c:5$$"))
  C --> F(("$$f:4$$"))
  F --> A2(("$$a:2$$"))
  A2 --> D1(("$$d:1$$"))
  F --> B1(("$$b:1$$"))
  C --> B2(("$$b:1$$"))
  Vazio --> A1(("$$a:1$$"))
  A1 --> D2(("$$d:1$$"))
```

H√° tamb√©m uma lista encadeada para todos os n√≥s com ocorr√™ncias de um mesmo item.

A lista encadeada serve para podermos percorrer todos os n√≥s de um mesmo item e calcularmos sua frequ√™ncia.

#### Continua na pr√≥xima aula

## Aula 06 | 03/04/2025 | Minera√ß√£o de conjuntos de itens

### Slide: aula04-FPGrowth (Aula 06)

#### FP-Growth (Aula 06)

- O FP-Growth foi proposto em 2000 por Jiawei Han, Jian Pei e Yiwen Yin
- O algoritmo atacou dois problemas presentes nas abordagens iniciais:
  1. Repetidas passadas sobre a base de dados; e
  2. Gera√ß√£o de candidatos [Mais cr√≠tico]
- O primeiro problema, como j√° discutimos, √© cr√≠tico pelo custo computacional inerente √† leitura em mem√≥ria secund√°ria
- O segundo problema est√° relacionado √† gera√ß√£o de candidatos desnecess√°rios
  - Muitos s√£o descartados pela propriedade do Apriori

##### FP-Tree [√Årvore de Prefixos] (Aula 06)

- O FP-Growth possui algumas similaridades ao Eclat:
  - Ambos adotam a estrat√©gia de busca em profundidade
  - Ambos adotam proje√ß√µes dos dados com o intuito de traz√™-los para mem√≥ria principal e reduzir o custo computacional
- O FP-Growth, no entanto, usa uma estrutura de dados diferente para suportar a busca pelos padr√µes
  - Uma √°rvore de prefixos chamada FP-Tree
- A busca pelos padr√µes se d√° inteiramente atrav√©s da √°rvore sem a necessidade de se voltar √† base de dados
- Dessa forma, a primeira tarefa do algoritmo √© construir essa estrutura

"A partir da Base de Dados, como fazer a √°rvore de prefixos?"

---

- A constru√ß√£o da FP-Tree ocorre em duas fases
- Primeiro, o algoritmo varre a base de dados para computar a frequ√™ncia individual de cada item
  - Itens infrequentes s√£o descartados, uma vez que n√£o podem formar padr√µes frequentes
- Segundo, o algoritmo percorre novamente a base processando as transa√ß√µes ordenadas pela frequ√™ncia dos itens
  - Os itens nas transa√ß√µes s√£o ordenados em ordem decrescente de frequ√™ncia e os infrequentes s√£o filtrados
- As transa√ß√µes s√£o ent√£o inseridas na √°rvore enquanto processadas
  - Itens s√£o n√≥s da √°rvore
  - Cada n√≥ armazena um item e sua frequ√™ncia (n√∫mero de transa√ß√µes que o cont√©m)

Basicamente, ele limpa os infrequentes, e depois disso, vai inserindo as transa√ß√µes em uma √°rvore.

---

- Para facilitar a busca pelos padr√µes, a √°rvore √© equipada com uma estrutura adicional para localizar a ocorr√™ncia dos itens e sua frequ√™ncia
- Exemplo

| **TID** | **Muesli (a)** | **Oats (b)** | **Milk (c)** | **Yoghurt (d)** | **Biscuits (e)** | **Tea (f)** |
| :------ | :------------: | :----------: | :----------: | :-------------: | :--------------: | :---------: |
| 1       |       1        |      0       |      1       |        1        |        0         |      1      |
| 2       |       0        |      1       |      1       |        0        |        0         |      0      |
| 3       |       0        |      0       |      1       |        0        |        1         |      1      |
| 4       |       1        |      0       |      0       |        1        |        0         |      0      |
| 5       |       0        |      1       |      1       |        0        |        0         |      1      |
| 6       |       1        |      0       |      1       |        0        |        0         |      1      |

#### Minera√ß√£o dos padr√µes [Aula 05] (Aula 06)

- A minera√ß√£o dos padr√µes se inicia uma vez que a FP-Tree tenha sido constru√≠da
- A constru√ß√£o agora ocorre aumentando-se prefixos dos padr√µes em ordem crescente de suporte
- As transa√ß√µes que satisfa√ßam (cont√©m) o padr√£o sendo constru√≠do s√£o projetadas em uma nova √°rvore
- Itens podem se tornar infrequentes nessa nova base e s√£o descartados
- Os padr√µes encontrados nessa nova √°rvore devem incluir o prefixo que a gerou
- O algoritmo segue com as extens√µes recursivamente at√© que um √∫nico ramo seja obtido
  - Se a √°rvore possui um √∫nico ramo, os padr√µes obten√≠veis s√£o todas as combina√ß√µes dos n√≥s

---

- Exemplo

$$
\begin{bmatrix}
  Item & Freq & Link \\
  c & 5 & \\
  f & 4 & \\
  a & 3 & \\
  b & 2 & \\
  d & 2 & \\
\end{bmatrix}
$$

```mermaid
flowchart LR
  Vazio(("$$\emptyset$$")) --> C(("$$c:5$$"))
  C --> F(("$$f:4$$"))
  F --> A2(("$$a:2$$"))
  A2 --> D1(("$$d:1$$"))
  F --> B1(("$$b:1$$"))
  C --> B2(("$$b:1$$"))
  Vazio --> A1(("$$a:1$$"))
  A1 --> D2(("$$d:1$$"))
```

H√° tamb√©m uma lista encadeada para todos os n√≥s com ocorr√™ncias de um mesmo item.

A lista encadeada serve para podermos percorrer todos os n√≥s de um mesmo item e calcularmos sua frequ√™ncia.

- [JV] Explica√ß√£o do Algoritmo
  - Para se minerar as transa√ß√µes de volta, percorremos a lista de itens e ent√£o subimos dele at√© a raiz.
  - Partindo do item menos frequente e indo pro item mais frequente, fazemos proje√ß√µes da √°rvore.
  - Essas proje√ß√µes s√£o sub-√°rvores da √°rvore original.
  - No caso do d, percorrerei todos os n√≥s da lista encadeada de de d's, indo dele at√© a raiz. A jun√ß√£o de todos os n√≥s que eu passar, formar√° uma nova √°rvore. E essa ser√° a proje√ß√£o do item d.
  - Mas ainda n√£o entendi o que precisa ser feito ap√≥s essa primeira proje√ß√£o.
- [JV] Explica√ß√£o 2
  - Primeiro filtra pelos itens frequentes, removendo os infrenquentes.
    - Ex: minsup = 2
  - Depois disso, ele faz a... "transposi√ß√£o horizontal(?)", ou seja, para cara transa√ß√£o, ele lista todos os itens frequentes que est√£o presentes nela.
  - Ent√£o ordena cada um desses itens por seu suporte.
    - Ex:
      - Em ordem de maior suporte pra menor suporte: cfabd; Obs.: Ignoram-se os itens infrequentes.
      1. cfad
      2. cb
      3. cf
      4. ad
      5. cfb
      6. cfa
  - Depois disso, ele vai inserindo esses itens em uma √°rvore de prefixos, em sequ√™ncia: do primeiro TID at√© o √∫ltimo TID, depois do primeiro item at√© o √∫ltimo item.
  - D√∫vida JV: Qual √© a sequ√™ncia para se percorrer as transa√ß√µes? Da primeira pra √∫ltima? Poder√≠amos ordenar as transa√ß√µes por suporte, e depois fazer a inser√ß√£o na √°rvore? Haveria benef√≠cio ao fazermos isso?
    - Resposta: Sim, a ideia √© percorrer as transa√ß√µes na ordem em que elas aparecem. Por√©m, existe sim benef√≠cio em ordenar, mas n√£o direi agora.
  - √Ä medida em que insere, atualiza a tabela de frequ√™ncia dos itens.
  - Depois de todos preenchidos, ele percorrer√° a tabela das frequ√™ncias, partindo do item menos frequente
  - Agora, percorrendo a lista encadeada do item menos frequente, ele vai subindo at√© a raiz, e ent√£o vai criando uma nova √°rvore de prefixos, que ser√° a proje√ß√£o dos sufixos do item menos frequente.
  - Por√©m, ao inv√©s de fazer uma lista dos sufixos, ele faz uma sub-√°rvore que representa todos os sufixos do item que estamos percorrendo. Por√©m, omitindo o item em si.
  - Ap√≥s criada essa sub-√°rvore, podaremos os itens que n√£o s√£o frequentes, segundo a sub-tabela de frequ√™ncias dessas sub-√°rvores.
  - Ent√£o √© feito um merge dos ramos que sobraram, somando os suportes dos itens que sobraram.
    - Eu estimo que esse merge seja feito partindo da raiz, e conferindo se todos os seus filhos n√£o diferentes entre si. Caso sejam iguais, eles s√£o mesclados e seus filhos tamb√©m, assim somando e unindo recursivamente.
  - Agora sim s√£o gerados os itemsets de padr√µes frequentes ao gerar todas as possibilidades partindo do conjunto vazio que √© a raiz, e parando em cada um dos n√≥s que sobraram.
    - **OBS.: ESSA ETAPA S√ì OCORRE CASO A √ÅRVORE TENHA SOMENTE UM RAMO. SE A √ÅRVORE TIVER MAIS DE UM RAMO, O ALGORITMO SE REPETE RECURSIVAMENTE, AT√â QUE A √ÅRVORE TENHA SOMENTE UM RAMO.**
    - Suponho tamb√©m que, na √∫ltima recurs√£o, o algoritmo, mesmo que sejam podados todos os n√≥s vis√≠veis (n√£o considerando o $\emptyset$), ele ainda assim gere os padr√µes frequentes, sendo ele o item da qual a √°rvore √© proje√ß√£o.

#### Quest√µes de implementa√ß√£o

- E se a FP-tree n√£o couber na mem√≥ria?
  - A solu√ß√£o √© particionar/projetar a base de dados em mem√≥ria secund√°ria antes de iniciar a constru√ß√£o da √°rvore
- Como construir a FP-tree de forma eficiente?

  - Solu√ß√£o proposta por Christian Borgelt otimiza mem√≥ria e tempo
  - Representa√ß√£o b√°sica dos dados: lista de vetores de inteiros
  - Dados (proje√ß√µes) s√£o carregados inteiramente para mem√≥ria
  - Lista √© seccionada com base no k-√©simo item; um n√≥ √© criado para cada se√ß√£o
  - N√≥s t√™m tamanho fixo (20 bytes em 32bits; 40 em 64bits)
    - 1x identificador de item
    - 1x contador de frequ√™ncia
    - 1x ponteiro para n√≥ pai
    - 1x ponteiro para pr√≥xima ocorr√™ncia do item
    - 1x ponteiro para n√≥ auxiliar

- [JV] Explica√ß√£o da cria√ß√£o
  - Ao inv√©s de fazer a FP-Tree, ele primeiro ordena todos as transa√ß√µes, ele, recursivamente:
    - agrupa elas pelo prefixo inicial. Cada prefixo inicial ser√° um n√≥ apontando ao seu pai. Inicialmente sendo o pai o n√≥ raiz, o $\emptyset$.
    - E ent√£o repete isso para cada um dos sufixos que sobraram, at√© que n√£o haja mais sufixos.

---

- Implementa√ß√£o tradicional, conforme descri√ß√£o do algoritmo, evita carregar dados para mem√≥ria
  - Por√©m, n√≥s ter√£o tamanho vari√°vel ou desperdi√ßam mem√≥ria (ponteiros para filhos que nunca ocorrem)
  - Melhora gerenciamento de mem√≥ria; grandes blocos podem ser alocados de uma vez e gerenciados internamente
  - Al√©m disso, ponteiros para pais s√£o mais √∫teis que ponteiros para filhos durante execu√ß√£o

---

- Proje√ß√µes s√£o executadas com dois la√ßos
  - Um la√ßo externo percorre o n√≠vel mais baixo (elemento condicionante da proje√ß√£o)
  - La√ßo interno percorre a os ramos origin√°rios do n√≥ folha
- A nova √°rvore √© constru√≠da como uma 'sombra da original
  - N√≥s s√£o duplicados conforme s√£o visitados (ponteiro auxiliar mant√©m elo de liga√ß√£o entre original e c√≥pia para atualiza√ß√µes necess√°rias durante constru√ß√£o)
  - Frequ√™ncia do n√≥ folha √© propagada para cima
- A sombra √© destacada da √°rvore original em uma segunda passada pelos n√≥s
- N√≥s infrequentes podem ser removidos e n√≥s com mesmo r√≥tulo mesclados

---

[Imagem (a): FP-Growth VS Eclat VS Apriori - Chess]

[Imagem (b): FP-Growth VS Eclat VS Apriori - Mushroom]

Figuras retiradas de Borgelt, C. An Implementation of the FP-growth Algorithm

#### Leitura [Aula 05]

- Se√ß√£o 6.6 Intro to Data Mining
- Se√ß√£o 8.2.3 Zaki e Meira
- [Borgelt, C. (2005) An Implementation of the FP-growth Algorithm][LinkFPGrowth]

[LinkFPGrowth]: https://borgelt.net/papers/fpgrowth.pdf

### Slide: aula05-repr-compactadas (Aula 06)

#### Introdu√ß√£o (Aula 06)

- Nessa aula, vamos discutir representa√ß√µes compactas para o conjunto de todos os conjuntos de itens frequentes de uma base de dados
- Representa√ß√µes compactas s√£o subconjuntos a partir dos quais √© poss√≠vel derivar todos os conjuntos de itens frequentes
- Para motivar a necessidade dessas representa√ß√µes, considere uma base de dados com somente duas transa√ß√µes e 100 itens:
  - $D = \{(0, a_{1}, a_{2}, \dots, a_{50}), (1, a_{1}, a_{2}, \dots, a_{100})\}$
- Se considerarmos um minsup=1, essa base ter√°

  - $\binom{100}{1} + \binom{100}{2} + \dots + \binom{100}{100} = 2^{100} - 1 \approx 1.27E^{30}$

- [JV]
  - Podemos considerar que:
    - $01 = a_{1}a_{2}\dots a_{50}$
    - $ 1 = a*{1}a*{2}\dots a\_{100}$
  - O que seriam representa√ß√µes compactas do conjunto de itemsets frequentes

---

- Nesse caso, o problema se torna incomput√°vel por qualquer das abordagens que vimos anteriormente
- Embora esse seja um caso extremo, n√£o √© raro que situa√ß√µes similares a essa ocorram na pr√°tica
  - √â poss√≠vel, por exemplo, que um subconjunto das transa√ß√µes e itens apresentem esse comportamento em uma base de dados maior
- Note que podemos particionar os itemsets frequentes em duas classes de equival√™ncia:
  - Os que ocorrem em ambas as transa√ß√µes; e
  - Os que ocorrem somente na segunda

---

- Os que ocorrem em ambas as transa√ß√µes, possuem cobertura $c(X)=01$, e, portanto, s√£o equivalentes a $a_{1}a_{2}\dots a_{50}$
- Os que ocorrem somente na segunda transa√ß√£o, possuem cobertura $c(X)=1$, e, portanto, s√£o equivalentes a $a_{1}a_{2}\dots a_{100}$
  - Al√©m disso, se estiv√©ssemos somente interessados nos itemsets frequentes sem a informa√ß√£o da frequ√™ncia, todos seriam equivalentes a esse itemset
- Em outras palavras, os mais de $10^{30}$ itemsets que seriam retornados por qualquer dos algoritmos vistos poderiam ser representados somente por esses dois conjuntos
- Esses conjuntos formam, dessa forma, uma **representa√ß√£o compacta** de todo o conjunto de itemsets frequentes
- Em particular, eles est√£o relacionados a dois tipos de representa√ß√µes compactas que veremos nessa aula
  - Conjuntos frequentes **m√°ximos**
  - Conjuntos frequentes **fechados**

#### Representa√ß√µes compactas

- O particionamento dos itemsets no exemplo anterior se deu pelos tidsets que compunham suas extens√µes
- De fato, o racioc√≠nio se aplica a qualquer base de dados. Ou seja, podemos particionar os itemsets conforme sua cobertura
- Dentro de cada classe de equival√™ncia, podemos ordenar os elementos conforme a rela√ß√£o de subconjunto
  - O maior elemento da classe √© chamado de conjunto fechado ou **closed itemset**
    - [JV:] O maior item poss√≠vel dos itemsets. Exemplo: $a_{1}a_{2}\dots a_{50}$ || $P = c(i(P))$
  - Os menores elementos da classe s√£o chamados de **minimal generators**
    - [JV:] Exemplo: cada um dos itenzinhos que foram concatenados. ($a_{1}, a_{2}, \dots, a_{50}$) || $X = i(c(X))$
- Os maiores elementos entre todos os conjuntos fechados s√£o chamados de conjuntos frequentes m√°ximos (**maximal itemsets**)
  - [JV:] O maior itemset poss√≠vel entre todos os conjuntos fechados. Exemplo: $a_{1}a_{2}\dots a_{100}$ || "S√£o os maiores itemsets frequentes"

---

- Os conjuntos m√°ximos s√£o os maiores itemsets frequentes
- Eles definem a 'borda entre o que √© frequente e infrequente
- Como, por defini√ß√£o, n√£o existem conjuntos frequentes maiores que eles, **todos os conjuntos frequentes podem ser derivados a partir dos conjuntos m√°ximos**
- No entanto, o c√°lculo do suporte n√£o pode ser obtido diretamente desses itemsets, sendo necess√°ria uma nova passada na base de dados para comput√°-lo
- O itemset $a_{1}a_{2} \dots a_{100}$ no nosso exemplo inicial √© um conjunto frequente m√°ximo

---

- Essa necessidade de novas passadas na base de dados para computar o suporte dos itemsets frequentes a partir dos m√°ximos torna a representa√ß√£o incompleta
- Os conjuntos fechados, por outro lado, s√£o uma representa√ß√£o completa, j√° que tanto os itemsets quanto seu suporte podem ser derivados desses conjuntos
- Como dito, todo conjunto fechado d√° origem a uma classe de equival√™ncia
  - $[X] = \{Y \subseteq I | c(Y) = c(X)\} = \{Y \subseteq I | i(c(Y)) = X\}$
- Assim, podemos verificar o suporte de um itemset frequente a partir dos conjuntos fechados da seguinte forma
  - $sup(ùëã) = max \{sup(Y) | Y \in \mathcal{C} \wedge X \subseteq Y\}$
  - Em outras palavras, basta encontrarmos a classe de equival√™ncia √† qual o itemset pertence; todo itemset frequente ou √© fechado ou pertence √† classe de equival√™ncia de algum conjunto fechado, como o suporte √© anti-monot√¥nico, se ele n√£o for fechado, ele pertence √† classe do de maior suporte.

---

- Exemplo: minsup=1

| TID | Muesli (m) | Oats (o) | Milk (m) | Yoghurt (y) |
| :-- | :--------: | :------: | :------: | :---------: |
| 1   |     1      |    0     |    1     |      1      |
| 2   |     0      |    1     |    1     |      0      |
| 3   |     0      |    0     |    1     |      0      |
| 4   |     1      |    0     |    0     |      1      |
| 5   |     0      |    1     |    1     |      0      |
| 6   |     1      |    0     |    1     |      0      |

---

```mermaid
flowchart LR

  %% Styles
  classDef Infrequent fill:#ffffff, color:#000000;
  classDef Frequent fill:#fffba6, color:#000000;
  classDef Maximal fill:#fffba6, color:#000000, stroke-width:5px, stroke:#000000;

  %% V√©rtices
  MOKY(("MOKY $$\{\}$$")):::Infrequent
  MOK(("MOK $$\{\}$$")):::Infrequent
  MOY(("MOY $$\{\}$$")):::Infrequent
  MKY(("MKY $$1$$")):::Maximal
  OKY(("OKY $$\{\}$$")):::Infrequent
  MO(("MO $$\{\}$$")):::Infrequent
  MK(("MK $$16$$")):::Frequent
  MY(("MY $$14$$")):::Frequent
  OK(("OK $$25$$")):::Maximal
  OY(("OY $$\{\}$$")):::Infrequent
  KY(("KY $$\{\}$$")):::Infrequent
  M(("M $$146$$")):::Frequent
  O(("O $$25$$")):::Frequent
  K(("K $$12356$$")):::Frequent
  Y(("Y $$14$$")):::Frequent
  Vazio(("$$\emptyset$$")):::Frequent

  %% Label
  Freq[Frequent Itemsets]:::Infrequent
  Infreq[Infrequent Itemsets]:::Frequent
  Max[Maximal Itemsets]:::Maximal

  %% Arestas
  MOKY --- MOK & MOY & MKY & OKY
  MOK --- MO & MK & OK
  MOY --- MO & MY & OY
  MKY --- MK & MY & KY
  OKY --- OK & OY & KY
  MO --- M & O
  MK --- M & K
  MY --- M & Y
  OK --- O & K
  OY --- O & Y
  KY --- K & Y
  M --- Vazio
  O --- Vazio
  K --- Vazio
  Y --- Vazio
```

---

```mermaid
flowchart LR

  %% Styles
  classDef Infrequent fill:#ffffff, color:#000000;
  classDef Frequent1 fill:#fffba6, color:#000000;
  classDef Frequent2 fill:#fffba6, color:#000000;
  classDef Frequent3 fill:#fffba6, color:#000000;
  classDef Maximal fill:#fffba6, color:#000000, stroke-width:5px, stroke:#000000;

  %% V√©rtices
  MOKY(("MOKY $$\{\}$$")):::Infrequent
  MOK(("MOK $$\{\}$$")):::Infrequent
  MOY(("MOY $$\{\}$$")):::Infrequent
  MKY(("MKY $$1$$")):::Maximal
  OKY(("OKY $$\{\}$$")):::Infrequent
  MO(("MO $$\{\}$$")):::Infrequent
  MK(("MK $$16$$")):::Frequent
  MY(("MY $$14$$")):::Frequent
  OK(("OK $$25$$")):::Maximal
  OY(("OY $$\{\}$$")):::Infrequent
  KY(("KY $$\{\}$$")):::Infrequent
  M(("M $$146$$")):::Frequent
  O(("O $$25$$")):::Frequent
  K(("K $$12356$$")):::Frequent
  Y(("Y $$14$$")):::Frequent
  Vazio(("$$\emptyset$$")):::Frequent

  %% Label
  Freq[Frequent Itemsets]:::Infrequent
  Infreq[Infrequent Itemsets]:::Frequent
  Max[Maximal Itemsets]:::Maximal

  %% Arestas
  MOKY --- MOK & MOY & MKY & OKY
  MOK --- MO & MK & OK
  MOY --- MO & MY & OY
  MKY --- MK & MY & KY
  OKY --- OK & OY & KY
  MO --- M & O
  MK --- M & K
  MY --- M & Y
  OK --- O & K
  OY --- O & Y
  KY --- K & Y
  M --- Vazio
  O --- Vazio
  K --- Vazio
  Y --- Vazio
```

Os azuis e verdes s√£o classes de equival√™ncia.

#### Algoritmos para encontrar representa√ß√µes compactas

- Os exemplos mostram que as representa√ß√µes compactas apresentam vantagens sobre o conjunto de todos os itemsets frequentes
- No entanto, se usarmos os algoritmos vistos anteriormente para encontrar essas representa√ß√µes, n√£o temos ganho computacional algum em rela√ß√£o ao problema anterior
  - Pode ser que a minera√ß√£o desses padr√µes continue invi√°vel
- Existem diversos algoritmos espec√≠ficos para minera√ß√£o de itemsets m√°ximos e fechados
- Veremos um representante de cada desses algoritmos

"Voc√™ consegue encontrar todos ..."

##### DCI_Closed - Pr√≥xima aula

## Aula 07 | 08/04/2025 | Minera√ß√£o de sequ√™ncias - Faltei pq tava passando mal

### Slide: aula05-repr-compactadas (Aula 07)

#### Algoritmos para encontrar representa√ß√µes compactas (Aula 07)

##### DCI_Closed (Aula 07)

- O algoritmo DCI_Closed foi proposto em 2004 por C. Lucchese, S. Orlando e R. Perego
- Ele tamb√©m adota uma representa√ß√£o vertical da base de dados para facilitar a verifica√ß√£o dos conjuntos fechados
- O algoritmo explora o espa√ßo de busca usando uma estrat√©gia dividir- e-conquistar
- Os autores demonstraram que o problema pode ser decomposto em subproblemas independentes, permitindo inclusive uma solu√ß√£o paralela

---

- A ideia central do algoritmo √© 'escalar' o reticulado de itemsets, percorrendo cada classe de equival√™ncia uma √∫nica vez
- Somente um candidato de cada classe √© avaliado para computar o seu conjunto fechado
- Novamente, assume-se uma ordem lexicogr√°fica sobre os itens da base, e sua extens√£o sobre os itemsets
  - Qualquer ordem serve, inclusive a ordem sobre os r√≥tulos dos itens
  - Essa ordem ser√° representado por $\prec$
- Novos candidatos s√£o gerados a partir dos conjuntos fechados obtidos, estendendo-os com itens ainda n√£o investigados
  - Esses candidatos s√£o chamados de **geradores**
  - Formalmente, um gerador √© um conjunto $X = Y_i$, para um conjunto fechado $Y$ e um item $i$

---

- Um gerador $X = Y_i$ √© dito **ordem-conservante** sse $i \prec (i(c(X)) - X)$
  - Em palavras, $X$ √© ordem-conservante se todo item que tiver que ser adicionado a X para obter o conjunto fechado for maior que $i$
- Teorema 1: Para todo conjunto fechado $Y \neq i(c(\emptyset))$, existe uma sequ√™ncia de $n \geq 1$ extens√µes (items) $i_0 \prec i_1 \prec \dots \prec i_{n-1}$ tais que $gen_0 = Y_0i_0$, $gen_1 = Y_1i_1$, $gen_{n-1} = Y_{n-1}i_{n-1}$, em que todos os $gen_k$ s√£o ordem- conservantes, $Y_0 = i(c(\emptyset))$, $Y_{j+1} = i(c(Y_ji_j))$ e $Y_n=Y$.
- Corol√°rio: Essa sequ√™ncia √© √∫nica.

---

- O problema agora √© verificar se um gerador √© ordem-conservante
- Lema 1: Seja $gen = Y_i$, para um conjunto fechado $Y$ e item $i$. Se $\exists j \prec i [j \notin gen \wedge c(gen) \subseteq c(j)]$, ent√£o $gen$ n√£o √© ordem-conservante.
  - Intuitivamente, $c(gen) \subseteq c(j)$ implica em $j \in i(c(gen))$, e como $j \notin gen$, $j \in i(c(gen)) - gen$; ou seja, $i \nprec i(c(gen)) - gen$
- Sendo assim, basta mantermos uma lista de elementos menores que $i$ n√£o pertencentes a $gen$ para verificarmos se ele √© ordem-conservante durante a execu√ß√£o do algoritmo
  - Essa lista √© chamada de **pre-set**
  - N√£o h√° necessidade de manter os conjuntos fechados em mem√≥ria!
- O espa√ßo de busca pode ser percorrido a partir de $i(c(\emptyset))$ e todos os itens frequentes como poss√≠veis extens√µes
  - Os geradores s√£o avaliados conforme a ordem lexicogr√°fica
  - Se encontrarmos um gerador n√£o ordem-conservante, podamos o ramo
  - Ap√≥s explorar o ramo com um item $i$, ele √© colocado no **pre-set**

---

- **procedure** $DCI\_Closed_d$ (CLOSED_SET, PRE_SET, POST_SET)
  - **while** POST_SET $neq \emptyset$ **do**
    - $i \leftarrow min_{\prec}$ (POST_SET)
    - POST_SET $leftarrow$ POST_SET \ $i$
    - $new\_gen \leftarrow$ CLOSED_SET $\cup i$ \\\\ Build a new generator
    - **if** $supp(new\_gen) \geq minsupp$ **then**
      - $\neg$ is_dup (new_gen, PRE_SET) **then** \\\\ if $new\_gen$ is both frequent and order preserving
      - CLOSED*SET $*{New} \leftarrow new_gen$
      - POST*SET $*{New} \leftarrow \emptyset$
      - **for all** $j \in$ POST_SET **do** \\\\ Compute closure of $new\_gen$
        - **if** $g(new\_gen) \subseteq g(j)$ **then**
          - CLOSED*SET $*{New} \leftarrow$ CLOSED*SET $*{New} \cup j$
        - **else**
          - POST*SET $*{New} \leftarrow$ POST*SET$*{New} \cup j$
        - **end if**
      - **end for**
      - **Write Out** CLOSED*SET $*{New}$ _and its support_
      - DCI*Closed $_d$ CLOSED_SET $*{New}$, PRE_SET, POST_SET $_{New}$
      - PRE_SET $leftarrow$ PRE_SET $\cup i$
    - **end if**
  - **end while**
- **end procedure**

---

- **function** $is\_dup$ ($new\_gen$, PRE_SET) \\ Duplicate check
  - **for all** $j \in$ PRE_SET **do**
    - **if** $g(new\_gen) \subseteq g(j)$ **then**
      - **return** TRUE \\ $new\_gen$ is not order preserving
    - **end if**
  - **end for**
  - **return** FALSE
- **end function**

---

- Exemplo: minsup = 2

| **TID** | **Muesli (a)** | **Oats (b)** | **Milk (c)** | **Yoghurt (d)** | **Biscuits (e)** | **Tea (f)** |
| :------ | :------------: | :----------: | :----------: | :-------------: | :--------------: | :---------: |
| 1       |       1        |      0       |      1       |        1        |        0         |      1      |
| 2       |       0        |      1       |      1       |        0        |        0         |      0      |
| 3       |       0        |      0       |      1       |        0        |        1         |      1      |
| 4       |       1        |      0       |      0       |        1        |        0         |      0      |
| 5       |       0        |      1       |      1       |        0        |        0         |      1      |
| 6       |       1        |      0       |      1       |        0        |        0         |      1      |

##### MAFIA: Maximal Frequent Itemset Algorithm - Pr√≥xima aula

## Aula 08 | 10/04/2025 | Minera√ß√£o de sequ√™ncias

### Slide: aula05-repr-compactadas (Aula 08)

#### Algoritmos para encontrar representa√ß√µes compactas (Aula 08)

##### MAFIA: Maximal Frequent Itemset Algorithm

- O algoritmo MAFIA foi proposto em 2001 por D. Burdick, M. Calimlim e J. Gehrke
- A ideia geral do algoritmo √© a de explorar o reticulado de itemsets com uma abordagem best-first/branch-and-bound
- Assim como o Eclat, o MAFIA tamb√©m assume uma representa√ß√£o vertical dos dados usando vetores de bits
- O algoritmo tamb√©m assume uma ordem lexicogr√°fica sobre os itens e a ordem parcial de subconjuntos entre os itemsets durante a explora√ß√£o

Se eu quero minimizar a quantidade de itens mostrados pro usu√°rio, posso mostrar os maximais.

Existem v√°rios algoritmos que encontram os maximais.

Nessa √©poca dos anos 2000, assim como hoje √© com IA e Deep Learning, na √©poca era essa minera√ß√£o de itens frequentes.

"Os maximais est√£o na fronteira"

"A maioria dos artigos na √©poca costumava usar vetores de bits para representar itemsets" Isso por causa dos benef√≠cios de velocidade de processamento para uni√µes e interse√ß√µes.

Na nota√ß√£o vertical, a primeira coluna s√£o os itens, e a segunda √© a lista das transa√ß√µes que possuem esse item. Para calcular rapidamente o suporte √© manter em uma estrutura auxiliar a quantidade de bits ativos.

---

- Durante a explora√ß√£o do espa√ßo de busca, o algoritmo divide os itens em dois grupos
  - **Head**: contendo o r√≥tulo do n√≥ corrente (itemset) na explora√ß√£o
  - **Tail**: os itens que s√£o maiores que o maior elemento do Head (poss√≠veis extens√µes para o itemset)
- O conjunto de todos os itens que podem aparecer numa dada sub√°rvore √© a uni√£o entre o head e o tail (chamado de **HUT** - head union tail - pelos autores)
- Ao inv√©s de adotar uma explora√ß√£o puramente em profundidade, em cada n√≥, o algoritmo avalia os filhos imediatos para remover poss√≠veis extens√µes do tail

  - Eles chamam essa estrat√©gia de **reordenamento din√¢mico (dynamic reordering)**

- [JV]

  - O conjunto de itens ser√° dividido em dois:
    - Head: itemset que j√° visitei
    - Tail: itens que ainda vou considerar
  - HUT: Maior itemset que pode ser obtido a partir da sub √°rvore explorada.
  - "Posso podar se encontrar algum que seja maior do que o j√° encontrado"

- [JV]
  - Algoritmo
    - No n√≠vel que eu t√¥, ele calcula o suporte de todos os filhos. Ent√£o ele pode podar todos os que forem infrequentes.
    - D√∫vida: se todos os items de determinado n√≠vel fossem infrequentes, isso implicaria na head ser infrequente tamb√©m?
      - N√£o. Isso porque sempre estamos indo do mais geral pro menos geral.

---

- Ao explorar o n√≥ P na figura ao lado
  - Head = 1
  - Tail = 234
  - HUT = 1234
- Antes de explorar em profundidade o n√≥, o algoritmo avalia os filhos 12, 13 e 14
- Como s√≥ 12 √© frequente, 3 e 4 podem ser removidos do tail porque qualquer candidato dessa sub√°rvore que inclua esses itens ser√° infrequente
  - O ramo de 12 √© podado

[Imagem]

- [JV]
  - Sempre descarta os itens que s√£o infrequentes.
  - Se o HUT encontrado √© subconjunto de um maximal j√° encontrado, posso podar todo o HUT.
  - Algoritmo:
    - Come√ßa gerando todas as combina√ß√µes simples do item do head com os itens do tail.
    - Poda todos os que forem infrequentes.
    - Depois de remover os infrequentes, remove tamb√©m do tail.
    - Obs.: Mesmo que um item frequente seja composto por diversos conjuntos infrequentes, ainda assim ele permanece sendo frequente.
      - Ex.: Se eu compro AB, compro AC, compro AD. Embora apenas exista uma ocorr√™ncia de AB, uma de AC e uma de AD, ainda assim, A foi comprado 3 vezes, logo, √© frequente.
    - Obs.: Best-First Search: Procura sempre o melhor. Procura sempre o caminho com maior estimativa de lucro. Parece um pouco com o conceito do algoritmo de Prim.
    - Ao final dessa an√°lise, ap√≥s podado o que deve ser podado, as folhas representam os conjuntos maximais.

---

- Note que, sempre que uma folha √© visitada, um candidato a itemset m√°ximo √© encontrado
  - Ele ser√° inclu√≠do na solu√ß√£o final somente se n√£o possuir um superconjunto j√° inclu√≠do
  - A visita√ß√£o em ordem lexicogr√°fica em profundidade garante que conjuntos n√£o tenham que ser removidos da solu√ß√£o final
- Outra poda vem do fato de que itemsets m√°ximos s√£o tamb√©m fechados e que, portanto, para um itemset $X$ (head) e $y \in X.tail$:
  - Se $c(X) \subseteq c(y)$, ent√£o $X_y \subseteq i(c(X))$ (isto √©, $y$ pertence ao conjunto fechado da classe √† qual $X$ pertence)
  - Nesse caso, $y$ pode ser incorporado ao head e removido do tail
- Essa poda √© chamada de Parent Equivalence Pruning (PEP)

- [JV]
  - "Sempre entra, nunca sai"
  - "Todo itemset maximal, √© tamb√©m um itemset fechado"
  - Itemset fechado: O maior itemset de uma classe de equival√™ncia.
    - Ele explicou um pouco mais. Tem algo sobre checar todos os que t√™m a mesma cobertura e verificar qual o maior ou algo assim.

---

- Finalmente, podemos usar a propriedade do Apriori para descartar um ramo baseado no HUT
  - Lembrando que HUT √© a uni√£o do head com o tail e representa o maior itemset que pode ser obtido a partir dessa sub√°rvore
- Se o HUT for subconjunto de algum itemset m√°ximo j√° descoberto anteriormente, sabemos que ele e todos os seus subconjuntos s√£o frequentes, e, al√©m disso, n√£o podem ser m√°ximos. Portanto, podemos podar a sub√°rvore.

---

- Pseudocode: _MAFIA_ (**C**, **MFI**, Boolean **IsHUT**)

  - name **HUT** = **C**.head $\cup$ **C**.tail
  - if **HUT** is in **MFI**
    - Stop generation of children and return
  - Count all children, use PEP to trim the tail, and reorder by increasing support
    - For each item **i** in **C**.trimmed_tail
      - **IsHUT** = whether **i** is in the first item in the trail
      - **newNode** = **C** $\cup$ **I**
      - _MAFIA_ (**newNode, MFI, IsHUT**)
    - if (**IsHUT** and all extensions are frequent)
      - Stop search and go back up subtree
    - if (**C** is a leaf and **C**.head is not in **MFI**)
      - Add **C**.head to **MFI**

- [JV]
  - Algoritmo
    - Faz o HUT
    - Faz a poda do apriori
    - Faz a poda do PEP para poder o Tail
    - E para cada item vai procurando usando o Best-First
  - No artigo tem os pseudo-c√≥digos dos outros sub-c√≥digos.
  - A maior cr√≠tica a esse m√©todo √© que n√£o tem muito uma m√©trica de qualidade. O que temos √© a defini√ß√£o do suporte e se ele √© bom o bastante pro usu√°rio.
- [Quadro]
  - $MFI \subseteq FCI \subseteq FIM$
  - Apriori: $FIM =  \{ X \subseteq I | sup (x) \geq minsup \}$
  - DCI: $FCI = \{ X \subseteq I | X = i(c(x)) \wedge sup (x) \geq minsup \}$
  - MAFIA: $MFI = \{ X \subseteq I | sup (x) \geq minsup \wedge \nexists Y \supseteq X \wedge sup (y) \geq minsup \}$

---

- Exemplo: minsup = 2

| **TID** | **Muesli (a)** | **Oats (b)** | **Milk (c)** | **Yoghurt (d)** | **Biscuits (e)** | **Tea (f)** |
| ------: | -------------: | -----------: | -----------: | --------------: | ---------------: | ----------: |
|       1 |              1 |            0 |            1 |               1 |                0 |           1 |
|       2 |              0 |            1 |            1 |               0 |                0 |           0 |
|       3 |              0 |            0 |            1 |               0 |                1 |           1 |
|       4 |              1 |            0 |            0 |               1 |                0 |           0 |
|       5 |              0 |            1 |            1 |               0 |                0 |           1 |
|       6 |              1 |            0 |            1 |               0 |                0 |           1 |

##### Leitura (Aula 08)

- Se√ß√£o 9.1 Zaki e Meira
- Se√ß√£o 6.2.6 Han et al.
- Burdick, D., Calimlim, M., & Gehrke, J. (2001, April). Mafia: A maximal frequent itemset algorithm for transactional databases. In Proceedings 17th international conference on data engineering (pp. 443-452). IEEE.
- Lucchese, C., Orlando, S., & Perego, R. (2004, November). DCI Closed: A Fast and Memory Efficient Algorithm to Mine Frequent Closed Itemsets. In FIMI.

- [JV]
  - √â interessante, agora que j√° entendeu como o algoritmo funciona, ler e entender a narrativa do artigo.

### Slide: aula06-sequencias (Aula 08)

#### Introdu√ß√£o (Aula 08)

- Nessa aula, vamos discutir o problema de minera√ß√£o de sequ√™ncias em bases de dados
- Esse problema ocorre com frequ√™ncia em diversas √°reas
  - Identificar trajet√≥rias dos alunos de computa√ß√£o
  - Identificar perfil (temporal) de compras dos clientes (celular $\to$ capa protetora $\to$ fone de ouvido)
  - Identificar padr√µes de genes e prote√≠nas no genoma
- Enquanto itemsets s√£o padr√µes intra-transa√ß√µes, aqui estamos buscando padr√µes inter-transa√ß√µes

- [JV]
  - Agora buscaremos ver as trajet√≥rias usuais.
  - Talvez poder√≠amos avaliar quais disciplinas os alunos tendam a fazer mais frequentemente. Distintos da sequ√™ncia usual.
  - Busca-se entender os comportamentos dos clientes.

---

- Para ilustrar, considere a seguinte base de dados
- Os clientes fazem diversas compras na loja
  - Mais de 1 item pode ser adquirido em uma transa√ß√£o
- Existe algum padr√£o de compras?
  - Determinados itens s√£o comprados em sequ√™ncia?
- Esse problema √© conhecido como **minera√ß√£o de sequ√™ncias (frequentes)**

| **ID Cliente** | **Data** | **Transa√ß√£o**         | **JV: Enxugado** |
| :------------- | :------: | :-------------------- | :--------------- |
| 1              | 25/06/19 | aveia                 | a                |
| 1              | 30/06/19 | castanha              | c                |
| 2              | 10/06/19 | granola, mel          | gm               |
| 2              | 15/06/19 | aveia                 | a                |
| 2              | 20/06/19 | banana, suco, leite   | bsl              |
| 3              | 25/06/19 | aveia, iogurte, leite | ail              |
| 4              | 25/06/19 | aveia                 | a                |
| 4              | 30/06/19 | banana, leite         | bl               |
| 4              | 25/07/19 | castanha              | c                |
| 5              | 12/06/19 | castanha              | c                |
| 6              | 10/06/19 | aveia, granola        | ag               |
| 6              | 11/06/19 | leite                 | l                |
| 6              | 17/06/19 | banana, leite         | bl               |

#### Defini√ß√µes

- A base de dados √© um conjunto de transa√ß√µes consistindo em:
  - ID do cliente
  - Timestamp da transa√ß√£o
  - Itens 'comprados'
- Os itens da transa√ß√£o s√£o um itemset de uma cole√ß√£o de poss√≠veis itens
- Uma **sequ√™ncia** √© uma **lista ordenada de itemsets**
  - Os itemsets tamb√©m s√£o chamados de elementos
- Para um conjunto de itens $I$, uma sequ√™ncia $s = \langle s_1 s_2 \dots s_n \rangle$ em que cada $s_i \subseteq I$ √© um itemset

  - Por defini√ß√£o, um item n√£o pode aparecer mais de uma vez num itemset, mas pode aparecer v√°rias vezes numa sequ√™ncia

- [JV]
  - Exemplo de sequ√™ncia: $\langle (gm) a (bsl) \rangle$
    - gm: granola e mel
    - a: aveia
    - bsl: banana, suco e leite
    - Cada grupinho faz parte de uma transa√ß√£o. Eles apenas est√£o ordenados temporalmente, n√£o internamente. Tanto que $\langle gm \rangle \equiv \langle mg \rangle$.

---

- Por exemplo, a sequ√™ncia $\langle (gm) a (bsl) \rangle$ representa a sequ√™ncia de compras do cliente 2 na base de dados anterior
  - Itemsets s√£o delimitados por par√™ntesis; itemsets unit√°rios s√£o representados sem par√™ntesis
- Uma sequ√™ncia $\alpha = \langle a_1 a_2 \dots a_n \rangle$ √© uma subsequ√™ncia de uma sequ√™ncia $\beta = \langle b_1 b_2 \dots b_m \rangle$, $\alpha \subseteq \beta$, se existe uma fun√ß√£o $\phi: [1:n] \to [1:m]$ tal que
  - $a_1 \subseteq b_{\phi(1)}$; e
  - $\forall i, j [i < j \to \phi(i) < \phi(j)]$
- As sequ√™ncias $\langle (gm) b \rangle$, $\langle mab \rangle$ e $\langle a \rangle$ s√£o subsequ√™ncias de $\langle (gm) a (bsl) \rangle$
- Note que a ordem √© definida somente entre elementos, e n√£o dentro dos itemsets

  - Contudo, vamos assumir que os elementos s√£o dispostos conforme alguma ordem dentro dos itemsets (em nosso caso, a ordem que forma apresentados na base original)

- [JV]
  - Para simplifica√ß√£o de nota√ß√£o, sempre que uma transa√ß√£o tiver apenas um item, ele n√£o fica englobado por par√™nteses. Se tiver 2 ou mais, tem.
  - Uma sequ√™ncia √© subsequ√™ncia de outra se todos os itens de um subconjunto aparecem no outro, e mant√©m a mesma ordem, mesmo que n√£o estejam nas mesmas posi√ß√µes, e n√£o precisam ser necessariamente cont√≠guos.
  - Ent√£o, uma forma de analisar √© se cada um dos grupinhos de itens das transa√ß√µes √© subconjunto das outras transa√ß√µes.
  - Eu ainda posso explicar isso aqui melhor. Eu entendi, mas n√£o anotei legal.

---

- Podemos redefinir a base de dados como um conjunto de pares $(sid, s)$ em que sid √© um identificador de sequ√™ncia e s uma sequ√™ncia
  - Cada identificador de cliente √© um sid
  - As diferentes transa√ß√µes de um cliente ordenados pelo tempo formam a sequ√™ncia
- Um cliente $(sid, s)$ suporta uma sequ√™ncia $\alpha$ se $\alpha \subseteq s$ para
- Assim, definimos o suporte de uma sequ√™ncia como
  - $sup(\alpha) = |\{(sid, s) | \alpha \subseteq s\}|$
- Exemplo:
  - $sup(\langle a \rangle) = 5$
  - $sup(\langle gb \rangle) = 2$
  - $sup(\langle l \rangle) = 4$

| **sid** | **s**                           |
| :-----: | :------------------------------ |
|    1    | $$\langle ac \rangle$$          |
|    2    | $$\langle (gm)a(bsl) \rangle$$  |
|    3    | $$\langle (ail) \rangle$$       |
|    4    | $$\langle a (bl) c \rangle$$    |
|    5    | $$\langle c \rangle$$           |
|    6    | $$\langle (ag) l (bl) \rangle$$ |

---

- Uma sequ√™ncia $\alpha$ √© frequente se $sup(\alpha) \geq minsup$
- Uma sequ√™ncia $\alpha$ tem tamanho $k$ (√© uma k-sequ√™ncia) se $\sum |a_i| = k$
- Uma sequ√™ncia frequente √© dita m√°xima se n√£o existe uma supersequ√™ncia pr√≥pria que seja frequente
- Ela √© fechada se n√£o existe uma supersequ√™ncia pr√≥pria com o mesmo suporte

## Aula 09 | 15/04/2025 | Minera√ß√£o de grafos

### Slide: aula06-sequencias (Aula 09)

#### Introdu√ß√£o (Aula 09)

- Nessa aula, vamos discutir o problema de minera√ß√£o de sequ√™ncias em bases de dados
- Esse problema ocorre com frequ√™ncia em diversas √°reas
  - Identificar trajet√≥rias dos alunos de computa√ß√£o
  - Identificar perfil (temporal) de compras dos clientes (celular $\to$ capa protetora $\to$ fone de ouvido)
  - Identificar padr√µes de genes e prote√≠nas no genoma
- Enquanto itemsets s√£o padr√µes intra-transa√ß√µes, aqui estamos buscando padr√µes inter-transa√ß√µes

---

- Para ilustrar, considere a seguinte base de dados
- Os clientes fazem diversas compras na loja
  - Mais de 1 item pode ser adquirido em uma transa√ß√£o
- Existe algum padr√£o de compras?
  - Determinados itens s√£o comprados em sequ√™ncia?
- Esse problema √© conhecido como **minera√ß√£o de sequ√™ncias (frequentes)**

| **ID Cliente** | **Data** | **Transa√ß√£o**         | **JV: Enxugado** |
| :------------- | :------: | :-------------------- | :--------------- |
| 1              | 25/06/19 | aveia                 | a                |
| 1              | 30/06/19 | castanha              | c                |
| 2              | 10/06/19 | granola, mel          | gm               |
| 2              | 15/06/19 | aveia                 | a                |
| 2              | 20/06/19 | banana, suco, leite   | bsl              |
| 3              | 25/06/19 | aveia, iogurte, leite | ail              |
| 4              | 25/06/19 | aveia                 | a                |
| 4              | 30/06/19 | banana, leite         | bl               |
| 4              | 25/07/19 | castanha              | c                |
| 5              | 12/06/19 | castanha              | c                |
| 6              | 10/06/19 | aveia, granola        | ag               |
| 6              | 11/06/19 | leite                 | l                |
| 6              | 17/06/19 | banana, leite         | bl               |

#### Defini√ß√µes (Aula 09)

- A base de dados √© um conjunto de transa√ß√µes consistindo em:
  - ID do cliente
  - Timestamp da transa√ß√£o
  - Itens 'comprados'
- Os itens da transa√ß√£o s√£o um itemset de uma cole√ß√£o de poss√≠veis itens
- Uma **sequ√™ncia** √© uma **lista ordenada de itemsets**
  - Os itemsets tamb√©m s√£o chamados de elementos
- Para um conjunto de itens $I$, uma sequ√™ncia $s = \langle s_1 s_2 \dots s_n \rangle$ em que cada $s_i \subseteq I$ √© um itemset
  - Por defini√ß√£o, um item n√£o pode aparecer mais de uma vez num itemset, mas pode aparecer v√°rias vezes numa sequ√™ncia

---

- Por exemplo, a sequ√™ncia $\langle (gm) a (bsl) \rangle$ representa a sequ√™ncia de compras do cliente 2 na base de dados anterior
  - Itemsets s√£o delimitados por par√™ntesis; itemsets unit√°rios s√£o representados sem par√™ntesis
- Uma sequ√™ncia $\alpha = \langle a_1 a_2 \dots a_n \rangle$ √© uma subsequ√™ncia de uma sequ√™ncia $\beta = \langle b_1 b_2 \dots b_m \rangle$, $\alpha \subseteq \beta$, se existe uma fun√ß√£o $\phi: [1:n] \to [1:m]$ tal que
  - $a_1 \subseteq b_{\phi(1)}$; e
  - $\forall i, j [i < j \to \phi(i) < \phi(j)]$
- As sequ√™ncias $\langle (gm) b \rangle$, $\langle mab \rangle$ e $\langle a \rangle$ s√£o subsequ√™ncias de $\langle (gm) a (bsl) \rangle$
- Note que a ordem √© definida somente entre elementos, e n√£o dentro dos itemsets

  - Contudo, vamos assumir que os elementos s√£o dispostos conforme alguma ordem dentro dos itemsets (em nosso caso, a ordem que forma apresentados na base original)

- [JV]
  - Tentando explicar sobre as subsequ√™ncias:

| Itemset $a$                    | Itemset $b$                    | $b$ √© subsequ√™ncia de $a$? |
| :----------------------------- | :----------------------------- | :------------------------: |
| $\langle (gm) a (bsl) \rangle$ | $\langle (gm) b \rangle$       |            Sim             |
| $\langle (mg) a (lsb) \rangle$ | $\langle (gm) a (bls) \rangle$ |            Sim             |
| $\langle (gm) a (bsl) \rangle$ | $\langle (gm) a \rangle$       |            Sim             |

...

---

- Podemos redefinir a base de dados como um conjunto de pares $(sid, s)$ em que sid √© um identificador de sequ√™ncia e s uma sequ√™ncia
  - Cada identificador de cliente √© um sid
  - As diferentes transa√ß√µes de um cliente ordenados pelo tempo formam a sequ√™ncia
- Um cliente $(sid, s)$ suporta uma sequ√™ncia $\alpha$ se $\alpha \subseteq s$ para
- Assim, definimos o suporte de uma sequ√™ncia como
  - $sup(\alpha) = |\{(sid, s) | \alpha \subseteq s\}|$
- Exemplo:
  - $sup(\langle a \rangle) = 5$
  - $sup(\langle gb \rangle) = 2$
  - $sup(\langle l \rangle) = 4$

| **sid** | **s**                           |
| :-----: | :------------------------------ |
|    1    | $$\langle ac \rangle$$          |
|    2    | $$\langle (gm)a(bsl) \rangle$$  |
|    3    | $$\langle (ail) \rangle$$       |
|    4    | $$\langle a (bl) c \rangle$$    |
|    5    | $$\langle c \rangle$$           |
|    6    | $$\langle (ag) l (bl) \rangle$$ |

- [JV]
  - Essa representa√ß√£o √© similar √† representa√ß√£o horizontal.
  - Nessa defini√ß√£o ignoramos o timestamp. Focamos apenas na sequ√™ncia cronol√≥gica.
  - Nesse caso o tempo apenas √© usado pra ordenar, mas n√£o √© considerado na sequ√™ncia.
  - Cobertura: todos os elementos do qual o itemset √© subsequ√™ncia.
  - O suporte √© o tamanho da cobertura.

---

- Uma sequ√™ncia $\alpha$ √© frequente se $sup(\alpha) \geq minsup$
- Uma sequ√™ncia $\alpha$ tem tamanho $k$ (√© uma k-sequ√™ncia) se $\sum |a_i| = k$
- Uma sequ√™ncia frequente √© dita m√°xima se n√£o existe uma supersequ√™ncia pr√≥pria que seja frequente
- Ela √© fechada se n√£o existe uma supersequ√™ncia pr√≥pria com o mesmo suporte

- [JV]
  - $\sum |a_i| = k$: conta o total de itens. Sem se preocupar com repeti√ß√£o ou n√£o.

#### Minera√ß√£o de sequ√™ncias frequentes - (Aula 09)

- O problema de minera√ß√£o de sequ√™ncias frequentes consiste em encontrar todas as sequ√™ncias cujo suporte esteja acima de um limiar m√≠nimo definido pelo usu√°rio
- Existem abordagens tanto para minerar todo o conjunto de sequ√™ncias frequentes quanto para representa√ß√µes compactas desse conjunto
- Nessa aula veremos apenas as abordagens para minerar todo o conjunto de sequ√™ncias frequentes
- Essas abordagens s√£o extens√µes dos principais algoritmos para minera√ß√£o de conjuntos de itens frequentes

- [JV]
  - N√£o veremos as maximais e fechadas das sequ√™ncias. Nem as formas densas, embora existam.

#### Generalized Sequential Patterns (GSP)

- O GSP √© um algoritmo baseado no Apriori para minerar sequ√™ncias frequentes
- Ele tamb√©m foi proposto por Agrawal e Srikant em 1996
- Por ser baseado no Apriori, o algoritmo adota a estrat√©gia de busca em largura
- O algoritmo usa sequ√™ncias frequentes de tamanho $k-1$ para gerar candidatas de tamanho $k$ e avaliar o suporte,
- Ele tamb√©m emprega a propriedade de antimonotonicidade do suporte para podar o espa√ßo de busca

- [JV]
  - Se uma sequ√™ncia √© infrequente, qualquer supersequ√™ncia dela tamb√©m ser√° infrequente.

---

- Assim como o Apriori, o algoritmo faz diversas passadas sobre a base de dados
- Na primeira passada, o algoritmo verifica o suporte de cada item
  - Os itens individualmente s√£o sequ√™ncias simples de tamanho 1
- Pela propriedade do Apriori, somente os itens frequentes s√£o mantidos e servem de base para a gera√ß√£o dos candidatos de tamanho 2
- Cada par $\langle x \rangle$ e $\langle y \rangle$ de sequ√™ncias de tamanho 1 d√° origem a duas sequ√™ncias de tamanho 2
  - $\langle xy \rangle$
  - $\langle (xy) \rangle$
- A exce√ß√£o se d√° quando $x=y$, nesse caso somente a primeira √© gerada
- Logo, o conjunto de candidatos de tamanho 2 √©:

  - $C^{(2)} = \{ \langle xy \rangle | (x, y) \in F^{(1)} \times F^{(1)} \} \cup \{ \langle (xy) \rangle | (x, y) \in F^{(1)} \times F^{(1)} \wedge x \neq y\}$

- [JV]
  - Primeiro calcula o suporte de cada item individual.
  - Depois, √† partir de cada sequ√™ncia, geram-se tr√™s candidatos
    - Ex: $\langle a \rangle$ e $\langle b \rangle$ geram:
      - $\langle ab \rangle$, $\langle ba \rangle$, $\langle (ab) \rangle$.
    - Ou dois candidatos, se desconsiderarmos que $\langle ba \rangle$ ser√° gerado em outra itera√ß√£o.
  - Todos de tamanho dois n√£o verificamos a propriedade apriori.

---

- Os candidatos que possuam subsequ√™ncias de tamanho $k-1$ infrequentes s√£o removidos do conjunto
- Ent√£o, o suporte dos candidatos √© computado com uma passada na base, e os infrequentes s√£o descartados
- A partir de $k=3$, os candidatos s√£o gerados da seguinte forma:
  - Sejam $s_1$ e $s_2$ duas sequ√™ncias frequentes de tamanho $k-1$ tais que ambas sejam id√™nticas ap√≥s a remo√ß√£o do primeiro item de $s_1$ e do √∫ltimo item de $s_2$
  - As sequ√™ncias s√£o unidas para gerar uma candidata de tamanha k
    - A nova candidata ser√° a sequ√™ncia $s_1$ estendida com o √∫ltimo item de $s_2$
  - O √∫ltimo item ser√° um elemento separado se ele era um elemento separado em $s_2$, ou ser√° agregado ao √∫ltimo elemento de $s_1$ caso contr√°rio
- O algoritmo repete o processo enquanto houverem candidatos no pr√≥ximo n√≠vel

- [JV]
  - Esse daqui foi muito confuso
  - ga+(am) = g(am)
  - ga+am = gam
  - (ga)+am = (ga)m
  - (ga)+(am) = (gam)
  - aba+bab = abab
  - bab+aba = baba

#### Sequential Pattern Discovery using Equivalence classes (Spade)

- Outra abordagem para minera√ß√£o de sequ√™ncias frequentes foi proposta por Zaki em 2001
- O algoritmo Spade √© baseado no Eclat
- Assim como o Eclat, ele utiliza uma representa√ß√£o da base de dados similar √† vertical e divide o espa√ßo de busca de acordo com o prefixo das sequ√™ncias

---

- Para obter a representa√ß√£o vertical, vamos associar a cada item $i$ uma tupla $(sid, pos(i))$
  - $pos(i)$ √© a lista de todos os elementos em que $i$ ocorre na sequ√™ncia referente ao cliente $sid$
- A lista de todos os pares sequ√™ncia-posi√ß√£o de um item √© chamada de **poslist** e √© denotada por $\mathcal{L}(i)$
- Exemplos:
  - $\mathcal{L}(l) = \{ (2, \{3\}), (3, \{1\}), (4, \{2\}), (6, \{2, 3\} ) \}$
  - $\mathcal{L}(g) = \{ (2, \{1\}), (6, \{1\}) \}$
- A representa√ß√£o vertical da base pode ser obtida pelas poslists de todos os itens
  - Note que $sup(i) = |\mathcal{L}(i)|$

| **sid** | **s**                           |
| :------ | :------------------------------ |
| 1       | $$\langle ac \rangle$$          |
| 2       | $$\langle (gm)a(bsl) \rangle$$  |
| 3       | $$\langle (ail) \rangle$$       |
| 4       | $$\langle a (bl) c \rangle$$    |
| 5       | $$\langle c \rangle$$           |
| 6       | $$\langle (ag) l (bl) \rangle$$ |

| **item** | **pos(item)**                         |
| :------- | :------------------------------------ |
| a        | { (1,1), (2,2), (3,1), (4,1), (6,1) } |
| b        | { (2,3), (4,2), (6,3) }               |
| c        | { (1,2), (4,3), (5,1) }               |
| g        | { (2,1), (6,1) }                      |
| i        | { (3,1) }                             |
| l        | { (2,3), (3,1), (4,2), (6,23) }       |
| s        | { (2,3) }                             |

- [JV]
  - No caso do l, o 23, na verdade √© uma lista $[2, 3]$

---

- Zaki demonstrou que novas sequ√™ncias podem ser geradas a partir da **jun√ß√£o temporal** de duas sequ√™ncias que perten√ßam a uma mesma classe de equival√™ncia (compartilham um prefixo de tamanho $k-1$)
- O suporte pode ser computado pela interse√ß√£o das poslists
- Supondo que as sequ√™ncias a serem unidas compartilham um prefixo P, tr√™s situa√ß√µes podem ocorrer:
- Juntar duas sequ√™ncias (Px) e (Py): resulta em (Pxy)
- Juntar duas sequ√™ncias (Px) e Py: resulta em (Px)y
- Juntar duas sequ√™ncias Px e Py: pode resultar em Pxy, Pyx e P(xy) dependendo da ordem temporal de x e y; um caso particular ocorre quando x=y, nesse caso, a jun√ß√£o s√≥ pode resultar em Pxx

---

- Juntar duas sequ√™ncias (Px) e (Py): resulta em (Pxy)
  - Sejam $\mathcal{L}(Px)$ e $\mathcal{L}(Py)$ as poslists de (Px) e (Py). A poslist de (Pxy) √© gerada da seguinte forma: $\mathcal{L}((Pxy)) = \left{ \left( i, pos((Px)) \cap pos((Py)) \right) | \left( i, pos((Px)) \right) \in \mathcal{L}((Px)) \wedge \left( i, pos((Py)) \right) \in \mathcal{L}((Py)) \wedge pos((Px)) \cap pos((Py)) \neq \emptyset \right}$
  - Ou seja, √© o conjunto de sequ√™ncia-posi√ß√µes em que ambos ocorrem ao mesmo tempo
  - O caso P(xy) √© an√°logo a esse
- Juntar duas sequ√™ncias (Px) e Py: resulta em (Px)y

  - $\mathcal{L}((Px)y) = \left{ \left( i, \left{ v \in pos(Py) | \exists u \in pos((Px)) u < v \wedge \left( i, pos(Py) \right) \in \mathcal{L}(Py) \wedge \left( i, pos((Px)) \right) \in \mathcal{L}((PX)) \right} \right) \right}$;
  - Ou seja, s√£o todas as sequ√™ncias em que ambas acontecem, por√©m agora somente as posi√ß√µes em que $y$ ocorre temporalmente ap√≥s $x$ s√£o mantidas
  - O caso √© equivalente a Pxy e Pyx

- [JV]
  - Agora veremos os dois prefixos. E apenas consideraremos o √∫ltimo item. N√£o a √∫ltima transa√ß√£o.
  - Se considerarmos Py, onde P ocorreu na posi√ß√£o 1, e y est√° na posi√ß√£o 6;
  - E temos Px, onde P ocorreu na posi√ß√£o 2, e x est√° na posi√ß√£o 2;
  - Se gerarmos Pyx, ter√≠amos que o y est√° antes do x, mas temporalmente deveria estar depois. Logo, consideramos que ele ser√° infrequente, podendo ent√£o podar.

## Aula 10 | 17/04/2025 | Minera√ß√£o de grafos

### Slide: aula06-sequencias (Aula 10)

#### Sequential Pattern Discovery using Equivalence classes (Spade) (Aula 10)

- Para obter a representa√ß√£o vertical, vamos associar a cada item $i$ uma tupla $(sid, pos(i))$
  - $pos(i)$ √© a lista de todos os elementos em que $i$ ocorre na sequ√™ncia referente ao cliente $sid$
- A lista de todos os pares sequ√™ncia-posi√ß√£o de um item √© chamada de **poslist** e √© denotada por $\mathcal{L}(i)$
- Exemplos:
  - $\mathcal{L}(l) = \{ (2, \{3\}), (3, \{1\}), (4, \{2\}), (6, \{2, 3\} ) \}$
  - $\mathcal{L}(g) = \{ (2, \{1\}), (6, \{1\}) \}$
- A representa√ß√£o vertical da base pode ser obtida pelas poslists de todos os itens
  - Note que $sup(i) = |\mathcal{L}(i)|$

| **sid** | **s**                           |
| :------ | :------------------------------ |
| 1       | $$\langle ac \rangle$$          |
| 2       | $$\langle (gm)a(bsl) \rangle$$  |
| 3       | $$\langle (ail) \rangle$$       |
| 4       | $$\langle a (bl) c \rangle$$    |
| 5       | $$\langle c \rangle$$           |
| 6       | $$\langle (ag) l (bl) \rangle$$ |

| **item** | **pos(item)**                             |
| :------- | :---------------------------------------- |
| a        | $\{ (1,1), (2,2), (3,1), (4,1), (6,1) \}$ |
| b        | $\{ (2,3), (4,2), (6,3) \}$               |
| c        | $\{ (1,2), (4,3), (5,1) \}$               |
| g        | $\{ (2,1), (6,1) \}$                      |
| i        | $\{ (3,1) \}$                             |
| l        | $\{ (2,3), (3,1), (4,2), (6,23) \}$       |
| s        | $\{ (2,3) \}$                             |

- [JV Anota√ß√£o no quadro]
  - $ms = minsup = 3$
  - itens com suporte m√≠nimo: $\{a, b, c, l\}$
  - Checa com o
  - $P_a:$
    - Verifica naquela lista ali de tr√°s de $pos(item)$, e vai verificando se os itens aparecem nessa ordem para os dois itens nas transa√ß√µes de um mesmo cliente.
    - $\emptyset a - \emptyset a:$ aa | ~~(aa)~~ (Como os dois s√£o o mesmo item, √© descartado)
      - $aa:$ Infrequente
    - $\emptyset a - \emptyset b: ab | (ab)$
      - $ab: pos = \{ (2,3), (4,2), (6,3) \}$
      - $(ab):$ Infrequente
    - $\emptyset a - \emptyset c: ac | (ac)$
      - $ac: pos = \{ (1,2), (4,3) \}$ Infrequente
      - $(ac):$ Infrequente
    - $\emptyset a - \emptyset l: al|(al)$
      - $al: pos = \{ (2,3), (4,2), (6, 23) \}$
      - $(al):$ Infrequente
  - Ent√£o, os frequentes s√£o: $\{ ab, al\}$
  - $P_{ab}$
    - $ab - ab: abb$
    - $ab - al: abl|a(bl)$
      - $a(bl): pos = (2,3), (4,2), (6,3) \}$
  - Ent√£o os frequentes s√£o: $\{ a(bl) \}$
  - $P_{a(bl)}$
    - $a(bl) - a(bl): a(bl)l$ Isso daqui eu n√£o entendi.
  -

---

- Zaki demonstrou que novas sequ√™ncias podem ser geradas a partir da **jun√ß√£o temporal** de duas sequ√™ncias que perten√ßam a uma mesma classe de equival√™ncia (compartilham um prefixo de tamanho $k-1$)
- O suporte pode ser computado pela interse√ß√£o das poslists
- Supondo que as sequ√™ncias a serem unidas compartilham um prefixo P, tr√™s situa√ß√µes podem ocorrer:
- Juntar duas sequ√™ncias (Px) e (Py): resulta em (Pxy)
- Juntar duas sequ√™ncias (Px) e Py: resulta em (Px)y
- Juntar duas sequ√™ncias Px e Py: pode resultar em Pxy, Pyx e P(xy) dependendo da ordem temporal de x e y; um caso particular ocorre quando x=y, nesse caso, a jun√ß√£o s√≥ pode resultar em Pxx

---

- Juntar duas sequ√™ncias (Px) e (Py): resulta em (Pxy)
  - Sejam $\mathcal{L}(Px)$ e $\mathcal{L}(Py)$ as poslists de (Px) e (Py). A poslist de (Pxy) √© gerada da seguinte forma: $\mathcal{L}((Pxy)) = \left{ \left( i, pos((Px)) \cap pos((Py)) \right) | \left( i, pos((Px)) \right) \in \mathcal{L}((Px)) \wedge \left( i, pos((Py)) \right) \in \mathcal{L}((Py)) \wedge pos((Px)) \cap pos((Py)) \neq \emptyset \right}$
  - Ou seja, √© o conjunto de sequ√™ncia-posi√ß√µes em que ambos ocorrem ao mesmo tempo
  - O caso P(xy) √© an√°logo a esse
- Juntar duas sequ√™ncias (Px) e Py: resulta em (Px)y

  - $\mathcal{L}((Px)y) = \left{ \left( i, \left{ v \in pos(Py) | \exists u \in pos((Px)) u < v \wedge \left( i, pos(Py) \right) \in \mathcal{L}(Py) \wedge \left( i, pos((Px)) \right) \in \mathcal{L}((PX)) \right} \right) \right}$;
  - Ou seja, s√£o todas as sequ√™ncias em que ambas acontecem, por√©m agora somente as posi√ß√µes em que $y$ ocorre temporalmente ap√≥s $x$ s√£o mantidas
  - O caso √© equivalente a Pxy e Pyx

- [JV]
  - Agora veremos os dois prefixos. E apenas consideraremos o √∫ltimo item. N√£o a √∫ltima transa√ß√£o.
  - Se considerarmos Py, onde P ocorreu na posi√ß√£o 1, e y est√° na posi√ß√£o 6;
  - E temos Px, onde P ocorreu na posi√ß√£o 2, e x est√° na posi√ß√£o 2;
  - Se gerarmos Pyx, ter√≠amos que o y est√° antes do x, mas temporalmente deveria estar depois. Logo, consideramos que ele ser√° infrequente, podendo ent√£o podar.

---

- Sejam as sequ√™ncias $\langle gb \rangle$ e $\langle gl \rangle$ pertencentes √† classe de equival√™ncia de $g$, e suas respectivas poslists $\mathcal{L}(gb) = \{ (2,3), (6,3) \}$ e $\mathcal{L}(gl) = \{ (2,3), (6,3) \}$
  - $\mathcal{L}(g(bl)) = \{ (2,3), (6,3) \}$
  - $\mathcal{L}(gbl) = \emptyset$
  - $\mathcal{L}(glb) = \emptyset$
- O algoritmo segue explorando o espa√ßo de busca enquanto as classes de equival√™ncia n√£o forem vazias

---

- **ALGORITHM 10.2. Algorithm SPADE**
  - `// Initial Call:` $\mathcal{F} \leftarrow \emptyset, k \leftarrow 0, P \leftarrow \left{ \langle s, \mathcal{L}(s) \rangle | s \in \sum, sup(s) \geq minsup \right}$
  - **SPADE** $(P, minsup, \mathcal{F}, k)$
    - **foreach** $r_a \in P$ **do**
      - $\mathcal{F} \leftarrow \mathcal{F} \cup \left{ (r_a, sup(r_a)) \right}$
      - $P_a \leftarrow \emptyset$
      - **foreach** $r_b \in P$ **do**
        - $r_{ab} = r_a + r_b$
        - $\mathcal{L}(r_{ab}) = \mathcal{L}(r_a) \cap \mathcal{L}(r_b)$
        - **if** $sup(r_{ab}) \geq minsup$ **then**
          - $P_a \leftarrow P_a \cup \left{ \langle r_{ab}, \mathcal{L}(r_{ab}) \rangle \right}$
      - **if** $P_a \neq \emptyset$ **then** SPADE $(P, minsup, \mathcal{F}, k+1)$

#### Leitura (Aula 10)

- Se√ß√µes 10.1 e 10.2 Zaki e Meira
- Cap√≠tulo 11 Aggarwal e Han
- [Link][Link_2001] Zaki, M.J. SPADE: An Efficient Algorithm for Mining Frequent Sequences. Machine Learning 42, 31‚Äì60 (2001).
- [Link][Link_1996] Srikant R., Agrawal R. (1996) Mining sequential patterns: Generalizations and performance improvements. In: Apers P., Bouzeghoub M., Gardarin G. (eds) Advances in Database Technology ‚Äî EDBT '96. EDBT 1996. Lecture Notes in Computer Science, vol 1057. Springer, Berlin, Heidelberg.

[Link_2001]: https://doi.org/10.1023/A:1007652502315
[Link_1996]: https://doi.org/10.1007/BFb0014140

### Slide: aula07-grafos (Aula 10)

#### Introdu√ß√£o - Aula 10

- Nessa aula, vamos discutir o problema de minera√ß√£o de subgrafos frequentes em bases de dadASDos de grafos.
- A minera√ß√£o de subgrafos frequentes tem ganhado destaque nos √∫ltimos anos com aplica√ß√µes em diversas √°reas:
  - Quimioinform√°tica: descoberta de princ√≠pios ativos de f√°rmacos
  - Bioinform√°tica: padr√µes de intera√ß√£o em redes de prote√≠nas
  - Engenharia de Software: an√°lise do fluxo de controle de aplica√ß√µes

---

- Considerando o contexto de quimioinform√°tica, temos o seguinte exemplo:

Cafe√≠na, Teobromina (chocolate), Teofilina (tratamento de asma), Sildenafil (Viagra)

(Imagens de estruturas qu√≠micas omitidas)

#### Defini√ß√µes (Aula 10)

- Um grafo √© uma qu√°drupla $(V, E, l, L)$ em que:

  - $V$ √© um conjunto de v√©rtices;
  - $E$ √© um conjunto de arestas;
  - $l: V \rightarrow \Sigma_v$ √© uma fun√ß√£o que determina o r√≥tulo dos v√©rtices;
  - $L: E \rightarrow \Sigma_e$ √© uma fun√ß√£o que determina o r√≥tulo das arestas.

- [JV]
  - Grafo Rotulado/Grafo com atributos
  - Aqui consideramos que esses r√≥tulos s√£o strings

(Imagem de um grafo)

---

- Sejam $G_1$ e $G_2$ dois grafos rotulados com os mesmos alfabetos $\Sigma_v$ e $\Sigma_e$.
- Dizemos que $G_1$ √© subgrafo isomorfo a $G_2$, representado por $G_1 \subseteq G_2$, se existe uma fun√ß√£o injetora $\upvarphi: V_1 \rightarrow V_2$ tal que:
  - $(u, v) \in E_1 \Leftrightarrow (\upvarphi(u), \upvarphi(v)) \in E_2$
  - $\forall u \in V_1,\ l(u) = l(\upvarphi(u))$
  - $\forall (u, v) \in E_1,\ L(u,v) = L(\upvarphi(u), \upvarphi(v))$
- [JV]
  - √â isomorfo se eu consigo mapear tanto os v√©rtices quanto os r√≥tulos.

(Tr√™s imagens de grafos: $G_1$, $G_3$ e $G_4$)

Nessa imagem, o $G_3$ √© subgrafo isomorfo de $G_1$, mas $G_4$ n√£o.

---

- Uma base de dados $D = \{G_1, G_2, \dots, G_n\}$ √© um conjunto de grafos rotulados.
- O suporte de um grafo (padr√£o) $G$ √© o n√∫mero de grafos em $D$ em que $G$ est√° contido:

  - $\text{sup}(G) = |\{G_i \in D\ |\ G \subseteq G_i\}|$

- Um padr√£o $G$ √© frequente se $\text{sup}(G) \geq minsup$

(Imagem de v√°rios subgrafos)

- [JV]
  - Esse conceito de conjunto de grafos rotulados t√° mais pr√≥ximo do problema de mol√©culas.
  - No caso de redes sociais como Facebook e Instagram, √© mais parecido com um grafo monolit√£o.
  - O Suporte √© calculado pegando um subgrafo e checando se ele encaixaria em um dos grafos. Ou algo assim.

---

- O problema de minera√ß√£o de subgrafos frequentes consiste em encontrar todos os **subgrafos conexos** que satisfa√ßam o suporte m√≠nimo definido pelo usu√°rio.
- Comparado aos problemas vistos at√© agora, esse √© o mais dif√≠cil em termos computacionais.
  - O problema de isomorfismo √© NP-dif√≠cil.
- Um grafo com $m$ v√©rtices possui $O(m^2)$ arestas.
- O n√∫mero de subgrafos com $m$ v√©rtices √©, portanto, $2^{O(m^2)}$.
- Considerando grafos rotulados com $|\Sigma_v| = |\Sigma_e| = s$, $s^m$ rotula√ß√µes diferentes de v√©rtices e $s^{m^2}$ rotula√ß√µes de arestas.
- Logo, o espa√ßo de busca do problema √© $2^{O(m^2)} \left(s^{O(m)}\right) \left(s^{O(m^2)}\right)$.

- [JV]
  - Ele colocou a mesma quantidade de rotula√ß√µes de arestas e v√©rtices com o mesmo valor s√≥ pra simplificar.
  - Tem gente que vai pra minera√ß√£o de grafos distribu√≠dos.

## Aula 11 | 22/04/2025 | Regras de associa√ß√£o e m√©tricas de qualidade

### Slide: aula07-grafos (Aula 11)

#### Minera√ß√£o de subgrafos frequentes

- Assim como aconteceu com os problemas anteriores, existem duas categorias de algoritmos:
  - Baseados no 'Apriori' (gera√ß√£o de candidatos)
  - Baseados no 'FP-Growth' (crescimento de padr√µes)
- Os algoritmos baseados no Apriori se subdividem em mais grupos:
  - Os baseados no aumento do n√∫mero de v√©rtices; e
  - Os baseados no aumento do n√∫mero de arestas.
- Os baseados em crescimento de padr√µes frequentemente utilizam extens√µes de arestas para cresc√™-los.

- [JV]
  - O de V√©rtices tende a ser pior que o de arestas.

##### Gera√ß√£o de candidatos (AGM)

- A gera√ß√£o de candidatos com aumento do n√∫mero de v√©rtices √© usada no algoritmo AGM (Apriori-based Graph Mining) proposto por Inokuchi et al. em 2000
- A ideia √© juntar dois grafos com k v√©rtices que compartilhem um n√∫cleo com k-1 v√©rtices para formar um candidato com k+1 v√©rtices

- [JV]
  - √â calculado calcular suporte por ter que procurar isomorfismo de grafos.
    - Automorfismo √© mais f√°cil que isomorfismo
  - Prioridade do apriori: antimonotonicidade
    - Se um grafo √© infrequente, qualquer supergrafo dele tamb√©m ser√° infrequente.
  - Algoritmo
    - Buscar o isomorfismo entre os dois grafos
      - Pega a matriz de adjac√™ncia de um grafo e compara com o outro
      - Caso removidas as colunas (i,i), ela n√£o resulte numa matriz de adjac√™ncia isomorfa, entre ambos, ent√£o tente todas as remanescentes permuta√ß√µes
      - Se for isomorfo
        - remove as √∫ltimas linhas e colunas das duas, e cria um novo candidato misturando a linha e coluna do primeiro com a linha e coluna do segundo.
          - Por√©m, n√£o se sabe qual o r√≥tulo existente, ou n√£o, entre os dois v√©rtices adicionados.
          - Ent√£o, criam-se candidatos com todos os poss√≠veis r√≥tulos, inclusive a inexist√™ncia de r√≥tulo.
      - Comparam-se tamb√©m os r√≥tulos, mas isso n√£o ficou t√£o claro.
    - Pra calcular o suporte, gerando a sub-estrutura, confere se ela √© subgrafo de outra
  - N√£o faz sentido falar de ordem lexicogr√°fica de strings
    - Representa√ß√£o can√¥nica de uma matriz de adjac√™ncia
    - Tipo uma identidade, um hash

##### Gera√ß√£o de candidatos (FSG)

- A gera√ß√£o de candidatos com aumento no n√∫mero de arestas √© usada pelo algoritmo FSG, proposto por Kuramochi e Karypis em 2001.
- Dois padr√µes com $k$ arestas s√£o juntados se eles compartilham um n√∫cleo (possuem um mesmo subgrafo) com $k-1$ arestas.
  - Ou seja, $G_1$ e $G_2$ s√£o juntados se existe uma aresta em $G_1$ e uma aresta em $G_2$ tais que a remo√ß√£o das duas torna os subgrafos isomorfos entre si.
  - Consequentemente, o candidato ter√° as arestas de $G_1$ mais essa aresta de $G_2$.
- Ao contr√°rio do que ocorre com o m√©todo baseado em v√©rtices, aqui o candidato resultante pode n√£o ter um n√∫mero maior de v√©rtices que os padr√µes do qual foi gerado

- [JV]
  - FSG - Frequent Sub Graph (?)

---

- Um conceito importante para a gera√ß√£o de candidatos √© o de v√©rtices topologicamente equivalentes
  - [JV]
    - Topologicamente equivalentes:
      - Grafos que mant√©m a mesma estrutura geral.
      - Pra comparar, pode-se comparar entre possibilidades de novas arestas a novos v√©rtices e/ou a arestas existentes.
      - Lembrando tamb√©m da compara√ß√£o dos r√≥tulos.
      - √â uma equival√™ncia estrutural entre v√©rtices de dois grafos distintos.
      - √â isomorfo se consegue-se fazer um mapeamento 1:1 entre os v√©rtices e arestas de um pro outro.
- Considere os grafos abaixo

(Imagens de grafos)

---

- Para entender o processo de gera√ß√£o de candidatos, vamos considerar dois grafos gen√©ricos $G_1$ e $G_2$ com $k$ arestas que compartilham um n√∫cleo comum com $k-1$ arestas
  - Na figura, o n√∫cleo √© omitido e somente as arestas n√£o compartilhadas s√£o ilustradas; os v√©rtices dentro da caixa fazem parte do n√∫cleo.
  - Se $a$ √© topologicamente equivalente a $c$, dizemos que $a=c$
  - Se $b$ e $d$ possuem o mesmo r√≥tulo, dizemos que $b=d$
  - [JV]
    - Nesse caso est√£o sendo definidos dois tipos de igualdades que ser√£o representados com o mesmo s√≠mbolo de igualdade.
- Temos 4 poss√≠veis situa√ß√µes
  - [JV]
    - Quando definimos esse n√∫cleo, √© como se as arestas que saem dele v√£o para os v√©rtices externos fossem perdidas.

| X          | $a=c$ | $a \neq c$ |
| ---------- | :---: | :--------: |
| $b=d$      |   1   |     4      |
| $b \neq d$ |   2   |     3      |

---

- Situa√ß√£o 1 [$a \neq c$ e $b \neq d$]: Nesse caso, somente um candidato pode ser gerado, j√° que a 'nova' aresta incide em v√©rtices distintos
  - [JV]
    - Obs.:
      - A imagem dele √© meio confusa. G1 e G2 s√£o os grafos inteiros. O quadradinho apenas representa o n√∫cleo desses grafos. O que eu havia entendido antes e est√° errado √© que o n√∫cleo se chamava G1 e o n√∫cleo se chamava G2.
      - Outro detalhe, os v√©rtices $a$ e $c$ s√£o v√©rtices contidos no n√∫cleo de um grafo grafo omitido;

---

- Situa√ß√£o 2 [$a=c$ e $b \neq d$]: Nesse caso, temos duas possibilidades:
  - A 'nova' aresta incide sobre v√©rtices distintos como na situa√ß√£o 1
  - A 'nova' aresta incide sobre o v√©rtice equivalente a $a$ (ou $c$)
  - [JV]
    - Uma an√°lise interessante √© observarmos que se apenas houver um mesmo v√©rtice $a$ que seja topologicamente equivalente a $c$, ent√£o, apenas o segundo caso ocorre, visto que n√£o h√° um segundo v√©rtice para se anexar a aresta.

---

- Situa√ß√£o 3 [$a \neq c$ e $b=d$]: Nesse caso, temos duas possibilidades:
  - A 'nova' aresta incide sobre v√©rtices distintos como na situa√ß√£o 1
  - A 'nova' aresta incide sobre o v√©rtice equivalente a $b$ (ou $d$)

---

- Situa√ß√£o 4 [$a=c$ e $b=d$]: Nesse caso, qualquer das possibilidades anteriores pode ocorrer; ou seja, pelo menos 3 candidatos s√£o gerados.

- [JV]
  - Foi comentado sobre o "pelo menos 3 candidatos". Ele disse n√£o saber porque escreveu assim, e considera que ser√£o sempre 3, ou talvez, "no m√°ximo 3".

---

- Al√©m dessas situa√ß√µes, dois grafos podem compartilhar m√∫ltiplos n√∫cleos. Cada um deles pode gerar candidatos distintos.

  - Podem existir at√© $k-1$ n√∫cleos distintos para dois padr√µes de tamanho $k$.

[Imagens de grafos]

---

- Naturalmente, muitos desses candidatos ser√£o isomorfos entre si, devendo, portanto, serem descartados exceto uma c√≥pia
- O controle de isomorfismo √© feito atribuindo-se um c√≥digo (string) a partir de cada matriz de adjac√™ncias
- Grafos com mesmo c√≥digo s√£o isomorfos
- Os detalhes s√£o omitidos, mas podem ser consultados no artigo original

- [JV]
  - Gera√ß√£o de vers√£o can√¥nica de string da matriz de adjac√™ncia. Feito na for√ßa bruta.
    - Da matriz triangular superior, coluna a coluna, lineariza-se todos os r√≥tulos.
    - Gera-se todas as as permuta√ß√µes dessa string e a mais lexicograficamente mais ordenada, √© a vers√£o can√¥nica.

## Aula 12 | 24/04/2025 | Aprendizado descritivo supervisionado: padr√µes emergentes, contrastantes e descoberta de subgrupos

### Slide: aula07-grafos (Aula 12)

#### Minera√ß√£o de subgrafos frequentes (Aula 12)

##### Crescimento de padr√µes (gSpan)

- De maneira similar ao que acontece com itemsets, os algoritmos baseados em gera√ß√£o de candidatos possuem desempenho inferior aos baseados em crescimento de padr√µes, j√° que muitos candidatos s√£o podados
- Em 2002, Yan e Han (FP-Growth) propuseram o algoritmo gSpan (Graph-based Substructure Pattern Mining)
- O algoritmo estende os padr√µes acrescentando arestas, uma por vez, de forma similar ao comportamento do FP-Growth
- Os autores tamb√©m propuseram representar grafos por sequ√™ncias de strings
  - Isso, de certa forma, mapeia o problema de minera√ß√£o de grafos para minera√ß√£o de sequ√™ncias; levando ao aproveitamento de ideias propostas na √∫ltima
- Cada aresta do grafo √© representada pela string:

  - $\langle v_i, v_j, l(v_i), l(v_j), L(v_i, v_j) \rangle$
    - [JV]
      - $v_i$ e $v_j$ s√£o os v√©rtices que comp√µem a aresta
      - $l(v_i)$ e $l(v_j)$ s√£o os r√≥tulos dos v√©rtices
      - $L(v_i, v_j)$ √© o r√≥tulo da aresta

- [JV]
  - Come√ßa com um grafo vazio e vai aumentando o grafo.
  - Projeta uma Base
  - Analisa todas as arestas poss√≠veis para expandir

---

- A extens√£o dos padr√µes proposta por Yan e Han se baseia na √°rvore geradora da busca em profundidade
  - [JV] Baseado na ordem em que encontrou.
- Os v√©rtices do grafo s√£o ordenados conforme a ordem de visita√ß√£o de uma busca em profundidade no grafo
  - V√©rtices visitados no in√≠cio s√£o menores que os visitados no fim
  - Subscritos dos v√©rtices indicam a ordem de visita√ß√£o
- O caminho mais curto entre o primeiro v√©rtice e o √∫ltimo √© chamado de **caminho mais √† direita (rightmost path)**
- Considerando a busca em profundidade, as arestas s√£o divididas em dois conjuntos:
  - **Fordward edges**: arestas que fazem parte da √°rvore geradora da DFS
  - **Backward edges**: demais arestas
    - [JV]
      - Arestas de retorno que v√£o para aresta j√° visitada antes.
      - $\langle v_i, v_j, \dots \rangle$ √© uma aresta de retorno se $v_i < v_j$, ou seja, $v_i$ foi visitado antes de $v_j$ na DFS

---

- As arestas podem ser ordenadas dentro dos respectivos conjuntos da seguinte forma:
  - **Forward edges**: $e_1 = \left( v_{i_1}, v_{j_1} \right)$, $e_2 = \left( v_{i_2}, v_{j_2} \right)$, $e_1 \preceq_F e_2 \leftrightarrow j_1 < j_2$
    - [JV]
      - $j_1$ e $j_2$ representam a sequ√™ncia de visita√ß√£o dos v√©rtices.
      - $\preceq_F$: √© definida a ordem de preced√™ncia entre arestas de avan√ßo.
  - **Backward edges**: $e_1 \preceq_B e_2 \leftrightarrow [i_1 < i_2 \lor (i_1 = i_2 \land j_1 < j_2)]$
    - [JV]
      - "Uma aresta (2, 0) vem antes de (3, 1)"; (3, 1) vem antes de (3, 2)
      - $\preceq_B$: √© definida a ordem de preced√™ncia entre arestas de retorno.
  - [JV]
    - resumindo essa quest√£o de ordena√ß√£o de arestas: Estamos rodando a DFS e depois ~~ordenando lexicograficamente, primeiro pela origem, depois pela chegada.~~
    - Esse entendimento superior √© falso. Na verdade, estamos ordenando justamente a sequ√™ncia das arestas visitadas. E a sequ√™ncia de compara√ß√µes √© bem esquisita mesmo.
- Adicionalmente comparamos arestas dos conjuntos da seguinte forma:
  - $e_1 \preceq_{BF} e_2$ sse:
    - $e_1$ √© uma aresta de retorno; $e_2$ √© uma aresta de √°rvore; e $i_1 < j_2$
    - $e_1$ √© uma aresta de √°rvore; $e_2$ √© uma aresta de retorno; e $j_1 \leq i_2$
- Combinando a ordem dos v√©rtices, com a ordem sobre as arestas e a ordem lexicogr√°fica definida sobre os r√≥tulos (de v√©rtices e arestas), definimos uma ordem total sobre as arestas (c√≥digos)
- Dessa forma, os grafos podem ser representados como sequ√™ncias de strings (c√≥digos) referentes √†s suas arestas

---

```mermaid
flowchart LR
  A
```

- [JV]

  - Arestas:

    - $e_x = \langle v_i, v_j, l(v_i), l(v_j), L(v_i, v_j) \rangle$
    - $e_1 = \langle 0, 1, a, a, q \rangle$
    - $e_2 = \langle 1, 2, a, a, r \rangle$
    - $e_3 = \langle 2, 0, a, a, r \rangle$
    - $e_4 = \langle 1, 3, a, b, r \rangle$
    - Ordem: $e_1 \preceq e_2 \preceq e_3 \preceq e_4$

  - Uma compara√ß√£o que causou muita d√∫vida, foi:
    - $e_4 = \langle 1, 3, a, b, r \rangle$
    - $e_4 = \langle 0, 3, a, b, r \rangle$
    - Entre elas, qual √© menor?
    - Resposta: a primeira

```mermaid
flowchart LR
  0 --- 1
  1 --- 2
  1 --- 3
  0 --- 3
```

- Exemplo:
  - Retirado da Figura 11.6 Zaki e Meira

(Imagem de grafos com arestas rotuladas)

- [JV] Exerc√≠cio: fazer as representa√ß√µes de arestas de todos os 3 grafos e ver que o G1 de fato √© menor.

---

- Note que diferentes buscas geram diferentes representa√ß√µes
- Podemos estender a ordem lexicogr√°fica das arestas para as representa√ß√µes (grafos)
- Chamamos de **representa√ß√£o can√¥nica** a menor dentre todas as representa√ß√µes de um grafo
- Dois grafos s√£o isomorfos se possuem a mesma representa√ß√£o can√¥nica
- Assim, o problema de encontrar subgrafos frequentes √© equivalente a enumerar suas representa√ß√µes can√¥nicas
  - Uma vez que as representa√ß√µes s√£o sequ√™ncias, o problema √© essencialmente um problema de minera√ß√£o de sequ√™ncias

---

- O gSpan explora em profundidade o espa√ßo de busca estendendo prefixos (subgrafos) com arestas no caminho mais √† direita
  - O artigo original demonstra que representa√ß√µes can√¥nicas s√≥ podem ser geradas por arestas seguindo o caminho mais √† direita
- Ele inicia com o prefixo vazio e o estende com cada uma das arestas em ordem
  - As poss√≠veis extens√µes s√£o determinadas a partir dos caminhos mais √† direita dos grafos em que o prefixo ocorre
- O suporte das extens√µes s√£o computados e, caso sejam frequentes e can√¥nicas, s√£o exploradas recursivamente

---

- **ALGORITHM 11.1. Algorithm gSpan**
  - // Initial Call: $C \leftarrow \emptyset$
  - **gSpan** $(C, D, minsup)$:
    - $\epsilon \leftarrow RightMostPath-Extensions(C, D)$ `// extensions and supports`
    - **foreach** $(t, sup(t)) \in \epsilon$ do
      - $C' \leftarrow C \cup t$ `// extend the code with extended edge tuple t`
      - $sup(C') \leftarrow sup(t)$ `// record the support of new extension`
      - `// recursively call gSpan if code is frequent and canonical`
      - **if** $sup(C') \geq minsup$ **and** IsCanonical $(C')$ **then**
        - **gSpan** $(C', D, minsup)$

---

- Exemplo: $D = \{G_1, G_2\}, minsup=2$

#### Extens√µes do problema

- Assim como ocorre com sequ√™ncias e itemsets, o n√∫mero de subgrafos frequentes pode ser bastante elevado em uma base
  - Isso acaba dificultando a an√°lise dos resultados
- Existem abordagens, como o CloseGraph proposto por Yan e Han, que mineram padr√µes fechados
- Pode-se tamb√©m buscar padr√µes contrastantes: com suporte m√≠nimo em um subconjunto e m√°ximo em outro
  - Exemplo: subgrafos que ocorrem com frequ√™ncia no conjunto de mol√©culas que potencializam um efeito desejado; e que sejam infrequentes no conjunto de mol√©culas que apresentem efeitos adversos
  - Essa abordagem foi sugerida por Borgelt e Berthold no algoritmo chamado MoFa
  - Suporte m√≠nimo √© usado para podar o espa√ßo de busca, enquanto o m√°ximo usado para filtrar padr√µes indesejados
- Na linha de algoritmos, existem abordagens heur√≠sticas para o problema, dado seu alto custo computacional
- Outras linhas de pesquisa incluem uso de abordagens distribu√≠das como o Fractal proposto por Dias et al. em 2019

#### Leitura

- Cap√≠tulo 11 Zaki e Meira
- Se√ß√£o 7.5 Tan et al.
- Cap√≠tulo 5 Mining Graph Data, Diane Cook e Lawrence Holder
- Akihiro Inokuchi, Takashi Washio, and Hiroshi Motoda. 2000. An Apriori-Based Algorithm for Mining
  Frequent Substructures from Graph Data. In Proceedings of the 4th European Conference on Principles of
  Data Mining and Knowledge Discovery (PKDD '00). Springer-Verlag, Berlin, Heidelberg, 13‚Äì23.
- Michihiro Kuramochi and George Karypis. 2001. Frequent Subgraph Discovery. In Proceedings of the 2001
  IEEE International Conference on Data Mining (ICDM '01). IEEE Computer Society, USA, 313‚Äì320.
- Xifeng Yan and Jiawei Han. 2002. GSpan: Graph-Based Substructure Pattern Mining. In Proceedings of the
  2002 IEEE International Conference on Data Mining (ICDM '02). IEEE Computer Society, USA, 721.
- Xifeng Yan and Jiawei Han. 2003. CloseGraph: mining closed frequent graph patterns. In Proceedings of the
  ninth ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '03). Association
  for Computing Machinery, New York, NY, USA, 286‚Äì295. <https://doi.org/10.1145/956750.956784>
- Christian Borgelt and Michael R. Berthold. 2002. Mining Molecular Fragments: Finding Relevant
  Substructures of Molecules. In Proceedings of the 2002 IEEE International Conference on Data Mining
  (ICDM '02). IEEE Computer Society, USA, 51

### Slide: aula08-regras-de-associa√ß√£o

## Aula 13 | 29/04/2025 | Descoberta de subgrupos

### Aula 14 | 06/05/2025 | Descoberta de subgrupos

### Aula 15 | 08/05/2025 | Descoberta de subgrupos

### Aula 16 | 13/05/2025 | Minera√ß√£o de modelos excepcionais

### Aula 17 | 15/05/2025 | Minera√ß√£o de modelos excepcionais

### Aula 18 | 20/05/2025 | Minera√ß√£o de modelos excepcionais

### Aula 19 | 22/05/2025 | Semin√°rios (Padr√µes Frequentes)

### Aula 20 | 27/05/2025 | Semin√°rios (Padr√µes Frequentes)

### Aula 21 | 29/05/2025 | Semin√°rios (Padr√µes Frequentes)

### Aula 22 | 03/06/2025 | Semin√°rios (SD)

### Aula 23 | 05/06/2025 | Semin√°rios (SD)

### Aula 24 | 10/06/2025 | Semin√°rios (SD)

### Aula 25 | 12/06/2025 | Semin√°rios (aplica√ß√µes)

### Aula 26 | 17/06/2025 | Semin√°rios (aplica√ß√µes)

### Aula 27 | 24/06/2025 | Semin√°rios (aplica√ß√µes)

### Aula 28 | 26/06/2025 | Projeto

### Aula 29 | 01/07/2025 | Projeto

### Aula 30 | 03/07/2025 | Projeto
